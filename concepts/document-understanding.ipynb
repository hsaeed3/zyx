{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Document Understanding & Interpretation w/ `zyx`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade --quiet zyx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zyx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `zyx.read()` function is capable of reading most document types, URLs to documents, as well as lists of URL's and paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try reading a paper on Arxiv\n",
    "document = zyx.read(\"https://arxiv.org/pdf/2407.21787\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The read function returns a `Document` object by default. A `Document` object contains two main fields: `content` and `metadata`. Lets try inspecting these two fields now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Metadata: \n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'file_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2407.21787'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'file_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'application/pdf'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'file_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">955583</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Metadata: \n",
       "\u001b[1m{\u001b[0m\u001b[32m'file_name'\u001b[0m: \u001b[32m'2407.21787'\u001b[0m, \u001b[32m'file_type'\u001b[0m: \u001b[32m'application/pdf'\u001b[0m, \u001b[32m'file_size'\u001b[0m: \u001b[1;36m955583\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Content:  Large Language Monkeys: Scaling Inference Compute\n",
       "with Repeated Sampling\n",
       "Bradley Brown∗†‡, Jordan Juravsky∗†, Ryan Ehrlich∗†, Ronald Clark‡, Quoc V. Le§,\n",
       "Christopher R´ e†, and Azalia Mirhoseini†§\n",
       "†De\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Content:  Large Language Monkeys: Scaling Inference Compute\n",
       "with Repeated Sampling\n",
       "Bradley Brown∗†‡, Jordan Juravsky∗†, Ryan Ehrlich∗†, Ronald Clark‡, Quoc V. Le§,\n",
       "Christopher R´ e†, and Azalia Mirhoseini†§\n",
       "†De\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Metadata: \", document.metadata)\n",
    "print(\"Content: \", document.content[:200]) # We'll print the first 200 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, the `zyx.read()` function can also return just a string, but for this example we will be specifically\n",
    "# using the `Document` object.\n",
    "\n",
    "# document = zyx.read(\"https://arxiv.org/pdf/2407.21787\", output = str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the neat things about the `Document` object is that it is directly queryable with LLMs to achieve `single-document QA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This supports all arguments of `zyx.completion()`, so it can be used with all LiteLLM compatible models.\n",
    "response = document.query(\n",
    "    \"In 2-3 sentences, summarize the key findings of this paper.\",\n",
    "    model = \"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The paper demonstrates that scaling inference compute through repeated sampling significantly improves the coverage\n",
       "of language models across various tasks, with increases in problem-solving success rates directly correlating to \n",
       "the number of samples generated. By amplifying weaker models with repeated sampling, the authors found that models \n",
       "like DeepSeek-Coder can outperform stronger models like GPT-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> despite lower individual capabilities, making this \n",
       "approach more cost-effective.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The paper demonstrates that scaling inference compute through repeated sampling significantly improves the coverage\n",
       "of language models across various tasks, with increases in problem-solving success rates directly correlating to \n",
       "the number of samples generated. By amplifying weaker models with repeated sampling, the authors found that models \n",
       "like DeepSeek-Coder can outperform stronger models like GPT-\u001b[1;36m4\u001b[0m despite lower individual capabilities, making this \n",
       "approach more cost-effective.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The response is in the standard OpenAI `ChatCompletion` or LiteLLM `ModelResponse` format, lets print out just the message content.\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can expand further with our `Document` now, by adding it to a collection. `zyx` contains three built-in collections: `Sql` or an SQLModel BM25 based document store, `VectorStore` a Qdrant based vector store, and `Rag` a collection that uses a combination of both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-26 12:39:49.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzyx.lib.data.vector_store\u001b[0m:\u001b[36m_create_collection\u001b[0m:\u001b[36m93\u001b[0m - \u001b[1mCollection 'document-understanding' does not exist. Creating it now.\u001b[0m\n",
      "\u001b[32m2024-09-26 12:39:49.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzyx.lib.data.vector_store\u001b[0m:\u001b[36m_create_collection\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mCollection 'document-understanding' created successfully.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Lets try using the `VectorStore` collection now.\n",
    "collection = zyx.VectorStore(\n",
    "    collection_name = \"document-understanding\",\n",
    "    location = \":memory:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-26 12:39:54.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzyx.lib.data.vector_store\u001b[0m:\u001b[36madd\u001b[0m:\u001b[36m181\u001b[0m - \u001b[1mSuccessfully added 14 points to the collection.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# .add() also supports adding strings of text\n",
    "# collection.add(\"This is a test document\")\n",
    "\n",
    "collection.add(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Although using document stores is the standard for document understanding with LLMs when working with multiple documents. LLM's now have a high enough context window that they can often perform single-document QA without the need for a document store. Hence, using `zyx.read()`, & `document.query()` are often enough for simple use cases.\n",
    "\n",
    "Lets now query our collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-26 12:39:54.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzyx.lib.data.vector_store\u001b[0m:\u001b[36mcompletion\u001b[0m:\u001b[36m303\u001b[0m - \u001b[1mInitial messages: What did the author do in their research?\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In their research, the authors explored the concept of scaling inference compute in large language models <span style=\"font-weight: bold\">(</span>LLMs<span style=\"font-weight: bold\">)</span> by\n",
       "utilizing repeated sampling to improve performance on various tasks. They conducted experiments to analyze how \n",
       "increasing the number of generated samples affected the coverage <span style=\"font-weight: bold\">(</span>the fraction of problems solved by any attempt<span style=\"font-weight: bold\">)</span> \n",
       "and precision <span style=\"font-weight: bold\">(</span>the ability to identify correct solutions from generated samples<span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "Key findings from their research include:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Significant improvement in coverage was observed by increasing the number of samples. For example, the coverage \n",
       "for certain tasks increased dramatically with more attempts, showing log-linear scalability.\n",
       "  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. In domains with automatic verification tools, such as coding and formal proofs, this repeated sampling approach \n",
       "translated directly into improved performance metrics. For instance, they were able to achieve a higher \n",
       "problem-solving rate compared to existing state-of-the-art models.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. The research highlighted the potential for cost-effective solutions using less powerful models amplified by \n",
       "repeated sampling. They found that using multiple samples from a less expensive model could outperform a single \n",
       "attempt from a more advanced model.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. The authors also identified challenges in precision when selecting correct solutions from a large set of \n",
       "generated samples, noting that common methods like majority voting often plateaued in effectiveness beyond a \n",
       "certain number of samples.\n",
       "\n",
       "Overall, the research emphasizes the benefits of repeated sampling as a strategy for enhancing the capabilities of \n",
       "LLMs in various reasoning tasks and suggests future research directions to improve sample verification methods.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In their research, the authors explored the concept of scaling inference compute in large language models \u001b[1m(\u001b[0mLLMs\u001b[1m)\u001b[0m by\n",
       "utilizing repeated sampling to improve performance on various tasks. They conducted experiments to analyze how \n",
       "increasing the number of generated samples affected the coverage \u001b[1m(\u001b[0mthe fraction of problems solved by any attempt\u001b[1m)\u001b[0m \n",
       "and precision \u001b[1m(\u001b[0mthe ability to identify correct solutions from generated samples\u001b[1m)\u001b[0m.\n",
       "\n",
       "Key findings from their research include:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. Significant improvement in coverage was observed by increasing the number of samples. For example, the coverage \n",
       "for certain tasks increased dramatically with more attempts, showing log-linear scalability.\n",
       "  \n",
       "\u001b[1;36m2\u001b[0m. In domains with automatic verification tools, such as coding and formal proofs, this repeated sampling approach \n",
       "translated directly into improved performance metrics. For instance, they were able to achieve a higher \n",
       "problem-solving rate compared to existing state-of-the-art models.\n",
       "\n",
       "\u001b[1;36m3\u001b[0m. The research highlighted the potential for cost-effective solutions using less powerful models amplified by \n",
       "repeated sampling. They found that using multiple samples from a less expensive model could outperform a single \n",
       "attempt from a more advanced model.\n",
       "\n",
       "\u001b[1;36m4\u001b[0m. The authors also identified challenges in precision when selecting correct solutions from a large set of \n",
       "generated samples, noting that common methods like majority voting often plateaued in effectiveness beyond a \n",
       "certain number of samples.\n",
       "\n",
       "Overall, the research emphasizes the benefits of repeated sampling as a strategy for enhancing the capabilities of \n",
       "LLMs in various reasoning tasks and suggests future research directions to improve sample verification methods.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = collection.completion(\n",
    "    \"What did the author do in their research?\"\n",
    ")\n",
    "\n",
    "print(results.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
