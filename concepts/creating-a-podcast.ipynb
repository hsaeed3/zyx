{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Creating a Podcast from a Document**\n",
    "\n",
    "One of the cool concepts that we can create using `zyx` is creating a spoken word podcast from a document. If you havent already, run the command below to install `zyx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade --quiet zyx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now lets get started!!**\n",
    "\n",
    "The first thing we need to do is load our document. For this example we will use a document that contains a newly released research paper (as of September 2024) that outlines a new reasoning method with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zyx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets read our document\n",
    "# You can replace this with any url or local file path\n",
    "document = zyx.read(\"https://arxiv.org/pdf/2407.21787\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `zyx.read()` function will automatically detect the file type and load it for us. It returns a `Document` object, containing the content and metadata of the document. Lets inspect the document to see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively you can return just a string; but for this example we'll use Document\n",
    "# document = zyx.read(\"https://arxiv.org/pdf/2407.21787\", output = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Metadata:\n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'file_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2407.21787'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'file_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'application/pdf'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'file_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">955583</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Metadata:\n",
       "\u001b[1m{\u001b[0m\u001b[32m'file_name'\u001b[0m: \u001b[32m'2407.21787'\u001b[0m, \u001b[32m'file_type'\u001b[0m: \u001b[32m'application/pdf'\u001b[0m, \u001b[32m'file_size'\u001b[0m: \u001b[1;36m955583\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Content: Large Language Monkeys: Scaling Inference Compute\n",
       "with Repeated Sampling\n",
       "Bradley Brown∗†‡, Jordan Ju\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Content: Large Language Monkeys: Scaling Inference Compute\n",
       "with Repeated Sampling\n",
       "Bradley Brown∗†‡, Jordan Ju\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Metadata:\", document.metadata)\n",
    "print(\"---\")\n",
    "print(\"Content:\", document.content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the neat things about the `Document` object is that it comes built in with an ability to query with an LLM, allowing for single document RAG. Lets query the document & ask it what the main topic is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The main topic of the paper is the exploration of repeated sampling as a method to scale inferential computation in\n",
       "large language models <span style=\"font-weight: bold\">(</span>LLMs<span style=\"font-weight: bold\">)</span> to enhance their problem-solving capabilities. The authors demonstrate that increasing\n",
       "the number of generated samples significantly improves the coverage and performance of these models across various \n",
       "tasks, particularly in domains where solutions can be automatically verified. Additionally, they reveal how \n",
       "repeated sampling can be a cost-effective strategy compared to using a single sample from more advanced models.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The main topic of the paper is the exploration of repeated sampling as a method to scale inferential computation in\n",
       "large language models \u001b[1m(\u001b[0mLLMs\u001b[1m)\u001b[0m to enhance their problem-solving capabilities. The authors demonstrate that increasing\n",
       "the number of generated samples significantly improves the coverage and performance of these models across various \n",
       "tasks, particularly in domains where solutions can be automatically verified. Additionally, they reveal how \n",
       "repeated sampling can be a cost-effective strategy compared to using a single sample from more advanced models.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = document.query(\n",
    "    \"what is the main topic of this paper? Respond in 2-3 sentences.\",\n",
    "    model = \"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "# The returned response will be in the OpenAI 'ChatCompletion format.\n",
    "# Lets print out just the response content\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zyx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
