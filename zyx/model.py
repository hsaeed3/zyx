"""zyx.models"""

from __future__ import annotations

import asyncio
from collections.abc import AsyncIterable, Iterable
from functools import lru_cache
from dataclasses import dataclass
from typing import (
    Any,
    Dict,
    Generic,
    List,
    Literal,
    Type,
    TypeVar,
    overload,
)

from pydantic import BaseModel, Field

from instructor.core.client import Instructor, AsyncInstructor
from instructor.auto_client import from_provider
from instructor.models import KnownModelName
from instructor.processing.response import prepare_response_model

from .providers import ProviderUtils


T = TypeVar("T")


class ModelSettings(BaseModel):
    """
    Simple container for common, provider-agnostic settings for a given
    model.

    NOTE: The `strict` and `max_retries` parameters are **`instructor`-specific**,
    and only used when creating structured outputs.

    You may also provide additional provider-specific kwargs through
    the `kwargs` field.
    """

    strict: bool | None = None
    """Whether to enforce strict validation of the response model.
    This is only used when creating structured outputs."""

    max_retries: int | None = None
    """The maximum number of retries to attempt when creating structured outputs.
    This is only used when creating structured outputs.

    NOTE: If this is not set, the default value of 3 will be used."""

    temperature: float | None = None
    """Controls the randomness of the generated output.
    Higher values make responses more creative and diverse, while lower values make them more deterministic and focused."""

    max_tokens: int | None = None
    """Specifies the maximum number of tokens that can be generated in the output.
    This acts as a hard limit to prevent excessively long responses."""

    top_p: float | None = None
    """Applies nucleus (top-p) sampling to limit token selection to a subset of likely candidates.
    Lower values narrow the output to high-probability choices, while higher values increase diversity."""

    stop: str | List[str] | None = None
    """Defines one or more sequences where the generated response should stop.
    If any of these sequences are encountered, generation will halt immediately."""

    seed: int | None = None
    """Sets a random seed to make outputs reproducible.
    Using the same seed and parameters will yield consistent results."""

    kwargs: Dict[str, Any] = Field(default_factory=dict)
    """Provider-specific or extra keyword arguments."""

    def dump_non_structured(self, **kwargs: Any) -> Dict[str, Any]:
        params = self.model_dump(
            exclude_none=True, exclude={"max_retries", "strict", "kwargs"}
        )
        extra = self.kwargs
        if extra:
            params.update(extra)
        if kwargs:
            params.update(kwargs)
        return params

    def dump_structured(self, **kwargs: Any) -> Dict[str, Any]:
        params = self.model_dump(exclude_none=True, exclude={"kwargs"})
        extra = self.kwargs
        if extra:
            params.update(extra)
        if kwargs:
            params.update(kwargs)
        return params


class ModelResponseToolCall(BaseModel):
    """
    Standardized object representation of a single tool call generated by a model. This
    is used internally by the framework for executing tools.
    """

    id: str | None = None
    """The ID of this tool call."""

    name: str | None = None
    """The name of the tool that was called."""

    arguments: Dict[str, Any] = Field(default_factory=dict)
    """The arguments to be passed to the tool during invocation."""


class ModelResponse(BaseModel, Generic[T]):
    """
    Unified container for a response generated from a model. This class
    is generic to type `T`, of the `output`, which represents the target
    type set during invocation.

    This class represents both structured outputs as well as standard
    text/tool call responses.
    """

    output: T | None = None
    """The 'final'/primary output from this response. This can be None in the case
    a response has tool calls with no text content."""

    raw: Any | None = Field(default=None, repr=False)
    """The raw, response from the model, in the provider's native format."""

    tool_calls: List[ModelResponseToolCall] | None = Field(
        default=None, repr=False
    )
    """A list of tool calls made during this response. This is only present
    in non-structured outputs."""

    usage: Dict[str, int] | None = Field(default=None, repr=False)
    """Token usage statistics with keys: prompt_tokens, completion_tokens, total_tokens."""

    finish_reason: str | None = Field(default=None, repr=False)
    """Why generation stopped (e.g., 'stop', 'tool_calls', 'length')."""


class ModelResponseChunk(BaseModel, Generic[T]):
    """
    Unified container for a chunk of a response generated from a model. This class
    is generic to type `T`, of the `output`, which represents the target
    type set during invocation.

    This class represents both structured outputs as well as standard
    text/tool call responses.
    """

    output_delta: T | None = None
    """The delta/change in the output from the previous chunk. This can be None
    in the case of the first chunk."""

    raw_delta: Any | None = Field(default=None, repr=False)
    """The raw, delta from the model, in the provider's native format."""

    tool_calls_delta: List[ModelResponseToolCall] | None = Field(
        default=None, repr=False
    )
    """A list of tool calls made during this chunk. This is only present
    in non-structured outputs."""

    usage: Dict[str, int] | None = Field(default=None, repr=False)
    """Token usage statistics (usually only in final chunk if provider supports it)."""

    finish_reason: str | None = Field(default=None, repr=False)
    """Why generation stopped (usually only in final chunk)."""

    is_complete: bool = Field(default=False)
    """Whether this chunk is the last chunk of the response."""


@dataclass(init=False)
class Model:
    """
    A wrapper around an `instructor` client that is used to interface
    with both the `instructor` client and the wrapped provider base client
    within it.
    """

    def __init__(
        self,
        model: KnownModelName | str | None = None,
        client: Instructor | AsyncInstructor | None = None,
        settings: ModelSettings | None = None,
        **kwargs: Any,
    ) -> None:
        if client is None and model is None:
            raise ValueError("Either a client or a model must be provided")

        if client is None and model is not None:
            self._client: AsyncInstructor = (
                _get_instructor_client_from_model(
                    model=model,
                    **kwargs,
                )
            )

            # !! model name must be cut here, as we use the model
            # provided by the `Model` during invocation
            model = model.split("/")[-1]  # type: ignore
        elif client is not None:
            self._client: AsyncInstructor = client
            self._model = model

        self._utils = _get_provider_utils_from_instructor_client(
            self._client
        )

        self._model = model
        self.settings = settings or ModelSettings(
            max_retries=3, strict=False
        )

    @overload
    async def arequest(
        self,
        messages: List[Any],
        *,
        target: Type[T] | Type[str] = str,
        tools: List[Dict[str, Any] | Any] | None = None,
        stream: Literal[True],
        **kwargs: Any,
    ) -> AsyncIterable[ModelResponseChunk[T]]: ...

    @overload
    async def arequest(
        self,
        messages: List[Any],
        *,
        target: Type[T] | Type[str] = str,
        tools: List[Dict[str, Any] | Any] | None = None,
        stream: Literal[False] = False,
        **kwargs: Any,
    ) -> ModelResponse[T]: ...

    async def arequest(
        self,
        messages: List[Any],
        *,
        target: Type[T] | Type[str] = str,
        tools: List[Dict[str, Any] | Any] | None = None,
        stream: bool = False,
        **kwargs: Any,
    ) -> ModelResponse[T] | AsyncIterable[ModelResponseChunk[T]]:
        is_structured = target is not str and target is not None

        if is_structured:
            create_kwargs = self.settings.dump_structured(**kwargs)
        else:
            create_kwargs = self.settings.dump_non_structured(**kwargs)

        if self._model:
            create_kwargs.setdefault("model", self._model)

        if tools:
            create_kwargs["tools"] = tools

        if stream:
            return self._astream(
                messages, target, is_structured, create_kwargs
            )

        if is_structured:
            return await self._astructured(messages, target, create_kwargs)

        return await self._aunstructured(messages, create_kwargs)  # type: ignore

    async def _astructured(
        self,
        messages: List[Any],
        target: Type[T],
        kwargs: Dict[str, Any],
    ) -> ModelResponse[T]:
        raw = None

        def _capture(response: Any) -> None:
            nonlocal raw
            raw = response

        self._client.on("completion:response", _capture)

        try:
            output = await self._client.create(
                response_model=target,
                messages=messages,
                **kwargs,
            )
        finally:
            self._client.off("completion:response", _capture)

        parsed = (
            self._utils["parse_raw_response"](raw, False) if raw else {}
        )
        return ModelResponse(
            output=output,
            raw=raw,
            usage=parsed.get("usage"),
            finish_reason=parsed.get("finish_reason"),
        )

    async def _aunstructured(
        self,
        messages: List[Any],
        kwargs: Dict[str, Any],
    ) -> ModelResponse[str]:
        raw = await self._client.create(
            response_model=None,
            messages=messages,
            **kwargs,
        )
        parsed = self._utils["parse_raw_response"](raw, False)
        tool_calls = None
        if parsed.get("tool_calls"):
            tool_calls = [
                ModelResponseToolCall(
                    id=tc.get("id"),
                    name=tc.get("name"),
                    arguments=tc.get("arguments")
                    if isinstance(tc.get("arguments"), dict)
                    else {},
                )
                for tc in parsed["tool_calls"]
            ]
        return ModelResponse(
            output=parsed.get("content"),
            raw=raw,
            tool_calls=tool_calls,
            usage=parsed.get("usage"),
            finish_reason=parsed.get("finish_reason"),
        )

    async def _astream(
        self,
        messages: List[Any],
        target: Type[T] | Type[str],
        is_structured: bool,
        kwargs: Dict[str, Any],
    ) -> AsyncIterable[ModelResponseChunk[T]]:
        if is_structured:
            async for chunk in self._astream_structured(
                messages, target, kwargs
            ):
                yield chunk
        else:
            async for chunk in self._astream_unstructured(
                messages, kwargs
            ):
                yield chunk

    async def _astream_structured(
        self,
        messages: List[Any],
        target: Type[T],
        kwargs: Dict[str, Any],
    ) -> AsyncIterable[ModelResponseChunk[T]]:
        wrapped_type = False
        if not hasattr(target, "__pydantic_model__"):
            target = prepare_response_model(target)
            wrapped_type = True

        async for partial in self._client.create_partial(
            response_model=target,
            messages=messages,
            **kwargs,
        ):
            yield ModelResponseChunk(
                output_delta=partial.content if wrapped_type else partial,
            )
        yield ModelResponseChunk(is_complete=True)

    async def _astream_unstructured(
        self,
        messages: List[Any],
        kwargs: Dict[str, Any],
    ) -> AsyncIterable[ModelResponseChunk[str]]:
        kwargs["stream"] = True
        stream = await self._client.create(
            response_model=None,
            messages=messages,
            **kwargs,
        )
        async for chunk in stream:
            parsed = self._utils["parse_raw_response"](chunk, True)
            tool_calls_delta = None
            if parsed.get("tool_calls"):
                tool_calls_delta = [
                    ModelResponseToolCall(
                        id=tc.get("id"),
                        name=tc.get("name"),
                        arguments=tc.get("arguments")
                        if isinstance(tc.get("arguments"), dict)
                        else {},
                    )
                    for tc in parsed["tool_calls"]
                ]
            is_done = parsed.get("finish_reason") is not None
            yield ModelResponseChunk(
                output_delta=parsed.get("content"),
                raw_delta=chunk,
                tool_calls_delta=tool_calls_delta,
                usage=parsed.get("usage") if is_done else None,
                finish_reason=parsed.get("finish_reason"),
                is_complete=is_done,
            )

    @overload
    def request(
        self,
        messages: List[Any],
        *,
        target: Type[T] | Type[str] = str,
        tools: List[Dict[str, Any] | Any] | None = None,
        stream: Literal[True],
        **kwargs: Any,
    ) -> Iterable[ModelResponseChunk[T]]: ...

    @overload
    def request(
        self,
        messages: List[Any],
        *,
        target: Type[T] | Type[str] = str,
        tools: List[Dict[str, Any] | Any] | None = None,
        stream: Literal[False] = False,
        **kwargs: Any,
    ) -> ModelResponse[T]: ...

    def request(
        self,
        messages: List[Any],
        *,
        target: Type[T] | Type[str] = str,
        tools: List[Dict[str, Any] | Any] | None = None,
        stream: bool = False,
        **kwargs: Any,
    ) -> ModelResponse[T] | Iterable[ModelResponseChunk[T]]:
        if stream:
            return self._stream_sync(messages, target, tools, kwargs)
        return _run_sync(
            self.arequest(
                messages,
                target=target,
                tools=tools,
                stream=False,
                **kwargs,
            )
        )

    def _stream_sync(
        self,
        messages: List[Any],
        target: Type[T] | Type[str],
        tools: List[Dict[str, Any] | Any] | None,
        kwargs: Dict[str, Any],
    ) -> Iterable[ModelResponseChunk[T]]:
        async def _collect() -> List[ModelResponseChunk[T]]:
            chunks: List[ModelResponseChunk[T]] = []
            async for chunk in await self.arequest(
                messages, target=target, tools=tools, stream=True, **kwargs
            ):
                chunks.append(chunk)
            return chunks

        return iter(_run_sync(_collect()))


def _run_sync(coro: Any) -> Any:
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None

    if loop is None:
        return asyncio.run(coro)
    else:
        import concurrent.futures

        with concurrent.futures.ThreadPoolExecutor() as pool:
            future = pool.submit(asyncio.run, coro)
            return future.result()


@lru_cache(maxsize=32)
def _get_provider_utils_from_instructor_client(
    client: Instructor | AsyncInstructor,
) -> ProviderUtils:
    name = client.mode.name

    if name.startswith("ANTHROPIC"):
        from .providers.anthropic import ANTHROPIC_PROVIDER_UTILS

        return ANTHROPIC_PROVIDER_UTILS

    if (
        name.startswith("GENAI")
        or name.startswith("GEMINI")
        or name.startswith("VERTEXAI")
    ):
        from .providers.gemini import GEMINI_PROVIDER_UTILS

        return GEMINI_PROVIDER_UTILS

    # OpenAI + all OpenAI-compatible providers
    from .providers.openai import OPENAI_PROVIDER_UTILS

    return OPENAI_PROVIDER_UTILS


_NEW_CLIENT_CACHE: Dict[str, AsyncInstructor] = {}


def _get_instructor_client_from_model(
    model: str,
    **kwargs: Any,
) -> AsyncInstructor:
    if model in _NEW_CLIENT_CACHE:
        return _NEW_CLIENT_CACHE[model]

    try:
        client = from_provider(model, async_client=True, **kwargs)
    except Exception as e:
        raise ValueError(
            f"Failed to create instructor client for model {model}"
        ) from e

    _NEW_CLIENT_CACHE[model] = client
    return client
