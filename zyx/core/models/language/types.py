"""zyx.core.models.language.types"""

from __future__ import annotations

from collections.abc import Iterable
from dataclasses import dataclass
from functools import cached_property
from typing import Generic, Literal, Type, TypeAliasType, TypeVar

from instructor import Mode
from openai.types.chat.chat_completion import ChatCompletion
from openai.types.chat.chat_completion_assistant_message_param import (
    ChatCompletionAssistantMessageParam,
)
from openai.types.chat.chat_completion_chunk import ChatCompletionChunk
from openai.types.chat.chat_completion_message_tool_call import (
    ChatCompletionMessageToolCall,
)
from openai.types.completion_usage import CompletionUsage

from ..definition import ModelSettings

__all__ = [
    "LanguageModelSettings",
    "LanguageModelResponse",
    "LanguageModelName",
]


T = TypeVar("T")


@dataclass
class LanguageModelSettings(ModelSettings):
    """A definition of a language model, and associated settings.

    A language model can be defined within `zyx` by using a
    `LanguageModelName` string, or by creating a `LanguageModel` instance directly.
    """

    max_tokens: int | None = None
    """The maximum number of tokens to generate in the completion."""

    temperature: float | None = None
    """The sampling temperature to use for the completion."""

    top_p: float | None = None
    """The nucleus sampling probability to use for the completion."""

    top_logprobs: int | None = None
    """The number of top log probabilities to return for each token."""

    frequency_penalty: float | None = None
    """The frequency penalty to use for the completion."""

    presence_penalty: float | None = None
    """The presence penalty to use for the completion."""

    tool_choice: Literal["auto", "required", "none"] | str | None = None
    """The tool choice strategy to use when invoking the model."""

    parallel_tool_calls: bool | None = None
    """Whether to allow parallel tool calls when invoking the model."""

    @property
    def kind(self) -> str:
        return "language_model"


@dataclass
class LanguageModelResponse(Generic[T]):
    """Unified representation of a response generated by a language model.
    This model provides an interface for:

    - Standard chat completions + structured outputs (using Instructor)
    - Standard chat completions + streamed chunks (and structured output chunks using `Instructor.create_partial()`)
    """

    content: T | None = None
    """The final content of the response. This matches the parameter: `type` passed when invoking
    the model. This is not present for streamed responses.
    """

    delta: T | None = None
    """The delta content of the response. This is present for streamed responses.
    """

    raw: ChatCompletion | ChatCompletionChunk | None = None
    """The raw response from the model. This is either the full chat completion, or a single
    chunk of a streamed response.

    NOTE: This argument is always present.
    """

    instructor_mode: Mode | None = None
    """The instructor mode used to generate the response. This is only present for structured outputs.
    """

    def __str__(self):
        from ...._internal._beautification import Beautifier

        return Beautifier.for_response(
            type_name="LanguageModelResponse",
            scheme=Beautifier.Scheme(
                primary=Beautifier.Colors.CORNFLOWER_BLUE,
                accent=Beautifier.Colors.SANDY_BROWN,
                border=Beautifier.Colors.DODGER_BLUE3,
                dim=Beautifier.Colors.GRAY63,
            ),
            content=self.content or self.delta,
            main_params={"Model": self.model} if self.model else {},
            metadata={
                "Type": self.type.__name__ if self.type else None,
                "Streamed": self.is_chunk,
                "Structured": self.is_structured,
            },
        ).__str__()

    def __rich__(self):
        from ...._internal._beautification import Beautifier

        return Beautifier.for_response(
            type_name="LanguageModelResponse",
            scheme=Beautifier.Scheme(
                primary=Beautifier.Colors.CORNFLOWER_BLUE,
                accent=Beautifier.Colors.SANDY_BROWN,
                border=Beautifier.Colors.DODGER_BLUE3,
                dim=Beautifier.Colors.GRAY63,
            ),
            content=self.content or self.delta,
            main_params={"Model": self.model} if self.model else {},
            metadata={
                "Type": self.type.__name__ if self.type else None,
                "Streamed": self.is_chunk,
                "Structured": self.is_structured,
            },
        ).rich()

    @property
    def type(self) -> Type[T]:
        """The type of the response. This is the type of the response passed to the model."""
        return type(self.content) if self.content else None

    @property
    def is_chunk(self) -> bool:
        """Whether the response is a chunk of a streamed response."""
        return self.raw is not None and isinstance(
            self.raw, ChatCompletionChunk
        )

    @property
    def model(self) -> str | None:
        """The model used to generate the response. This is present for both standard chat completions
        and streamed responses.
        """
        return self.raw.model if self.raw else None

    @property
    def is_structured(self) -> bool:
        """Whether the response is a structured output. This is only present for structured outputs."""
        return self.instructor_mode is not None

    @property
    def usage(self) -> CompletionUsage | None:
        """The usage information for the response. This is present for both standard chat completions
        and streamed responses.
        """
        return self.raw.usage if self.raw else None

    @cached_property
    def tool_calls(self) -> Iterable[ChatCompletionMessageToolCall]:
        """
        Returns an iterable of ChatCompletionMessageToolCall objects representing tool calls
        in the response. Handles both standard and streamed completions.
        """
        if not self.has_tool_calls or not self.raw:
            return []

        # Standard ChatCompletion
        if isinstance(self.raw, ChatCompletion):
            return self.raw.choices[0].message.tool_calls or []

        # Streamed ChatCompletionChunk
        tool_calls = []
        # Each chunk may contain a delta with tool_calls
        for chunk in self.raw.choices:
            delta = getattr(chunk, "delta", None)
            if delta and getattr(delta, "tool_calls", None):
                for tc in delta.tool_calls:
                    # Defensive: ensure function and its fields exist
                    name = (
                        getattr(tc.function, "name", "")
                        if getattr(tc, "function", None)
                        else ""
                    )
                    arguments = (
                        getattr(tc.function, "arguments", "")
                        if getattr(tc, "function", None)
                        else ""
                    )
                    tool_call = ChatCompletionMessageToolCall(
                        name=name, arguments=arguments
                    )
                    tool_calls.append(tool_call)
        return tool_calls

    @cached_property
    def has_tool_calls(self) -> bool:
        """Whether the response contains tool calls."""
        if self.raw:
            if isinstance(self.raw, ChatCompletion):
                return self.raw.choices[0].message.tool_calls is not None
            elif isinstance(self.raw, ChatCompletionChunk):
                # check if partial content is available within the chunk
                return self.raw.choices[0].delta.tool_calls is not None

    @property
    def tools_called(self) -> Iterable[str]:
        """Get the names of the tools called in the response, if any."""
        return [tc.function.name for tc in self.tool_calls]

    def as_message(self) -> ChatCompletionAssistantMessageParam:
        if self.is_chunk:
            raise ValueError("Cannot convert a chunk to a message")

        return self.raw.choices[0].message.model_dump()


LanguageModelName = TypeAliasType(
    "LanguageModelName",
    Literal[
        "mock",
        "anthropic/claude-3-5-haiku-20241022",
        "anthropic/claude-3-5-haiku-latest",
        "anthropic/claude-3-5-sonnet-20240620",
        "anthropic/claude-3-5-sonnet-20241022",
        "anthropic/claude-3-5-sonnet-latest",
        "anthropic/claude-haiku-4-5",
        "anthropic/claude-haiku-4-5-20251001",
        "anthropic/claude-3-7-sonnet-20250219",
        "anthropic/claude-3-7-sonnet-latest",
        "anthropic/claude-3-haiku-20240307",
        "anthropic/claude-3-opus-20240229",
        "anthropic/claude-3-opus-latest",
        "anthropic/claude-4-opus-20250514",
        "anthropic/claude-4-sonnet-20250514",
        "anthropic/claude-opus-4-0",
        "anthropic/claude-opus-4-1-20250805",
        "anthropic/claude-opus-4-20250514",
        "anthropic/claude-sonnet-4-0",
        "anthropic/claude-sonnet-4-20250514",
        "anthropic/claude-sonnet-4-5",
        "anthropic/claude-sonnet-4-5-20250929",
        "bedrock/amazon.titan-tg1-large",
        "bedrock/amazon.titan-text-lite-v1",
        "bedrock/amazon.titan-text-express-v1",
        "bedrock/us.amazon.nova-pro-v1:0",
        "bedrock/us.amazon.nova-lite-v1:0",
        "bedrock/us.amazon.nova-micro-v1:0",
        "bedrock/anthropic.claude-3-5-sonnet-20241022-v2:0",
        "bedrock/us.anthropic.claude-3-5-sonnet-20241022-v2:0",
        "bedrock/anthropic.claude-3-5-haiku-20241022-v1:0",
        "bedrock/us.anthropic.claude-3-5-haiku-20241022-v1:0",
        "bedrock/anthropic.claude-instant-v1",
        "bedrock/anthropic.claude-v2:1",
        "bedrock/anthropic.claude-v2",
        "bedrock/anthropic.claude-3-sonnet-20240229-v1:0",
        "bedrock/us.anthropic.claude-3-sonnet-20240229-v1:0",
        "bedrock/anthropic.claude-3-haiku-20240307-v1:0",
        "bedrock/us.anthropic.claude-3-haiku-20240307-v1:0",
        "bedrock/anthropic.claude-3-opus-20240229-v1:0",
        "bedrock/us.anthropic.claude-3-opus-20240229-v1:0",
        "bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0",
        "bedrock/us.anthropic.claude-3-5-sonnet-20240620-v1:0",
        "bedrock/anthropic.claude-3-7-sonnet-20250219-v1:0",
        "bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0",
        "bedrock/anthropic.claude-opus-4-20250514-v1:0",
        "bedrock/us.anthropic.claude-opus-4-20250514-v1:0",
        "bedrock/anthropic.claude-sonnet-4-20250514-v1:0",
        "bedrock/us.anthropic.claude-sonnet-4-20250514-v1:0",
        "bedrock/cohere.command-text-v14",
        "bedrock/cohere.command-r-v1:0",
        "bedrock/cohere.command-r-plus-v1:0",
        "bedrock/cohere.command-light-text-v14",
        "bedrock/meta.llama3-8b-instruct-v1:0",
        "bedrock/meta.llama3-70b-instruct-v1:0",
        "bedrock/meta.llama3-1-8b-instruct-v1:0",
        "bedrock/us.meta.llama3-1-8b-instruct-v1:0",
        "bedrock/meta.llama3-1-70b-instruct-v1:0",
        "bedrock/us.meta.llama3-1-70b-instruct-v1:0",
        "bedrock/meta.llama3-1-405b-instruct-v1:0",
        "bedrock/us.meta.llama3-2-11b-instruct-v1:0",
        "bedrock/us.meta.llama3-2-90b-instruct-v1:0",
        "bedrock/us.meta.llama3-2-1b-instruct-v1:0",
        "bedrock/us.meta.llama3-2-3b-instruct-v1:0",
        "bedrock/us.meta.llama3-3-70b-instruct-v1:0",
        "bedrock/mistral.mistral-7b-instruct-v0:2",
        "bedrock/mistral.mixtral-8x7b-instruct-v0:1",
        "bedrock/mistral.mistral-large-2402-v1:0",
        "bedrock/mistral.mistral-large-2407-v1:0",
        "cerebras/gpt-oss-120b",
        "cerebras/llama3.1-8b",
        "cerebras/llama-3.3-70b",
        "cerebras/llama-4-scout-17b-16e-instruct",
        "cerebras/llama-4-maverick-17b-128e-instruct",
        "cerebras/qwen-3-235b-a22b-instruct-2507",
        "cerebras/qwen-3-32b",
        "cerebras/qwen-3-coder-480b",
        "cerebras/qwen-3-235b-a22b-thinking-2507",
        "cohere/c4ai-aya-expanse-32b",
        "cohere/c4ai-aya-expanse-8b",
        "cohere/command-nightly",
        "cohere/command-r-08-2024",
        "cohere/command-r-plus-08-2024",
        "cohere/command-r7b-12-2024",
        "deepseek/deepseek-chat",
        "deepseek/deepseek-reasoner",
        "gemini/gemini-2.0-flash",
        "gemini/gemini-2.0-flash-lite",
        "gemini/gemini-2.5-flash",
        "gemini/gemini-2.5-flash-preview-09-2025",
        "gemini/gemini-flash-latest",
        "gemini/gemini-2.5-flash-lite",
        "gemini/gemini-2.5-flash-lite-preview-09-2025",
        "gemini/gemini-flash-lite-latest",
        "gemini/gemini-2.5-pro",
        "vertex_ai/gemini-2.0-flash",
        "vertex_ai/gemini-2.0-flash-lite",
        "vertex_ai/gemini-2.5-flash",
        "vertex_ai/gemini-2.5-flash-preview-09-2025",
        "vertex_ai/gemini-flash-latest",
        "vertex_ai/gemini-2.5-flash-lite",
        "vertex_ai/gemini-2.5-flash-lite-preview-09-2025",
        "vertex_ai/gemini-flash-lite-latest",
        "vertex_ai/gemini-2.5-pro",
        "xai/grok-4",
        "xai/grok-4-0709",
        "xai/grok-3",
        "xai/grok-3-mini",
        "xai/grok-3-fast",
        "xai/grok-3-mini-fast",
        "xai/grok-2-vision-1212",
        "xai/grok-2-image-1212",
        "groq/distil-whisper-large-v3-en",
        "groq/gemma2-9b-it",
        "groq/llama-3.3-70b-versatile",
        "groq/llama-3.1-8b-instant",
        "groq/llama-guard-3-8b",
        "groq/llama3-70b-8192",
        "groq/llama3-8b-8192",
        "groq/moonshotai/kimi-k2-instruct",
        "groq/whisper-large-v3",
        "groq/whisper-large-v3-turbo",
        "groq/playai-tts",
        "groq/playai-tts-arabic",
        "groq/qwen-qwq-32b",
        "groq/mistral-saba-24b",
        "groq/qwen-2.5-coder-32b",
        "groq/qwen-2.5-32b",
        "groq/deepseek-r1-distill-qwen-32b",
        "groq/deepseek-r1-distill-llama-70b",
        "groq/llama-3.3-70b-specdec",
        "groq/llama-3.2-1b-preview",
        "groq/llama-3.2-3b-preview",
        "groq/llama-3.2-11b-vision-preview",
        "groq/llama-3.2-90b-vision-preview",
        "heroku/claude-3-5-haiku",
        "heroku/claude-3-5-sonnet-latest",
        "heroku/claude-3-7-sonnet",
        "heroku/claude-4-sonnet",
        "heroku/claude-3-haiku",
        "heroku/gpt-oss-120b",
        "heroku/nova-lite",
        "heroku/nova-pro",
        "huggingface/Qwen/QwQ-32B",
        "huggingface/Qwen/Qwen2.5-72B-Instruct",
        "huggingface/Qwen/Qwen3-235B-A22B",
        "huggingface/Qwen/Qwen3-32B",
        "huggingface/deepseek-ai/DeepSeek-R1",
        "huggingface/meta-llama/Llama-3.3-70B-Instruct",
        "huggingface/meta-llama/Llama-4-Maverick-17B-128E-Instruct",
        "huggingface/meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "mistral/codestral-latest",
        "mistral/mistral-large-latest",
        "mistral/mistral-moderation-latest",
        "mistral/mistral-small-latest",
        "moonshotai/moonshot-v1-8k",
        "moonshotai/moonshot-v1-32k",
        "moonshotai/moonshot-v1-128k",
        "moonshotai/moonshot-v1-8k-vision-preview",
        "moonshotai/moonshot-v1-32k-vision-preview",
        "moonshotai/moonshot-v1-128k-vision-preview",
        "moonshotai/kimi-latest",
        "moonshotai/kimi-thinking-preview",
        "moonshotai/kimi-k2-0711-preview",
        "ollama/cogito",
        "ollama/deepcoder",
        "ollama/deepseek-r1",
        "ollama/deepseek-v3.1",
        "ollama/gemma3",
        "ollama/gemma3n",
        "ollama/glm-4.6",
        "ollama/granite3",
        "ollama/granite3.2-vision",
        "ollama/granite3.3",
        "ollama/granite4",
        "ollama/kimi-k2",
        "ollama/llama3.1",
        "ollama/llama3.2",
        "ollama/llama4",
        "ollama/magistral",
        "ollama/minimax-m2",
        "ollama/mistral",
        "ollama/mistral-small3.1",
        "ollama/phi3",
        "ollama/phi4-mini",
        "ollama/qwen2.5",
        "ollama/qwen3",
        "ollama/qwen3-coder",
        "ollama/qwen3-vl",
        "openai/chatgpt-4o-latest",
        "openai/codex-mini-latest",
        "openai/gpt-3.5-turbo",
        "openai/gpt-3.5-turbo-0125",
        "openai/gpt-3.5-turbo-0301",
        "openai/gpt-3.5-turbo-0613",
        "openai/gpt-3.5-turbo-1106",
        "openai/gpt-3.5-turbo-16k",
        "openai/gpt-3.5-turbo-16k-0613",
        "openai/gpt-4",
        "openai/gpt-4-0125-preview",
        "openai/gpt-4-0314",
        "openai/gpt-4-0613",
        "openai/gpt-4-1106-preview",
        "openai/gpt-4-32k",
        "openai/gpt-4-32k-0314",
        "openai/gpt-4-32k-0613",
        "openai/gpt-4-turbo",
        "openai/gpt-4-turbo-2024-04-09",
        "openai/gpt-4-turbo-preview",
        "openai/gpt-4-vision-preview",
        "openai/gpt-4.1",
        "openai/gpt-4.1-2025-04-14",
        "openai/gpt-4.1-mini",
        "openai/gpt-4.1-mini-2025-04-14",
        "openai/gpt-4.1-nano",
        "openai/gpt-4.1-nano-2025-04-14",
        "openai/gpt-4o",
        "openai/gpt-4o-2024-05-13",
        "openai/gpt-4o-2024-08-06",
        "openai/gpt-4o-2024-11-20",
        "openai/gpt-4o-audio-preview",
        "openai/gpt-4o-audio-preview-2024-10-01",
        "openai/gpt-4o-audio-preview-2024-12-17",
        "openai/gpt-4o-audio-preview-2025-06-03",
        "openai/gpt-4o-mini",
        "openai/gpt-4o-mini-2024-07-18",
        "openai/gpt-4o-mini-audio-preview",
        "openai/gpt-4o-mini-audio-preview-2024-12-17",
        "openai/gpt-4o-mini-search-preview",
        "openai/gpt-4o-mini-search-preview-2025-03-11",
        "openai/gpt-4o-search-preview",
        "openai/gpt-4o-search-preview-2025-03-11",
        "openai/gpt-5",
        "openai/gpt-5-2025-08-07",
        "openai/o1",
        "openai/gpt-5-chat-latest",
        "openai/o1-2024-12-17",
        "openai/gpt-5-mini",
        "openai/o1-mini",
        "openai/gpt-5-mini-2025-08-07",
        "openai/o1-mini-2024-09-12",
        "openai/gpt-5-nano",
        "openai/o1-preview",
        "openai/gpt-5-nano-2025-08-07",
        "openai/o1-preview-2024-09-12",
        "openai/o1-pro",
        "openai/o1-pro-2025-03-19",
        "openai/o3",
        "openai/o3-2025-04-16",
        "openai/o3-deep-research",
        "openai/o3-deep-research-2025-06-26",
        "openai/o3-mini",
        "openai/o3-mini-2025-01-31",
        "openai/o4-mini",
        "openai/o4-mini-2025-04-16",
        "openai/o4-mini-deep-research",
        "openai/o4-mini-deep-research-2025-06-26",
        "openai/o3-pro",
        "openai/o3-pro-2025-06-10",
        "openai/computer-use-preview",
        "openai/computer-use-preview-2025-03-11",
    ],
)
"""Known model names for language models. This is just a beautification alias,
rather than something with functional impact."""
