"""zyx.models.language.types"""

from __future__ import annotations

from typing import TypeVar, Generic, Type, Literal, Iterable, TypeAliasType

from pydantic import BaseModel, Field
from instructor import Mode as InstructorMode
from openai.types.shared import Reasoning
from openai.types.chat.chat_completion import ChatCompletion
from openai.types.chat.chat_completion_message_tool_call import (
    ChatCompletionMessageToolCall,
)
from openai.types.chat.chat_completion_chunk import ChatCompletionChunk

__all__ = [
    "LanguageModelSettings",
    "LanguageModelResponse",
    "LanguageModelName",
]


T = TypeVar("T")
"""Generic variable alias for the structured output type of a language model."""


class LanguageModelSettings(BaseModel):
    """Definition of settings for a language model invocation. This represents
    various parameters that can be configured when calling a language model including
    sampling parameters, tool usage strategies, and more.
    """

    model_config = {"arbitrary_types_allowed": True}

    max_tokens: int | None = None
    """The maximum number of tokens to generate in the completion."""

    temperature: float | None = None
    """The sampling temperature to use for the completion."""

    top_p: float | None = None
    """The nucleus sampling probability to use for the completion."""

    top_logprobs: int | None = None
    """The number of top log probabilities to return for each token."""

    frequency_penalty: float | None = None
    """The frequency penalty to use for the completion."""

    presence_penalty: float | None = None
    """The presence penalty to use for the completion."""

    tool_choice: Literal["auto", "required", "none"] | str | None = None
    """The tool choice strategy to use when invoking the model."""

    parallel_tool_calls: bool | None = None
    """Whether to allow parallel tool calls when invoking the model."""

    reasoning: Reasoning | None = None
    """The reasoning strategy to use for the model, if supported."""


def _pretty_print_language_model_response(response: LanguageModelResponse) -> str:
    content = "LanguageModelResponse:\n"
    content += (
        f"{response.output if response.output is not None else 'No Output Content'}"
    )

    content += f"\n\n>>> Model: {response.model}"

    if response.output and not isinstance(response.output, str):
        content += f"\n>>> Type: {response.type}"
        content += f"\n>>> Instructor Mode: {response.instructor_mode.name}"

    if response.is_streamed:
        content += "\n>>> Streamed Response: True"

    # NOTE: we check for 'is_structured' to avoid printing the default response models or
    # provided response models used if Mode.TOOLS or a similar instructor mode that uses
    # function calling to generate a structured output.
    if response.has_tool_calls and not response.is_structured:
        content += f"\n>>> Tools Called: {', '.join(response.get_tools_called)}"
    return content


def _rich_pretty_print_language_model_response(response: LanguageModelResponse) -> str:
    content = "[bold dodger_blue2]LanguageModelResponse:[/bold dodger_blue2]\n"
    content += (
        f"{response.output if response.output is not None else 'No Output Content'}"
    )

    content += f"\n\n[bold sandy_brown]>>>[/bold sandy_brown] [sandy_brown]Model[/sandy_brown]: [italic]{response.model}[/italic]"

    if response.output and not isinstance(response.output, str):
        content += f"\n[bold sandy_brown]>>>[/bold sandy_brown] [sandy_brown]Type[/sandy_brown]: [italic]{response.type}[/italic]"
        content += f"\n[bold sandy_brown]>>>[/bold sandy_brown] [sandy_brown]Instructor Mode[/sandy_brown]: [italic]{response.instructor_mode.name}[/italic]"

    if response.is_streamed:
        content += "\n[bold sandy_brown]>>>[/bold sandy_brown] [sandy_brown]Streamed Response[/sandy_brown]: [italic]True[/italic]"
    if response.has_tool_calls and not response.is_structured:
        content += f"\n[bold sandy_brown]>>>[/bold sandy_brown] [sandy_brown]Tools Called[/sandy_brown]: [italic]{', '.join(response.get_tools_called)}[/italic]"
    return content


class LanguageModelResponse(BaseModel, Generic[T]):
    """A response produced by a language model. This is a representation of both
    standard chat completions, as well as structured outputs generated by the
    `instructor` library. This is also a unification for standard and streamed
    language model responses.

    The output field represents either the structured output type `T` or the
    raw chat completion message content as a string (if available). This can
    be `None` in the case a language model response contains no message content,
    (e.g., when only tool calls are made).
    """

    model_config = {"arbitrary_types_allowed": True}

    output: T | str | None = Field(default=None)
    """The structured output produced by the language model, or the raw
    chat completion message content as a string (if available). This can be
    `None` in the case a language model response contains no message content,
    (e.g., when only tool calls are made)."""

    completion: ChatCompletion | ChatCompletionChunk | None = Field(default=None)
    """The full chat completion response or streamed chat completion chunk from
    the model."""

    instructor_mode: InstructorMode | None = Field(default=None)
    """The `instructor` mode used to generate the structured output, if applicable.
    This is `None` if the response does not involve structured output generation.
    """

    def __repr__(self) -> str:
        return _pretty_print_language_model_response(self)

    def __str__(self) -> str:
        return _pretty_print_language_model_response(self)

    def __rich__(self) -> str:
        return _rich_pretty_print_language_model_response(self)

    @property
    def model(self) -> str:
        """The model used to generate this response."""
        if self.completion:
            return self.completion.model
        return "unknown"

    @property
    def type(self) -> Type[T] | None:
        """The type of the response output."""
        if self.output:
            return type(self.output)
        return None

    @property
    def is_structured(self) -> bool:
        """Whether the response output is structured (i.e., not a plain string)."""
        return self.output is not None and not isinstance(self.output, str)

    @property
    def is_streamed(self) -> bool:
        """Whether the response was produced via streaming."""
        return isinstance(self.completion, ChatCompletionChunk)

    @property
    def has_tool_calls(self) -> bool:
        """Whether the response contains tool calls."""
        if self.completion:
            if isinstance(self.completion, ChatCompletion):
                return self.completion.choices[0].message.tool_calls is not None
            elif isinstance(self.completion, ChatCompletionChunk):
                # check if partial content is available within the chunk
                return self.completion.choices[0].delta.tool_calls is not None

    @property
    def get_tool_calls(self) -> Iterable[ChatCompletionMessageToolCall]:
        """
        Returns an iterable of ChatCompletionMessageToolCall objects representing tool calls
        in the response. Handles both standard and streamed completions.
        """
        if not self.has_tool_calls or not self.completion:
            return []

        # Standard ChatCompletion
        if isinstance(self.completion, ChatCompletion):
            return self.completion.choices[0].message.tool_calls or []

        # Streamed ChatCompletionChunk
        tool_calls = []
        # Each chunk may contain a delta with tool_calls
        for chunk in self.completion.choices:
            delta = getattr(chunk, "delta", None)
            if delta and getattr(delta, "tool_calls", None):
                for tc in delta.tool_calls:
                    # Defensive: ensure function and its fields exist
                    name = (
                        getattr(tc.function, "name", "")
                        if getattr(tc, "function", None)
                        else ""
                    )
                    arguments = (
                        getattr(tc.function, "arguments", "")
                        if getattr(tc, "function", None)
                        else ""
                    )
                    tool_call = ChatCompletionMessageToolCall(
                        name=name, arguments=arguments
                    )
                    tool_calls.append(tool_call)
        return tool_calls

    @property
    def get_tools_called(self) -> Iterable[str]:
        """Get the names of the tools called in the response, if any."""
        return [tc.function.name for tc in self.get_tool_calls]


LanguageModelName = TypeAliasType(
    "LanguageModelName",
    Literal[
        "anthropic/claude-3-5-haiku-20241022",
        "anthropic/claude-3-5-haiku-latest",
        "anthropic/claude-3-5-sonnet-20240620",
        "anthropic/claude-3-5-sonnet-20241022",
        "anthropic/claude-3-5-sonnet-latest",
        "anthropic/claude-haiku-4-5",
        "anthropic/claude-haiku-4-5-20251001",
        "anthropic/claude-3-7-sonnet-20250219",
        "anthropic/claude-3-7-sonnet-latest",
        "anthropic/claude-3-haiku-20240307",
        "anthropic/claude-3-opus-20240229",
        "anthropic/claude-3-opus-latest",
        "anthropic/claude-4-opus-20250514",
        "anthropic/claude-4-sonnet-20250514",
        "anthropic/claude-opus-4-0",
        "anthropic/claude-opus-4-1-20250805",
        "anthropic/claude-opus-4-20250514",
        "anthropic/claude-sonnet-4-0",
        "anthropic/claude-sonnet-4-20250514",
        "anthropic/claude-sonnet-4-5",
        "anthropic/claude-sonnet-4-5-20250929",
        "bedrock/amazon.titan-tg1-large",
        "bedrock/amazon.titan-text-lite-v1",
        "bedrock/amazon.titan-text-express-v1",
        "bedrock/us.amazon.nova-pro-v1:0",
        "bedrock/us.amazon.nova-lite-v1:0",
        "bedrock/us.amazon.nova-micro-v1:0",
        "bedrock/anthropic.claude-3-5-sonnet-20241022-v2:0",
        "bedrock/us.anthropic.claude-3-5-sonnet-20241022-v2:0",
        "bedrock/anthropic.claude-3-5-haiku-20241022-v1:0",
        "bedrock/us.anthropic.claude-3-5-haiku-20241022-v1:0",
        "bedrock/anthropic.claude-instant-v1",
        "bedrock/anthropic.claude-v2:1",
        "bedrock/anthropic.claude-v2",
        "bedrock/anthropic.claude-3-sonnet-20240229-v1:0",
        "bedrock/us.anthropic.claude-3-sonnet-20240229-v1:0",
        "bedrock/anthropic.claude-3-haiku-20240307-v1:0",
        "bedrock/us.anthropic.claude-3-haiku-20240307-v1:0",
        "bedrock/anthropic.claude-3-opus-20240229-v1:0",
        "bedrock/us.anthropic.claude-3-opus-20240229-v1:0",
        "bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0",
        "bedrock/us.anthropic.claude-3-5-sonnet-20240620-v1:0",
        "bedrock/anthropic.claude-3-7-sonnet-20250219-v1:0",
        "bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0",
        "bedrock/anthropic.claude-opus-4-20250514-v1:0",
        "bedrock/us.anthropic.claude-opus-4-20250514-v1:0",
        "bedrock/anthropic.claude-sonnet-4-20250514-v1:0",
        "bedrock/us.anthropic.claude-sonnet-4-20250514-v1:0",
        "bedrock/cohere.command-text-v14",
        "bedrock/cohere.command-r-v1:0",
        "bedrock/cohere.command-r-plus-v1:0",
        "bedrock/cohere.command-light-text-v14",
        "bedrock/meta.llama3-8b-instruct-v1:0",
        "bedrock/meta.llama3-70b-instruct-v1:0",
        "bedrock/meta.llama3-1-8b-instruct-v1:0",
        "bedrock/us.meta.llama3-1-8b-instruct-v1:0",
        "bedrock/meta.llama3-1-70b-instruct-v1:0",
        "bedrock/us.meta.llama3-1-70b-instruct-v1:0",
        "bedrock/meta.llama3-1-405b-instruct-v1:0",
        "bedrock/us.meta.llama3-2-11b-instruct-v1:0",
        "bedrock/us.meta.llama3-2-90b-instruct-v1:0",
        "bedrock/us.meta.llama3-2-1b-instruct-v1:0",
        "bedrock/us.meta.llama3-2-3b-instruct-v1:0",
        "bedrock/us.meta.llama3-3-70b-instruct-v1:0",
        "bedrock/mistral.mistral-7b-instruct-v0:2",
        "bedrock/mistral.mixtral-8x7b-instruct-v0:1",
        "bedrock/mistral.mistral-large-2402-v1:0",
        "bedrock/mistral.mistral-large-2407-v1:0",
        "cerebras/gpt-oss-120b",
        "cerebras/llama3.1-8b",
        "cerebras/llama-3.3-70b",
        "cerebras/llama-4-scout-17b-16e-instruct",
        "cerebras/llama-4-maverick-17b-128e-instruct",
        "cerebras/qwen-3-235b-a22b-instruct-2507",
        "cerebras/qwen-3-32b",
        "cerebras/qwen-3-coder-480b",
        "cerebras/qwen-3-235b-a22b-thinking-2507",
        "cohere/c4ai-aya-expanse-32b",
        "cohere/c4ai-aya-expanse-8b",
        "cohere/command-nightly",
        "cohere/command-r-08-2024",
        "cohere/command-r-plus-08-2024",
        "cohere/command-r7b-12-2024",
        "deepseek/deepseek-chat",
        "deepseek/deepseek-reasoner",
        "gemini/gemini-2.0-flash",
        "gemini/gemini-2.0-flash-lite",
        "gemini/gemini-2.5-flash",
        "gemini/gemini-2.5-flash-preview-09-2025",
        "gemini/gemini-flash-latest",
        "gemini/gemini-2.5-flash-lite",
        "gemini/gemini-2.5-flash-lite-preview-09-2025",
        "gemini/gemini-flash-lite-latest",
        "gemini/gemini-2.5-pro",
        "vertex_ai/gemini-2.0-flash",
        "vertex_ai/gemini-2.0-flash-lite",
        "vertex_ai/gemini-2.5-flash",
        "vertex_ai/gemini-2.5-flash-preview-09-2025",
        "vertex_ai/gemini-flash-latest",
        "vertex_ai/gemini-2.5-flash-lite",
        "vertex_ai/gemini-2.5-flash-lite-preview-09-2025",
        "vertex_ai/gemini-flash-lite-latest",
        "vertex_ai/gemini-2.5-pro",
        "xai/grok-4",
        "xai/grok-4-0709",
        "xai/grok-3",
        "xai/grok-3-mini",
        "xai/grok-3-fast",
        "xai/grok-3-mini-fast",
        "xai/grok-2-vision-1212",
        "xai/grok-2-image-1212",
        "groq/distil-whisper-large-v3-en",
        "groq/gemma2-9b-it",
        "groq/llama-3.3-70b-versatile",
        "groq/llama-3.1-8b-instant",
        "groq/llama-guard-3-8b",
        "groq/llama3-70b-8192",
        "groq/llama3-8b-8192",
        "groq/moonshotai/kimi-k2-instruct",
        "groq/whisper-large-v3",
        "groq/whisper-large-v3-turbo",
        "groq/playai-tts",
        "groq/playai-tts-arabic",
        "groq/qwen-qwq-32b",
        "groq/mistral-saba-24b",
        "groq/qwen-2.5-coder-32b",
        "groq/qwen-2.5-32b",
        "groq/deepseek-r1-distill-qwen-32b",
        "groq/deepseek-r1-distill-llama-70b",
        "groq/llama-3.3-70b-specdec",
        "groq/llama-3.2-1b-preview",
        "groq/llama-3.2-3b-preview",
        "groq/llama-3.2-11b-vision-preview",
        "groq/llama-3.2-90b-vision-preview",
        "heroku/claude-3-5-haiku",
        "heroku/claude-3-5-sonnet-latest",
        "heroku/claude-3-7-sonnet",
        "heroku/claude-4-sonnet",
        "heroku/claude-3-haiku",
        "heroku/gpt-oss-120b",
        "heroku/nova-lite",
        "heroku/nova-pro",
        "huggingface/Qwen/QwQ-32B",
        "huggingface/Qwen/Qwen2.5-72B-Instruct",
        "huggingface/Qwen/Qwen3-235B-A22B",
        "huggingface/Qwen/Qwen3-32B",
        "huggingface/deepseek-ai/DeepSeek-R1",
        "huggingface/meta-llama/Llama-3.3-70B-Instruct",
        "huggingface/meta-llama/Llama-4-Maverick-17B-128E-Instruct",
        "huggingface/meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "mistral/codestral-latest",
        "mistral/mistral-large-latest",
        "mistral/mistral-moderation-latest",
        "mistral/mistral-small-latest",
        "moonshotai/moonshot-v1-8k",
        "moonshotai/moonshot-v1-32k",
        "moonshotai/moonshot-v1-128k",
        "moonshotai/moonshot-v1-8k-vision-preview",
        "moonshotai/moonshot-v1-32k-vision-preview",
        "moonshotai/moonshot-v1-128k-vision-preview",
        "moonshotai/kimi-latest",
        "moonshotai/kimi-thinking-preview",
        "moonshotai/kimi-k2-0711-preview",
        "ollama/cogito",
        "ollama/deepcoder",
        "ollama/deepseek-r1",
        "ollama/deepseek-v3.1",
        "ollama/gemma3",
        "ollama/gemma3n",
        "ollama/glm-4.6",
        "ollama/granite3",
        "ollama/granite3.2-vision",
        "ollama/granite3.3",
        "ollama/granite4",
        "ollama/kimi-k2",
        "ollama/llama3.1",
        "ollama/llama3.2",
        "ollama/llama4",
        "ollama/magistral",
        "ollama/minimax-m2",
        "ollama/mistral",
        "ollama/mistral-small3.1",
        "ollama/phi3",
        "ollama/phi4-mini",
        "ollama/qwen2.5",
        "ollama/qwen3",
        "ollama/qwen3-coder",
        "ollama/qwen3-vl",
        "openai/chatgpt-4o-latest",
        "openai/codex-mini-latest",
        "openai/gpt-3.5-turbo",
        "openai/gpt-3.5-turbo-0125",
        "openai/gpt-3.5-turbo-0301",
        "openai/gpt-3.5-turbo-0613",
        "openai/gpt-3.5-turbo-1106",
        "openai/gpt-3.5-turbo-16k",
        "openai/gpt-3.5-turbo-16k-0613",
        "openai/gpt-4",
        "openai/gpt-4-0125-preview",
        "openai/gpt-4-0314",
        "openai/gpt-4-0613",
        "openai/gpt-4-1106-preview",
        "openai/gpt-4-32k",
        "openai/gpt-4-32k-0314",
        "openai/gpt-4-32k-0613",
        "openai/gpt-4-turbo",
        "openai/gpt-4-turbo-2024-04-09",
        "openai/gpt-4-turbo-preview",
        "openai/gpt-4-vision-preview",
        "openai/gpt-4.1",
        "openai/gpt-4.1-2025-04-14",
        "openai/gpt-4.1-mini",
        "openai/gpt-4.1-mini-2025-04-14",
        "openai/gpt-4.1-nano",
        "openai/gpt-4.1-nano-2025-04-14",
        "openai/gpt-4o",
        "openai/gpt-4o-2024-05-13",
        "openai/gpt-4o-2024-08-06",
        "openai/gpt-4o-2024-11-20",
        "openai/gpt-4o-audio-preview",
        "openai/gpt-4o-audio-preview-2024-10-01",
        "openai/gpt-4o-audio-preview-2024-12-17",
        "openai/gpt-4o-audio-preview-2025-06-03",
        "openai/gpt-4o-mini",
        "openai/gpt-4o-mini-2024-07-18",
        "openai/gpt-4o-mini-audio-preview",
        "openai/gpt-4o-mini-audio-preview-2024-12-17",
        "openai/gpt-4o-mini-search-preview",
        "openai/gpt-4o-mini-search-preview-2025-03-11",
        "openai/gpt-4o-search-preview",
        "openai/gpt-4o-search-preview-2025-03-11",
        "openai/gpt-5",
        "openai/gpt-5-2025-08-07",
        "openai/o1",
        "openai/gpt-5-chat-latest",
        "openai/o1-2024-12-17",
        "openai/gpt-5-mini",
        "openai/o1-mini",
        "openai/gpt-5-mini-2025-08-07",
        "openai/o1-mini-2024-09-12",
        "openai/gpt-5-nano",
        "openai/o1-preview",
        "openai/gpt-5-nano-2025-08-07",
        "openai/o1-preview-2024-09-12",
        "openai/o1-pro",
        "openai/o1-pro-2025-03-19",
        "openai/o3",
        "openai/o3-2025-04-16",
        "openai/o3-deep-research",
        "openai/o3-deep-research-2025-06-26",
        "openai/o3-mini",
        "openai/o3-mini-2025-01-31",
        "openai/o4-mini",
        "openai/o4-mini-2025-04-16",
        "openai/o4-mini-deep-research",
        "openai/o4-mini-deep-research-2025-06-26",
        "openai/o3-pro",
        "openai/o3-pro-2025-06-10",
        "openai/computer-use-preview",
        "openai/computer-use-preview-2025-03-11",
    ],
)
"""Known model names for language models. This is just a beautification alias,
rather than something with functional impact."""
