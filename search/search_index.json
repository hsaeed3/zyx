{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"<code>zyx</code>","text":""},{"location":"#an-easier-way-to-work-with-llms","title":"An Easier Way to Work With LLMs","text":"<p><code>zyx</code> is a python package built with the sole purpose of providing an intutive and simple API when working and building with LLMs. It aims to remove the current boilerplate of most LLM libraries, through quick-use functions and modules.</p> <p>The point of <code>zyx</code> is not to be a flashy new library, everything here can be done without much effort, it just helps you do them quicker. <code>zyx</code> is a developer focused library, it's built so your ideas come out easier.</p> <p>The framework is built off of incredibly well-built libraries, most notably:</p> <ul> <li>Instructor</li> <li>LiteLLM</li> <li>Chroma</li> <li>Pydantic</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<p>Install <code>zyx</code> using pip:</p> <pre><code>pip install --upgrade zyx\n</code></pre> <pre><code>import zyx\n\ndef my_favorite_food(name : str) -&gt; str:\n    \"\"\"Returns the user's favorite food.\"\"\"\n    return \"pizza\"\n\nzyx.completion(\n    \"What is my favorite food?\",\n    model = \"gpt-4o-mini\", # Any LiteLLM model is compatible\n    tools = [my_favorite_food], # Any function, basemodel or openai tool\n    run_tools = True # Optional: automatically executes tools\n    # response_model = BaseModel # Optional: Pydantic model for structured outputs\n    # mode = \"tool_call\" # Optional: Instructor completion mode (refer to instructor docs)\n)\n\n# \"Your favorite food is pizza!\"\n</code></pre>"},{"location":"examples/example/","title":"Examples","text":"<p>In this section, we will go over a few examples of the various capabilities of <code>zyx</code>.</p>"},{"location":"examples/agents/llm-as-a-judge/","title":"LLM as a Judge","text":"<p>The <code>judge()</code> function is a multi-use tool, that can be used for:</p> <ul> <li>Hallucination Detection</li> <li>Schema Validation</li> <li>Multi-Response Accuracy</li> <li>Guardrails Enforcement</li> <li>Response Regeneration</li> </ul>"},{"location":"examples/agents/llm-as-a-judge/#using-the-judge-function","title":"Using the <code>judge()</code> function","text":""},{"location":"examples/agents/llm-as-a-judge/#fact-checking","title":"Fact Checking","text":"<p>For most quick fact-checking tasks, the judge function can be utilized without any verifier schema or instructions.</p> <pre><code>import zyx\n\nfact = \"The capital of France is India\"\n\nzyx.judge(\n    fact,\n    process = \"fact_check\",\n    model = \"gpt-4o-mini\"\n)\n</code></pre> <pre><code>FactCheckResult(\n    is_accurate=False,\n    explanation='The statement that the capital of France is India is incorrect. The capital of France is Paris, not India. India is a country\nin South Asia, and it has its own capital, which is New Delhi.',\n    confidence=1.0\n)\n</code></pre>"},{"location":"examples/agents/llm-as-a-judge/#accuracy-judgement","title":"Accuracy Judgement","text":"<p>You can use the <code>judge()</code> function to compare multiple responses to a given prompt and determine which one is the most accurate, helpful, and relevant.</p> <pre><code>import zyx\n\nprompt = \"Explain the theory of relativity\"\nresponses = [\n    \"The theory of relativity describes how space and time are interconnected.\",\n    \"Einstein's theory states that E=mc^2, which relates energy and mass.\"\n]\n\nzyx.judge(\n    prompt,\n    responses=responses,\n    process=\"accuracy\"\n)\n</code></pre> <pre><code>JudgmentResult(\n    explanation='Response 1 provides a broader understanding of the theory of relativity by mentioning the interconnection of space and time,\nwhich is fundamental to both the special and general theories of relativity. Response 2 focuses on the famous equation E=mc^2, which is a key\nresult of the theory but does not provide a comprehensive explanation of the overall theory. Therefore, while both responses are accurate,\nResponse 1 is more helpful and relevant as it captures a core aspect of the theory.',\n    verdict='Response 1 is the most accurate, helpful, and relevant.'\n)\n</code></pre>"},{"location":"examples/agents/llm-as-a-judge/#schema-validation","title":"Schema Validation","text":"<p>The <code>judge()</code> function can also be used to validate responses against a predefined schema or set of criteria.</p> <pre><code>import zyx\n\nprompt = \"Describe the water cycle\"\nresponse = \"Water evaporates, forms clouds, and then falls as rain.\"\nschema = \"The response should include: 1) Evaporation, 2) Condensation, 3) Precipitation, 4) Collection\"\n\nresult = zyx.judge(\n    prompt,\n    responses=response,\n    process=\"validate\",\n    schema=schema\n)\n\nprint(result)\n</code></pre> <pre><code>ValidationResult(\n    is_valid=False,\n    explanation='The response does not include all required components of the water cycle as outlined in the schema. Specifically, it mentions\nevaporation and precipitation, but it fails to mention condensation and collection.'\n)\n</code></pre>"},{"location":"examples/agents/llm-as-a-judge/#response-regeneration","title":"Response Regeneration","text":"<p>An important functionality of this module is the ability to regenerate a correct response if the original one was determined to be inaccurate or incomplete. This can be useful for generating high-quality responses for a given prompt based on the schema or instructions provided.</p> <pre><code>import zyx\n\nprompt = \"Explain photosynthesis\"\nresponses = [\n    \"Photosynthesis is how plants make food.\",\n    \"Plants use sunlight to convert CO2 and water into glucose and oxygen.\"\n]\n\nregenerated_response = zyx.judge(\n    prompt,\n    responses=responses,\n    process=\"accuracy\",\n    regenerate=True,\n    verbose=True\n)\n\nprint(regenerated_response)\n</code></pre> <pre><code>[09/29/24 00:42:45] INFO     judge - judge - Judging responses for prompt: Explain photosynthesis\n...\n[09/29/24 00:42:47] WARNING  judge - judge - Response is not accurate. Regenerating response.\n...\nRegeneratedResponse(\n    response='Photosynthesis is a biochemical process used by plants...'\n)\n</code></pre>"},{"location":"examples/agents/llm-as-a-judge/#guardrails","title":"Guardrails","text":"<p>The final big functionality of the <code>judge()</code> function is the ability to enforce guardrails on the responses generated. This can help ensure that the responses are accurate, relevant, and appropriate for the given prompt. If a response violates guardrails, it will always be regenerated.</p> <pre><code>import zyx\n\nprompt = \"Describe the benefits of exercise\"\nresponses = [\"Exercise helps you lose weight and build muscle.\"]\nguardrails = [\n    \"Ensure the response mentions mental health benefits.\",\n    \"Include at least three distinct benefits of exercise.\",\n    \"Avoid focusing solely on physical appearance.\"\n]\n\nresult = zyx.judge(\n    prompt,\n    responses=responses,\n    process=\"accuracy\",\n    guardrails=guardrails,\n    verbose=True\n)\n\nprint(result)\n</code></pre> <pre><code>[09/29/24 00:50:30] INFO     judge - judge - Judging responses for prompt: Describe the benefits of exercise\n...\n[09/29/24 00:50:33] WARNING  judge - judge - Response violates guardrails. Regenerating response.\n...\nRegeneratedResponse(\n    response=\"Exercise offers a multitude of benefits that extend beyond...\"\n)\n</code></pre>"},{"location":"examples/agents/llm-as-a-judge/#api-reference","title":"API Reference","text":"<p>Judge responses based on accuracy, validate against a schema, or fact-check a single response, with an option to regenerate an optimized response.</p> Example <pre><code>&gt;&gt;&gt; judge(\n    prompt=\"Explain the concept of quantum entanglement.\",\n    responses=[\n        \"Quantum entanglement is a phenomenon where two particles become interconnected and their quantum states cannot be described independently.\",\n        \"Quantum entanglement is when particles are really close to each other and move in the same way.\"\n    ],\n    process=\"accuracy\",\n    verbose=True\n)\n\nAccuracy Judgment:\nExplanation: The first response is more accurate as it provides a clear definition of quantum entanglement.\nVerdict: The first response is the most accurate.\n\nValidation Result:\nIs Valid: True\nExplanation: The response adheres to the provided schema.\n\nFact-Check Result:\nIs Accurate: True\nExplanation: The response accurately reflects the fact that quantum entanglement occurs when two particles are separated by a large distance but still instantaneously affect each other's quantum states.\nConfidence: 0.95\n\nRegenerated Response:\nResponse: Quantum entanglement is a phenomenon where two particles become interconnected and their quantum states cannot be described independently.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The original prompt or question.</p> required <code>responses</code> <code>List[str]</code> <p>List of responses to judge, validate, or fact-check.</p> <code>None</code> <code>process</code> <code>Literal['accuracy', 'validate', 'fact_check']</code> <p>The type of verification to perform.</p> <code>'accuracy'</code> <code>schema</code> <code>Optional[Union[str, dict]]</code> <p>Schema for validation or fact-checking (optional for fact_check).</p> <code>None</code> <code>regenerate</code> <code>bool</code> <p>Whether to regenerate an optimized response.</p> <code>False</code> <code>model</code> <code>str</code> <p>The model to use for judgment.</p> <code>'gpt-4o-mini'</code> <code>api_key</code> <code>Optional[str]</code> <p>API key for the LLM service.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>Base URL for the LLM service.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Temperature for response generation.</p> <code>0.7</code> <code>mode</code> <code>InstructorMode</code> <p>Mode for the instructor.</p> <code>'markdown_json_mode'</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries for API calls.</p> <code>3</code> <code>organization</code> <code>Optional[str]</code> <p>Organization for the LLM service.</p> <code>None</code> <code>client</code> <code>Optional[Literal['openai', 'litellm']]</code> <p>Client to use for API calls.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to log verbose output.</p> <code>False</code> <code>guardrails</code> <code>Optional[Union[str, List[str]]]</code> <p>Guardrails for content moderation.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[JudgmentResult, ValidationResult, RegeneratedResponse, FactCheckResult]</code> <p>Union[JudgmentResult, ValidationResult, RegeneratedResponse, FactCheckResult]: The result of the judgment, validation, fact-check, or regeneration.</p> Source code in <code>zyx/resources/completions/agents/judge.py</code> <pre><code>def judge(\n    prompt: str,\n    responses: Optional[Union[List[str], str]] = None,\n    process: Literal[\"accuracy\", \"validate\", \"fact_check\"] = \"accuracy\",\n    schema: Optional[Union[str, dict]] = None,\n    regenerate: bool = False,\n    model: str = \"gpt-4o-mini\",\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    temperature: float = 0.7,\n    mode: InstructorMode = \"markdown_json_mode\",\n    max_retries: int = 3,\n    organization: Optional[str] = None,\n    client: Optional[Literal[\"openai\", \"litellm\"]] = None,\n    verbose: bool = False,\n    guardrails: Optional[Union[str, List[str]]] = None\n) -&gt; Union[JudgmentResult, ValidationResult, RegeneratedResponse, FactCheckResult]:\n    \"\"\"\n    Judge responses based on accuracy, validate against a schema, or fact-check a single response,\n    with an option to regenerate an optimized response.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; judge(\n            prompt=\"Explain the concept of quantum entanglement.\",\n            responses=[\n                \"Quantum entanglement is a phenomenon where two particles become interconnected and their quantum states cannot be described independently.\",\n                \"Quantum entanglement is when particles are really close to each other and move in the same way.\"\n            ],\n            process=\"accuracy\",\n            verbose=True\n        )\n\n        Accuracy Judgment:\n        Explanation: The first response is more accurate as it provides a clear definition of quantum entanglement.\n        Verdict: The first response is the most accurate.\n\n        Validation Result:\n        Is Valid: True\n        Explanation: The response adheres to the provided schema.\n\n        Fact-Check Result:\n        Is Accurate: True\n        Explanation: The response accurately reflects the fact that quantum entanglement occurs when two particles are separated by a large distance but still instantaneously affect each other's quantum states.\n        Confidence: 0.95\n\n        Regenerated Response:\n        Response: Quantum entanglement is a phenomenon where two particles become interconnected and their quantum states cannot be described independently.\n        ```\n\n    Args:\n        prompt (str): The original prompt or question.\n        responses (List[str]): List of responses to judge, validate, or fact-check.\n        process (Literal[\"accuracy\", \"validate\", \"fact_check\"]): The type of verification to perform.\n        schema (Optional[Union[str, dict]]): Schema for validation or fact-checking (optional for fact_check).\n        regenerate (bool): Whether to regenerate an optimized response.\n        model (str): The model to use for judgment.\n        api_key (Optional[str]): API key for the LLM service.\n        base_url (Optional[str]): Base URL for the LLM service.\n        temperature (float): Temperature for response generation.\n        mode (InstructorMode): Mode for the instructor.\n        max_retries (int): Maximum number of retries for API calls.\n        organization (Optional[str]): Organization for the LLM service.\n        client (Optional[Literal[\"openai\", \"litellm\"]]): Client to use for API calls.\n        verbose (bool): Whether to log verbose output.\n        guardrails (Optional[Union[str, List[str]]]): Guardrails for content moderation.\n\n    Returns:\n        Union[JudgmentResult, ValidationResult, RegeneratedResponse, FactCheckResult]: The result of the judgment, validation, fact-check, or regeneration.\n    \"\"\"\n    if verbose:\n        logger.info(f\"Judging responses for prompt: {prompt}\")\n        logger.info(f\"process: {process}\")\n        logger.info(f\"Regenerate: {regenerate}\")\n\n    if isinstance(responses, str):\n        responses = [responses]\n\n    completion_client = Client(\n        api_key=api_key,\n        base_url=base_url,\n        organization=organization,\n        provider=client,\n        verbose=verbose\n    )\n\n    if process == \"accuracy\":\n        system_message = (\n            \"You are an impartial judge evaluating responses to a given prompt. \"\n            \"Compare the responses and determine which one is the most accurate, helpful, and relevant. \"\n            \"Provide a brief explanation for your decision and then state your verdict.\"\n        )\n        user_message = f\"Prompt: {prompt}\\n\\nResponses:\\n\"\n        for idx, response in enumerate(responses, 1):\n            user_message += f\"{idx}. {response}\\n\\n\"\n\n        result = completion_client.completion(\n            messages=[\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": user_message}\n            ],\n            model=model,\n            response_model=JudgmentResult,\n            mode=mode,\n            max_retries=max_retries,\n            temperature=temperature,\n        )\n\n        if regenerate:\n\n            if verbose:\n                logger.warning(f\"Response is not accurate. Regenerating response.\")\n\n            system_message = (\n                \"Based on the judgment provided, generate an optimized response \"\n                \"that addresses the prompt more effectively than the original responses.\"\n            )\n            user_message = f\"Original prompt: {prompt}\\n\\nJudgment: {result.explanation}\\n\\nGenerate an optimized response:\"\n\n            regenerated = completion_client.completion(\n                messages=[\n                    {\"role\": \"system\", \"content\": system_message},\n                    {\"role\": \"user\", \"content\": user_message}\n                ],\n                model=model,\n                response_model=RegeneratedResponse,\n                mode=mode,\n                max_retries=max_retries,\n                temperature=temperature,\n            )\n            result = regenerated\n\n    elif process == \"validate\":\n        if not schema:\n            raise ValueError(\"Schema is required for validation.\")\n\n        system_message = (\n            \"You are a validation expert. Your task is to determine if the given response \"\n            \"matches the provided schema or instructions. Provide a detailed explanation \"\n            \"of your validation process and state whether the response is valid or not.\"\n        )\n        user_message = f\"Prompt: {prompt}\\n\\nResponse: {responses[0]}\\n\\nSchema/Instructions: {schema}\"\n\n        result = completion_client.completion(\n            messages=[\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": user_message}\n            ],\n            model=model,\n            response_model=ValidationResult,\n            mode=mode,\n            max_retries=max_retries,\n            temperature=temperature,\n        )\n\n        if regenerate and not result.is_valid:\n\n            if verbose:\n                logger.warning(f\"Response is not valid. Regenerating response.\")\n\n            system_message = (\n                \"Based on the validation result, generate a new response that \"\n                \"correctly adheres to the given schema or instructions.\"\n            )\n            user_message = f\"Original prompt: {prompt}\\n\\nSchema/Instructions: {schema}\\n\\nGenerate a valid response:\"\n\n            regenerated = completion_client.completion(\n                messages=[\n                    {\"role\": \"system\", \"content\": system_message},\n                    {\"role\": \"user\", \"content\": user_message}\n                ],\n                model=model,\n                response_model=RegeneratedResponse,\n                mode=mode,\n                max_retries=max_retries,\n                temperature=temperature,\n            )\n            result = regenerated\n\n    elif process == \"fact_check\":\n        if responses is None:\n            responses = [prompt]  # Use the prompt as the response for fact-checking\n        elif len(responses) != 1:\n            raise ValueError(\"Fact-check requires exactly one response.\")\n\n        system_message = (\n            \"You are a fact-checking expert. Your task is to determine if the given response \"\n            \"is accurate based on the prompt and your knowledge. Provide a detailed explanation \"\n            \"of your fact-checking process, state whether the response is accurate or not, \"\n            \"and provide a confidence score between 0.0 and 1.0.\"\n        )\n        user_message = f\"Prompt: {prompt}\\n\\nResponse to fact-check: {responses[0]}\"\n        if schema:\n            user_message += f\"\\n\\nAdditional fact-checking guidelines: {schema}\"\n\n        result = completion_client.completion(\n            messages=[\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": user_message}\n            ],\n            model=model,\n            response_model=FactCheckResult,\n            mode=mode,\n            max_retries=max_retries,\n            temperature=temperature,\n        )\n\n        if regenerate and not result.is_accurate:\n\n            if verbose:\n                logger.warning(f\"Response is not accurate. Regenerating response.\")\n\n            system_message = (\n                \"Based on the fact-check result, generate a new response that \"\n                \"is accurate and addresses the original prompt correctly.\"\n            )\n            user_message = f\"Original prompt: {prompt}\\n\\nFact-check result: {result.explanation}\\n\\nGenerate an accurate response:\"\n\n            regenerated = completion_client.completion(\n                messages=[\n                    {\"role\": \"system\", \"content\": system_message},\n                    {\"role\": \"user\", \"content\": user_message}\n                ],\n                model=model,\n                response_model=RegeneratedResponse,\n                mode=mode,\n                max_retries=max_retries,\n                temperature=temperature,\n            )\n            result = regenerated\n\n    else:\n        raise ValueError(\"Invalid process. Choose 'accuracy', 'validate', or 'fact_check'.\")\n\n    # Add guardrails check after the main process\n    if guardrails:\n        guardrails_result = check_guardrails(prompt, result, guardrails, completion_client, model, mode, max_retries, temperature, verbose)\n        if not guardrails_result.passed:\n            if verbose:\n                logger.warning(f\"Response violates guardrails. Regenerating response.\")\n            result = regenerate_response(prompt, guardrails_result.explanation, completion_client, model, mode, max_retries, temperature)\n\n    return result\n</code></pre>"},{"location":"examples/agents/planning/","title":"Planning","text":"<p>A big piece in chain of thought reasoning is planning. The <code>zyx</code> library comes with a <code>plan()</code> function, allowing helpful utility for planning for agentic tasks.</p>"},{"location":"examples/agents/planning/#tree-of-thought-planning","title":"Tree of Thought Planning","text":"<p>Paper</p> <pre><code>from zyx import plan\nfrom pydantic import BaseModel\nfrom typing import List\n\ngoal = \"Create a marketing strategy for a new smartphone\"\ntot_plan = plan(\n    goal,\n    steps=5,\n    verbose = True\n)\n\nprint(tot_plan)\n</code></pre> Output <pre><code>Plan(\n    tasks=[\n        Task(description='### Actionable Tasks for Integrated Multi-Channel Marketing Campaign', details=None),\n        Task(description='1. **Develop a Comprehensive Marketing Plan:**', details=None),\n        Task(\n            description='- **Define Objectives:** Establish clear goals for the campaign, such as increasing brand awareness, driving traffic\nto the website, or achieving a specific sales target.',\n            details=None\n        ),\n        Task(\n            description='- **Identify Target Audience:** Conduct research to define the target audience segments, including demographics,\ninterests, and behaviors.',\n            details=None\n        ),\n        Task(\n            description='- **Budget Allocation:** Determine the budget for each channel (social media, email, PPC, etc.) and allocate\nresources accordingly.',\n            details=None\n        ),\n        Task(\n            description=\"- **Key Messages:** Craft core messages that resonate with the target audience and align with the brand's voice.\",\n            details=None\n        ),\n        Task(\n            description='- **Timeline:** Create a timeline for the campaign, including key milestones and deadlines for each phase of the\nmarketing activities.',\n            details=None\n        ),\n        Task(description='2. **Create Engaging Content:**', details=None),\n        Task(\n            description='- **Content Calendar:** Develop a content calendar that outlines what content will be published on which channels and\nwhen.',\n            details=None\n        ),\n        Task(description='- **Tailored Content Creation:** Produce high-quality content tailored for each platform, such as:', details=None),\n        Task(\n            description='- **Social Media Posts:** Eye-catching graphics and engaging captions for platforms like Instagram, Facebook, and\nTwitter.',\n            details=None\n        ),\n        Task(\n            description=\"- **Email Newsletters:** Informative and visually appealing emails that highlight the smartphone's features and\npromotions.\",\n            details=None\n        ),\n        Task(\n            description=\"- **Blog Articles:** In-depth articles that provide insights into the smartphone's technology, benefits, and user\nexperiences.\",\n            details=None\n        ),\n        Task(\n            description=\"- **Video Ads:** Create short, engaging videos showcasing the smartphone's features and user testimonials.\",\n            details=None\n        ),\n        Task(description='3. **Leverage Influencer Partnerships:**', details=None),\n        Task(\n            description=\"- **Identify Influencers:** Research and compile a list of influencers who align with the brand's values and have a\nfollowing that matches the target audience.\",\n            details=None\n        ),\n        Task(\n            description='- **Outreach Strategy:** Develop a strategy for reaching out to influencers, including personalized messages and\ncollaboration proposals.',\n            details=None\n        ),\n        Task(\n            description='- **Content Collaboration:** Work with influencers to create authentic content that showcases the smartphone, such as\nunboxing videos, reviews, or lifestyle posts.',\n            details=None\n        ),\n        Task(\n            description='- **Track Engagement:** Monitor the performance of influencer content to assess reach, engagement, and conversion\nrates.',\n            details=None\n        ),\n        Task(description='4. **Implement Tracking and Analytics:**', details=None),\n        Task(\n            description='- **Set Up Tracking Tools:** Utilize tools like Google Analytics, social media insights, and email marketing\nanalytics to track campaign performance.',\n            details=None\n        ),\n        Task(\n            description='- **Define KPIs:** Establish key performance indicators (KPIs) to measure success, such as website traffic,\nconversion rates, and social media engagement.',\n            details=None\n        ),\n        Task(\n            description='- **Real-Time Monitoring:** Implement real-time monitoring to assess the effectiveness of each channel and make\nadjustments as needed.',\n            details=None\n        ),\n        Task(\n            description='- **Post-Campaign Analysis:** After the campaign, conduct a thorough analysis of the data to evaluate what worked\nwell and what can be improved for future campaigns.',\n            details=None\n        ),\n        Task(description='5. **Launch and Promote the Campaign:**', details=None),\n        Task(\n            description='- **Coordinated Launch:** Ensure that all channels are prepared for the launch, with content scheduled and ready to\ngo live simultaneously.',\n            details=None\n        ),\n        Task(\n            description='- **Engagement Strategies:** Implement strategies to engage the audience during the launch, such as live Q&amp;A\nsessions, giveaways, or contests.',\n            details=None\n        ),\n        Task(\n            description=\"- **Consistent Messaging:** Maintain consistent messaging across all channels to reinforce the campaign's key\nmessages and brand identity.\",\n            details=None\n        ),\n        Task(\n            description='- **Follow-Up Promotions:** Plan follow-up promotions or content to sustain engagement and interest after the initial\nlaunch.',\n            details=None\n        )\n    ]\n)\n</code></pre>"},{"location":"examples/agents/planning/#planning-with-a-custom-basemodel-input","title":"Planning with a Custom BaseModel Input","text":"<pre><code>import zyx\nfrom pydantic import BaseModel\nfrom typing import List\n\nclass ResearchTask(BaseModel):\n    topic: str\n    resources: List[str]\n    estimated_time: int\n\nplan_model_tot = zyx.plan(\n    ResearchTask,\n    steps=4,\n    verbose=True\n)\n\nprint(plan_model_tot)\n</code></pre> Output <pre><code>ResearchTaskPlan(\n    tasks=[\n        ResearchTask(\n            topic='Understanding Machine Learning',\n            resources=[\n                \"Textbook: 'Pattern Recognition and Machine Learning'\",\n                \"Online Course: 'Machine Learning by Stanford'\",\n                \"Research Paper: 'A survey of machine learning'\"\n            ],\n            estimated_time=12\n        ),\n        ResearchTask(\n            topic='Exploring Neural Networks',\n            resources=[\n                \"Textbook: 'Deep Learning' by Ian Goodfellow\",\n                'Video Lecture Series on YouTube',\n                \"Research Paper: 'Neural Networks for Machine Learning'\"\n            ],\n            estimated_time=15\n        ),\n        ResearchTask(\n            topic='Data Preprocessing Techniques',\n            resources=[\n                \"Online Article: 'A Comprehensive Guide to Data Preprocessing'\",\n                'Video Tutorial on Data Cleaning',\n                \"Research Paper: 'Data Preprocessing for Machine Learning'\"\n            ],\n            estimated_time=8\n        ),\n        ResearchTask(\n            topic='Evaluation Metrics in Machine Learning',\n            resources=[\n                \"Online Course: 'Evaluation Metrics for ML Models'\",\n                \"Textbook: 'The Elements of Statistical Learning'\",\n                \"Research Paper: 'Statistical Methods for Evaluating Learning Algorithms'\"\n            ],\n            estimated_time=10\n        )\n    ]\n)\n</code></pre>"},{"location":"examples/agents/planning/#api-reference","title":"API Reference","text":"<p>Generates a plan or batch of plans based on the input using the Tree of Thoughts method.</p> Example <pre><code>&gt;&gt;&gt; plan(\n    input=\"Create a marketing strategy for a new smartphone\",\n    steps=5,\n    verbose=True\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Union[str, Type[BaseModel]]</code> <p>The input can be either a string describing the task or a Pydantic model class.</p> required <code>instructions</code> <code>Optional[str]</code> <p>Additional instructions for the planning process.</p> <code>None</code> <code>process</code> <code>Literal['single', 'batch']</code> <p>Process can be either \"single\" or \"batch\".</p> <code>'single'</code> <code>n</code> <code>int</code> <p>Number of plans to generate.</p> <code>1</code> <code>batch_size</code> <code>int</code> <p>Number of plans to generate in a single batch.</p> <code>3</code> <code>steps</code> <code>int</code> <p>Number of steps per plan.</p> <code>5</code> <code>model</code> <code>str</code> <p>The model to use for the planning process.</p> <code>'gpt-4o-mini'</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key to use for the planning process.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>The base URL to use for the planning process.</p> <code>None</code> <code>organization</code> <code>Optional[str]</code> <p>The organization to use for the planning process.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>The temperature to use for the planning process.</p> <code>None</code> <code>mode</code> <code>InstructorMode</code> <p>The mode to use for the planning process.</p> <code>'markdown_json_mode'</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries to use for the planning process.</p> <code>3</code> <code>client</code> <code>Optional[Literal['openai', 'litellm']]</code> <p>The client to use for the planning process.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print the planning process to the console.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Plan, List[Plan], Any, List[Any]]</code> <p>Union[Plan, List[Plan], Any, List[Any]]: The plan or batch of plans.</p> Source code in <code>zyx/resources/completions/agents/plan.py</code> <pre><code>def plan(\n    input: Union[str, Type[BaseModel]],\n    instructions: Optional[str] = None,\n    process: Literal[\"single\", \"batch\"] = \"single\",\n    n: int = 1,\n    batch_size: int = 3,\n    steps: int = 5,\n    model: str = \"gpt-4o-mini\",\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    organization: Optional[str] = None,\n    temperature: Optional[float] = None,\n    mode: InstructorMode = \"markdown_json_mode\",\n    max_retries: int = 3,\n    client: Optional[Literal[\"openai\", \"litellm\"]] = None,\n    verbose: bool = False,\n) -&gt; Union[Plan, List[Plan], Any, List[Any]]:\n    \"\"\"\n    Generates a plan or batch of plans based on the input using the Tree of Thoughts method.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; plan(\n            input=\"Create a marketing strategy for a new smartphone\",\n            steps=5,\n            verbose=True\n        )\n        ```\n\n    Args:\n        input (Union[str, Type[BaseModel]]): The input can be either a string describing the task or a Pydantic model class.\n        instructions (Optional[str]): Additional instructions for the planning process.\n        process (Literal[\"single\", \"batch\"]): Process can be either \"single\" or \"batch\".\n        n (int): Number of plans to generate.\n        batch_size (int): Number of plans to generate in a single batch.\n        steps (int): Number of steps per plan.\n        model (str): The model to use for the planning process.\n        api_key (Optional[str]): The API key to use for the planning process.\n        base_url (Optional[str]): The base URL to use for the planning process.\n        organization (Optional[str]): The organization to use for the planning process.\n        temperature (Optional[float]): The temperature to use for the planning process.\n        mode (InstructorMode): The mode to use for the planning process.\n        max_retries (int): The maximum number of retries to use for the planning process.\n        client (Optional[Literal[\"openai\", \"litellm\"]]): The client to use for the planning process.\n        verbose (bool): Whether to print the planning process to the console.\n\n    Returns:\n        Union[Plan, List[Plan], Any, List[Any]]: The plan or batch of plans.\n    \"\"\"\n\n    if verbose:\n        logger.info(f\"Generating {n} plan(s) using Tree of Thoughts method\")\n        logger.info(f\"Using model: {model}\")\n        logger.info(f\"Number of steps per plan: {steps}\")\n        logger.info(f\"Process: {process}\")\n        logger.info(f\"Batch size: {batch_size}\")\n\n    completion_client = Client(\n        api_key=api_key,\n        base_url=base_url,\n        organization=organization,\n        provider=client,\n        verbose=verbose\n    )\n\n    if isinstance(input, str):\n        system_message = _get_string_system_message(input, steps)\n        response_model = Plan\n    elif isinstance(input, type) and issubclass(input, BaseModel):\n        system_message = _get_model_system_message(input, steps)\n        response_model = create_model(f\"{input.__name__}Plan\", tasks=(List[input], ...))\n    else:\n        raise ValueError(\"Input must be either a string or a Pydantic model class.\")\n\n    user_message = instructions if instructions else f\"Generate a plan with {steps} steps.\"\n\n    if process == \"single\" or n == 1:\n        result = completion_client.completion(\n            messages=[\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": user_message}\n            ],\n            model=model,\n            response_model=response_model,\n            mode=mode,\n            max_retries=max_retries,\n            temperature=temperature,\n        )\n        return result\n    else:  # batch process\n        batch_response_model = create_model(\"ResponseModel\", items=(List[response_model], ...))\n        results = []\n        for i in range(0, n, batch_size):\n            batch_n = min(batch_size, n - i)\n            batch_message = f\"Generate {batch_n} plans, each with {steps} steps.\"\n            if results:\n                batch_message += f\"\\nPreviously generated plans: {results[-3:]}\\nEnsure these new plans are different.\"\n\n            result = completion_client.completion(\n                messages=[\n                    {\"role\": \"system\", \"content\": system_message},\n                    {\"role\": \"user\", \"content\": batch_message}\n                ],\n                model=model,\n                response_model=batch_response_model,\n                mode=mode,\n                max_retries=max_retries,\n                temperature=temperature,\n            )\n\n            results.extend(result.items)\n\n        return results\n</code></pre>"},{"location":"examples/agents/web-scraping/","title":"Web Scraping","text":"<p><code>zyx</code> comes with a quick way to scrape the web for information using agentic reasoning through LLMs with the <code>scrape()</code> function.</p> <p>Node-based scraping coming soon for more complex scraping tasks.</p>"},{"location":"examples/agents/web-scraping/#simple-scraping","title":"Simple Scraping","text":"<pre><code>from zyx import scrape\n\nresult = scrape(\n    \"The latest &amp; hottest AI hardware\",\n    model = \"openai/gpt-4o\"\n    workers = 5,\n    max_results = 3\n)\n\nprint(result)\n</code></pre> Output <pre><code>...\n'summary': \"The AI hardware market has seen rapid advancements and fierce competition, with several key players releasing\ninnovative products to meet the growing demand for AI capabilities. Here are the most notable companies and their contributions to AI hardware\nas of 2024:\\n\\n1. **Nvidia**: A leader in the AI hardware space, Nvidia's chips like the A100 and H100 are critical for data centers. The\nrecent introduction of the H200 and B200 chips, along with the Grace Hopper superchip, emphasizes Nvidia's focus on performance and\nscalability in AI applications.\\n\\n2. **AMD**: AMD continues to compete with Nvidia, having launched its MI300 series of AI chips, which rival\nNvidia's offerings in terms of memory capacity and bandwidth. The new Zen 5 CPU microarchitecture enhances AMD's capabilities in AI\nworkloads.\\n\\n3. **Intel**: Intel has introduced its Xeon 6 processors and the Gaudi 3 AI accelerator, which aims to improve processing\nefficiency. Intel's longstanding presence in the CPU market is now complemented by its focus on AI-specific hardware.\\n\\n4. **Alphabet\n(Google)**: With its Cloud TPU v5p and the recently announced Trillium TPU, Alphabet is committed to developing powerful AI chips tailored for\nlarge-scale machine learning tasks.\\n\\n5. **Amazon Web Services (AWS)**: AWS has shifted towards chip production with its Trainium and\nInferentia chips, designed for training and deploying machine learning models, respectively. Their latest instance types offer significant\nimprovements in memory and processing power.\\n\\n6. **Cerebras Systems**: Known for its wafer-scale engine, the WSE-3, Cerebras has achieved\nremarkable performance with its massive core count and memory bandwidth, making it a strong contender in the AI hardware market.\\n\\n7.\n**IBM**: IBM's AI Unit and the upcoming NorthPole chip focus on energy efficiency and performance improvements, aiming to compete with\nexisting AI processors.\\n\\n8. **Qualcomm**: Although newer to the AI hardware scene, Qualcomm's Cloud AI 100 chip has shown competitive\nperformance against Nvidia, particularly in data center applications.\\n\\n9. **Tenstorrent**: Founded by a former AMD architect, Tenstorrent\nfocuses on scalable AI hardware solutions, including its Wormhole processors.\\n\\n10. **Emerging Startups**: Companies like Groq, SambaNova\nSystems, and Mythic are also making strides in the AI hardware space, offering specialized solutions for AI workloads.\\n\\nIn summary, the\ncompetitive landscape for AI hardware is characterized by rapid innovation, with established tech giants and emerging startups alike vying to\ncreate the most powerful and efficient AI chips. This ongoing evolution is driven by the increasing demands of AI applications, particularly\nin data centers and for large-scale machine learning models.\",\n  'evaluation': {\n      'is_successful': True,\n      'explanation': 'The summary effectively captures the current landscape of AI hardware as of 2024, highlighting key players and\ntheir contributions. It provides relevant details about the advancements made by major companies like Nvidia, AMD, Intel, and others, which\ndirectly relates to the query about the latest and hottest AI hardware. The structure is clear, listing companies and their innovations,\nmaking it easy for readers to understand the competitive dynamics in the AI hardware market. Overall, the summary is comprehensive, relevant,\nand well-organized, making it a successful response to the query.',\n      'content': None\n  }\n}\n},\nmessages=[]\n)\n</code></pre>"},{"location":"examples/agents/web-scraping/#api-reference","title":"API Reference","text":"<p>Scrapes the web for topics &amp; content about a query, generates a well-written summary, and returns a Document object.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query.</p> required <code>max_results</code> <code>Optional[int]</code> <p>Maximum number of search results to process.</p> <code>5</code> <code>workers</code> <code>int</code> <p>Number of worker threads to use.</p> <code>5</code> <code>model</code> <code>str</code> <p>The model to use for completion.</p> <code>'gpt-4o-mini'</code> <code>client</code> <code>Literal['openai', 'litellm']</code> <p>The client to use for completion.</p> <code>'openai'</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key to use for completion.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>The base URL to use for completion.</p> <code>None</code> <code>mode</code> <code>InstructorMode</code> <p>The mode to use for completion.</p> <code>'tool_call'</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries to use for completion.</p> <code>3</code> <code>temperature</code> <code>float</code> <p>The temperature to use for completion.</p> <code>0.5</code> <code>run_tools</code> <code>Optional[bool]</code> <p>Whether to run tools for completion.</p> <code>False</code> <code>tools</code> <code>Optional[List[ToolType]]</code> <p>The tools to use for completion.</p> <code>None</code> <p>Returns:</p> Type Description <code>Document</code> <p>A Document object containing the summary and metadata.</p> Source code in <code>zyx/resources/completions/agents/scrape.py</code> <pre><code>def scrape(\n    query: str,\n    max_results: Optional[int] = 5,\n    workers: int = 5,\n    model: str = \"gpt-4o-mini\",\n    client: Literal[\"openai\", \"litellm\"] = \"openai\",\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    mode: InstructorMode = \"tool_call\",\n    max_retries: int = 3,\n    temperature: float = 0.5,\n    run_tools: Optional[bool] = False,\n    tools: Optional[List[ToolType]] = None,\n    parallel_tool_calls: Optional[bool] = False,\n    tool_choice: Optional[Literal[\"none\", \"auto\", \"required\"]] = \"auto\",\n    verbose: Optional[bool] = False,\n    **kwargs,\n) -&gt; Document:\n    \"\"\"\n    Scrapes the web for topics &amp; content about a query, generates a well-written summary, and returns a Document object.\n\n    Parameters:\n        query: The search query.\n        max_results: Maximum number of search results to process.\n        workers: Number of worker threads to use.\n        model: The model to use for completion.\n        client: The client to use for completion.\n        api_key: The API key to use for completion.\n        base_url: The base URL to use for completion.\n        mode: The mode to use for completion.\n        max_retries: The maximum number of retries to use for completion.\n        temperature: The temperature to use for completion.\n        run_tools: Whether to run tools for completion.\n        tools: The tools to use for completion.\n\n\n    Returns:\n        A Document object containing the summary and metadata.\n    \"\"\"\n    import threading\n    from bs4 import BeautifulSoup\n\n    client = Client(\n        api_key = api_key,\n        base_url = base_url,\n        provider = client,\n        verbose = verbose\n    )\n\n    workflow = ScrapeWorkflow(query=query)\n\n    if verbose:\n        print(f\"Starting scrape for query: {query}\")\n\n    # Step 1: Use web_search() to get search results\n    workflow.current_step = ScrapingStep.SEARCH\n    search_results = web_search(query, max_results=max_results)\n    workflow.search_results = search_results\n    urls = [result[\"href\"] for result in search_results if \"href\" in result]\n\n    if verbose:\n        print(f\"Found {len(urls)} URLs\")\n\n    # Step 2: Define a function to fetch and parse content from a URL\n    def fetch_content(url: str) -&gt; str:\n        try:\n            headers = {\"User-Agent\": \"Mozilla/5.0\"}\n            response = requests.get(url, headers=headers, timeout=10)\n            response.raise_for_status()\n            soup = BeautifulSoup(response.content, \"html.parser\")\n            # Extract text content from the page\n            # You might want to refine this to get more meaningful content\n            texts = soup.find_all(text=True)\n            visible_texts = filter(tag_visible, texts)\n            content = \" \".join(t.strip() for t in visible_texts)\n            return content\n        except Exception as e:\n            if verbose:\n                print(f\"Error fetching {url}: {e}\")\n            return \"\"\n\n    # Helper function to filter visible text\n    from bs4.element import Comment\n\n    def tag_visible(element):\n        if element.parent.name in [\n            \"style\",\n            \"script\",\n            \"head\",\n            \"title\",\n            \"meta\",\n            \"[document]\",\n        ]:\n            return False\n        if isinstance(element, Comment):\n            return False\n        return True\n\n    # Step 3: Use ThreadPoolExecutor to fetch content in parallel\n    workflow.current_step = ScrapingStep.FETCH\n    contents = []\n    with ThreadPoolExecutor(max_workers=workers) as executor:\n        future_to_url = {executor.submit(fetch_content, url): url for url in urls}\n        for future in future_to_url:\n            content = future.result()\n            if content:\n                contents.append(content)\n\n    workflow.fetched_contents = contents\n\n    if verbose:\n        print(f\"Collected content from {len(contents)} pages\")\n\n    # Step 4: Combine the content\n    workflow.current_step = ScrapingStep.SUMMARIZE\n    combined_content = \"\\n\\n\".join(contents)\n\n    # Optionally, you can chunk the content if it's too large\n    # For now, we'll assume it's manageable\n\n    # Step 5: Use the completion function to generate a summary\n    # Prepare the prompt\n    system_prompt = (\n        \"You are an AI assistant that summarizes information gathered from multiple web pages. \"\n        \"Provide a comprehensive, well-written summary of the key points related to the following query.\"\n    )\n    user_prompt = f\"Query: {query}\\n\\nContent:\\n{combined_content}\\n\\nSummary:\"\n\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_prompt},\n    ]\n\n    # Call the completion function\n    response = client.completion(\n        messages=messages,\n        model=model,\n        mode=mode,\n        max_retries=max_retries,\n        temperature=temperature,\n        run_tools=run_tools,\n        tools=tools,\n        parallel_tool_calls=parallel_tool_calls,\n        tool_choice=tool_choice,\n        verbose=verbose,\n        **kwargs,\n    )\n\n    # Extract the summary\n    summary = response.choices[0].message.content\n    workflow.summary = summary\n\n    # Step 6: Evaluate\n    workflow.current_step = ScrapingStep.EVALUATE\n    evaluation_prompt = (\n        f\"Evaluate the quality and relevance of the following summary for the query: '{query}'\\n\\n\"\n        f\"Summary:\\n{summary}\\n\\n\"\n        \"Provide an explanation of your evaluation and determine if the summary is successful or needs refinement.\"\n    )\n\n    evaluation_response = client.completion(\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are an expert evaluator of summaries.\"},\n            {\"role\": \"user\", \"content\": evaluation_prompt},\n        ],\n        model=model,\n        response_model=StepResult,\n        mode=mode,\n        max_retries=max_retries,\n        temperature=temperature,\n    )\n\n    workflow.evaluation = evaluation_response\n\n    # Step 7: Refine if necessary\n    if not evaluation_response.is_successful:\n        workflow.current_step = ScrapingStep.REFINE\n        refine_prompt = (\n            f\"The previous summary for the query '{query}' needs improvement.\\n\\n\"\n            f\"Original summary:\\n{summary}\\n\\n\"\n            f\"Evaluation feedback:\\n{evaluation_response.explanation}\\n\\n\"\n            \"Please provide an improved and refined summary addressing the feedback.\"\n        )\n\n        refined_response = client.completion(\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an expert at refining and improving summaries.\"},\n                {\"role\": \"user\", \"content\": refine_prompt},\n            ],\n            model=model,\n            mode=mode,\n            max_retries=max_retries,\n            temperature=temperature,\n        )\n\n        summary = refined_response.choices[0].message.content\n\n    if verbose:\n        print(\"Generated summary:\")\n        print(summary)\n\n    # Create a Document object\n    document = Document(\n        content=summary,\n        metadata={\n            \"query\": query,\n            \"urls\": urls,\n            \"model\": model,\n            \"client\": client,\n            \"workflow\": workflow.dict(),\n        },\n    )\n\n    return document\n</code></pre>"},{"location":"examples/document-ai/multi-document-rag/","title":"Multi Document RAG","text":"<p>Multi Document RAG is not as simple as single document QA. <code>zyx</code> has decided to let the professionals handle this one, and provides a simple interface for working with <code>Document</code> objects using the <code>chromadb</code> library, with a simple wrapper interface.</p>"},{"location":"examples/document-ai/multi-document-rag/#example","title":"Example","text":"<p>Lets begin by loading the documents we're going to use for this example.</p> <pre><code>import zyx\n\nlinks = [\n    \"https://openreview.net/pdf?id=zAdUB0aCTQ\", # AgentBench: Evaluating LLMs as Agents\n    \"https://openreview.net/pdf?id=z8TW0ttBPp\", # MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning\n    \"https://openreview.net/pdf?id=yoVq2BGQdP\", # Achieving Fairness in Multi-Agent MDP Using Reinforcement Learning\n    \"https://openreview.net/pdf?id=yRrPfKyJQ2\", # Conversational Drug Editing Using Retrieval and Domain Feedback\n]\n\ndocuments = zyx.read(links)                     # Handles lists of links/paths as well\n</code></pre> <p>Lets now investigate the documents we read</p> <pre><code>for doc in documents:\n    print(\"---\")\n    print(doc.content[:200])\n</code></pre> Output <pre><code>---\nPublished as a conference paper at ICLR 2024\nAGENT BENCH : EVALUATING LLM S AS AGENTS\nXiao Liu1,*, Hao Yu1,*,\u2020, Hanchen Zhang1,*, Yifan Xu1, Xuanyu Lei1, Hanyu Lai1, Yu Gu2,\u2020,\nHangliang Ding1, Kaiwen\n---\nPublished as a conference paper at ICLR 2024\nMATHCODER : S EAMLESS CODE INTEGRATION IN\nLLM S FOR ENHANCED MATHEMATICAL REASONING\nKe Wang1,4\u2217Houxing Ren1\u2217Aojun Zhou1\u2217Zimu Lu1\u2217Sichun Luo3\u2217\nWeikang Shi1\u2217\n---\nPublished as a conference paper at ICLR 2024\nACHIEVING FAIRNESS IN MULTI -AGENT MDP U SING\nREINFORCEMENT LEARNING\nPeizhong Ju\nDepartment of ECE\nThe Ohio State University\nColumbus, OH 43210, USA\nju.171\n---\nPublished as a conference paper at ICLR 2024\nCONVERSATIONAL DRUG EDITING USING RETRIEVAL\nAND DOMAIN FEEDBACK\nShengchao Liu1 *, Jiongxiao Wang2 *, Yijin Yang3, Chengpeng Wang4, Ling Liu5,\nHongyu Guo6,7\n</code></pre>"},{"location":"examples/document-ai/multi-document-rag/#creating-a-memory-store","title":"Creating a Memory Store","text":"<pre><code># Initialize an on memory store\nstore = zyx.Memory()\n</code></pre> <p>Now lets add our documents to the store</p> <pre><code>store.add(documents)\n\n# Now we can use the store to search for documents\n# One of our papers is about LLM's in the domain of Drug Editing\nresults = store.search(\"Drug Editing\")\n</code></pre>"},{"location":"examples/document-ai/multi-document-rag/#llm-completions-in-the-store","title":"LLM Completions in the Store","text":"<pre><code># We can also wuery our store with an LLM\nresponse = store.completion(\"How have LLM's been used in the domain of Drug Editing?\")\n\nprint(response)\n</code></pre> Output <pre><code>ChatCompletion(\n    id='chatcmpl-ACvGG7JCm2pCwIZgxNCQa5Iew9HEZ',\n    choices=[\n        Choice(\n            finish_reason='stop',\n            index=0,\n            logprobs=None,\n            message=ChatCompletionMessage(\n                content='Large Language Models (LLMs) have been utilized in the domain of drug editing primarily for their capabilities in data analysis,\npredictive modeling, and natural language processing. They assist in the identification of potential drug candidates by analyzing vast databases of chemical\ncompounds and biological data. LLMs can predict the interactions between drugs and biological targets, facilitate the design of novel drug molecules, and\nstreamline the drug discovery process by automating literature reviews and synthesizing relevant information. Moreover, their ability to generate hypotheses and\nsimulate molecular interactions aids researchers in optimizing drug formulations and improving efficacy. Overall, LLMs enhance efficiency and innovation in drug\nediting and development.',\n                refusal=None,\n                role='assistant',\n                function_call=None,\n                tool_calls=None\n            )\n        )\n    ],\n    created=1727643412,\n    model='gpt-4o-mini-2024-07-18',\n    object='chat.completion',\n    service_tier=None,\n    system_fingerprint='fp_f85bea6784',\n    usage=CompletionUsage(completion_tokens=126, prompt_tokens=46, total_tokens=172, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0))\n)\n</code></pre>"},{"location":"examples/document-ai/multi-document-rag/#api-reference","title":"API Reference","text":"<p>Class for storing and retrieving data using Chroma.</p> Source code in <code>zyx/resources/stores/memory.py</code> <pre><code>class Memory:\n\n    \"\"\"\n    Class for storing and retrieving data using Chroma.\n    \"\"\"\n\n    def __init__(\n        self,\n        collection_name: str = \"my_collection\",\n        model_class: Optional[Type[BaseModel]] = None,\n        embedding_api_key: Optional[str] = None,\n        location: Union[Literal[\":memory:\"], str] = \":memory:\",\n        persist_directory: str = \"chroma_db\",\n        chunk_size: int = 512,\n        model: str = \"gpt-4o-mini\",\n    ):\n        \"\"\"\n        Class for storing and retrieving data using Chroma.\n\n        Args:\n            collection_name (str): The name of the collection.\n            model_class (Type[BaseModel], optional): Model class for storing data.\n            embedding_api_key (str, optional): API key for embedding model.\n            location (str): \":memory:\" for in-memory database or a string path for persistent storage.\n            persist_directory (str): Directory for persisting Chroma database (if not using in-memory storage).\n            chunk_size (int): Size of chunks for text splitting.\n            model (str): Model name for text summarization.\n        \"\"\"\n\n        self.collection_name = collection_name\n        self.embedding_api_key = embedding_api_key\n        self.model_class = model_class\n        self.location = location\n        self.persist_directory = persist_directory\n        self.chunk_size = chunk_size\n        self.model = model\n\n        self.client = self._initialize_client()\n        self.collection = self._create_or_get_collection()\n\n    def _initialize_client(self):\n        \"\"\"\n        Initialize Chroma client. Use in-memory database if location is \":memory:\",\n        otherwise, use persistent storage at the specified directory.\n        \"\"\"\n        if self.location == \":memory:\":\n            logger.info(\"Using in-memory Chroma storage.\")\n            return Client()  # In-memory by default\n        else:\n            logger.info(f\"Using persistent Chroma storage at {self.persist_directory}.\")\n            settings = Settings(persist_directory=self.persist_directory)\n            return Client(settings)\n\n    def _create_or_get_collection(self):\n        \"\"\"Retrieve or create a Chroma collection with a custom embedding function.\"\"\"\n        embedding_fn = CustomEmbeddingFunction(api_key=self.embedding_api_key)\n        if self.collection_name in self.client.list_collections():\n            logger.info(f\"Collection '{self.collection_name}' already exists.\")\n            return self.client.get_collection(self.collection_name, embedding_function=embedding_fn)\n        else:\n            logger.info(f\"Creating collection '{self.collection_name}'.\")\n            return self.client.create_collection(name=self.collection_name, embedding_function=embedding_fn)\n\n    def _get_embedding(self, text: str) -&gt; List[float]:\n        \"\"\"Generate embeddings for a given text using the custom embedding function.\n\n        Args:\n            text (str): The text to generate an embedding for.\n\n        Returns:\n            List[float]: The embedding for the text.\n        \"\"\"\n        embedding_fn = CustomEmbeddingFunction(api_key=self.embedding_api_key)\n        return embedding_fn([text])[0]  # Return the first (and only) embedding\n\n    def add(\n        self,\n        data: Union[str, List[str], Document, List[Document]],\n        metadata: Optional[dict] = None,\n    ):\n        \"\"\"Add documents or data to Chroma.\n\n        Args:\n            data (Union[str, List[str], Document, List[Document]]): The data to add to Chroma.\n            metadata (Optional[dict]): The metadata to add to the data.\n        \"\"\"\n        if isinstance(data, str):\n            data = [data]\n        elif isinstance(data, Document):\n            data = [data]\n\n        ids, embeddings, texts, metadatas = [], [], [], []\n\n        for item in data:\n            try:\n                if isinstance(item, Document):\n                    text = item.content\n                    metadata = item.metadata\n                else:\n                    text = item\n\n                # Chunk the content\n                chunks = chunk(text, chunk_size=self.chunk_size, model=self.model)\n\n                for chunk_text in chunks:\n                    embedding_vector = self._get_embedding(chunk_text)\n                    ids.append(str(uuid.uuid4()))\n                    embeddings.append(embedding_vector)\n                    texts.append(chunk_text)\n                    chunk_metadata = metadata.copy() if metadata else {}\n                    chunk_metadata['chunk'] = True\n                    metadatas.append(chunk_metadata)\n            except Exception as e:\n                logger.error(f\"Error processing item: {item}. Error: {e}\")\n\n        if embeddings:\n            try:\n                # Ensure metadatas is not empty\n                metadatas = [m if m else {\"default\": \"empty\"} for m in metadatas]\n                self.collection.add(\n                    ids=ids, embeddings=embeddings, metadatas=metadatas, documents=texts\n                )\n                logger.info(f\"Successfully added {len(embeddings)} chunks to the collection.\")\n            except Exception as e:\n                logger.error(f\"Error adding points to collection: {e}\")\n        else:\n            logger.warning(\"No valid embeddings to add to the collection.\")\n\n    def search(self, query: str, top_k: int = 5) -&gt; SearchResponse:\n        \"\"\"Search in Chroma collection.\n\n        Args:\n            query (str): The query to search for.\n            top_k (int): The number of results to return.\n\n        Returns:\n            SearchResponse: The search results.\n        \"\"\"\n        try:\n            query_embedding = self._get_embedding(query)\n            search_results = self.collection.query(query_embeddings=[query_embedding], n_results=top_k)\n\n            nodes = []\n            for i in range(len(search_results[\"ids\"][0])):  # Note the [0] here\n                node = ChromaNode(\n                    id=search_results[\"ids\"][0][i],\n                    text=search_results[\"documents\"][0][i],\n                    embedding=query_embedding,\n                    metadata=search_results[\"metadatas\"][0][i] if search_results[\"metadatas\"] else {}\n                )\n                nodes.append(node)\n            return SearchResponse(query=query, results=nodes)\n        except Exception as e:\n            logger.error(f\"Error during search: {e}\")\n            return SearchResponse(query=query)  # Return empty results on error\n\n    def _summarize_results(self, results: List[ChromaNode]) -&gt; str:\n        \"\"\"Summarize the search results.\n\n        Args:\n            results (List[ChromaNode]): The search results.\n\n        Returns:\n            str: The summary of the search results.\n        \"\"\"\n        class SummaryModel(BaseModel):\n            summary: str\n\n        texts = [node.text for node in results]\n        combined_text = \"\\n\\n\".join(texts)\n\n        summary = generate(\n            SummaryModel,\n            instructions=\"Provide a concise summary of the following text, focusing on the most important information:\",\n            model=self.model,\n            n=1\n        )\n\n        return summary.summary\n\n    def completion(\n        self,\n        messages: Union[str, List[dict]] = None,\n        model: Optional[str] = None,\n        top_k: Optional[int] = 5,\n        tools: Optional[List[Union[Callable, dict, BaseModel]]] = None,\n        run_tools: Optional[bool] = True,\n        response_model: Optional[BaseModel] = None,\n        mode: Optional[InstructorMode] = \"tool_call\",\n        base_url: Optional[str] = None,\n        api_key: Optional[str] = None,\n        organization: Optional[str] = None,\n        top_p: Optional[float] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        max_retries: Optional[int] = 3,\n        verbose: Optional[bool] = False,\n    ):\n        \"\"\"Perform completion with context from Chroma.\n\n        Args:\n            messages (Union[str, List[dict]]): The messages to use for the completion.\n            model (Optional[str]): The model to use for the completion.\n            top_k (Optional[int]): The number of results to return from the search.\n            tools (Optional[List[Union[Callable, dict, BaseModel]]]): The tools to use for the completion.\n            run_tools (Optional[bool]): Whether to run the tools for the completion.\n            response_model (Optional[BaseModel]): The response model to use for the completion.\n            mode (Optional[InstructorMode]): The mode to use for the completion.\n            base_url (Optional[str]): The base URL to use for the completion.\n            api_key (Optional[str]): The API key to use for the completion.\n            organization (Optional[str]): The organization to use for the completion.\n            top_p (Optional[float]): The top p to use for the completion.\n            temperature (Optional[float]): The temperature to use for the completion.\n            max_tokens (Optional[int]): The maximum number of tokens to generate.\n            max_retries (Optional[int]): The maximum number of retries to use for the completion.\n            verbose (Optional[bool]): Whether to print the messages to the console.\n        \"\"\"\n        logger.info(f\"Initial messages: {messages}\")\n\n        if isinstance(messages, str):\n            messages = [{\"role\": \"user\", \"content\": messages}]\n        elif isinstance(messages, list):\n            messages = [{\"role\": \"user\", \"content\": m} if isinstance(m, str) else m for m in messages]\n\n        query = messages[-1].get(\"content\", \"\") if messages else \"\"\n\n        try:\n            results = self.search(query, top_k=top_k)\n            summarized_results = self._summarize_results(results.results)\n        except Exception as e:\n            logger.error(f\"Error during search or summarization: {e}\")\n            summarized_results = \"\"\n\n        if messages:\n            if not any(message.get(\"role\", \"\") == \"system\" for message in messages):\n                system_message = {\n                    \"role\": \"system\",\n                    \"content\": f\"Relevant information retrieved: \\n {summarized_results}\",\n                }\n                messages.insert(0, system_message)\n            else:\n                for message in messages:\n                    if message.get(\"role\", \"\") == \"system\":\n                        message[\"content\"] += f\"\\nAdditional context: {summarized_results}\"\n\n        try:\n            result = completion(\n                messages=messages,\n                model=model or self.model,\n                tools=tools,\n                run_tools=run_tools,\n                response_model=response_model,\n                mode=mode,\n                base_url=base_url,\n                api_key=api_key,\n                organization=organization,\n                top_p=top_p,\n                temperature=temperature,\n                max_tokens=max_tokens,\n                max_retries=max_retries,\n            )\n\n            if verbose:\n                logger.info(f\"Completion result: {result}\")\n\n            return result\n        except Exception as e:\n            logger.error(f\"Error during completion: {e}\")\n            raise\n</code></pre>"},{"location":"examples/document-ai/multi-document-rag/#zyx.resources.stores.memory.Memory.__init__","title":"<code>__init__(collection_name='my_collection', model_class=None, embedding_api_key=None, location=':memory:', persist_directory='chroma_db', chunk_size=512, model='gpt-4o-mini')</code>","text":"<p>Class for storing and retrieving data using Chroma.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>The name of the collection.</p> <code>'my_collection'</code> <code>model_class</code> <code>Type[BaseModel]</code> <p>Model class for storing data.</p> <code>None</code> <code>embedding_api_key</code> <code>str</code> <p>API key for embedding model.</p> <code>None</code> <code>location</code> <code>str</code> <p>\":memory:\" for in-memory database or a string path for persistent storage.</p> <code>':memory:'</code> <code>persist_directory</code> <code>str</code> <p>Directory for persisting Chroma database (if not using in-memory storage).</p> <code>'chroma_db'</code> <code>chunk_size</code> <code>int</code> <p>Size of chunks for text splitting.</p> <code>512</code> <code>model</code> <code>str</code> <p>Model name for text summarization.</p> <code>'gpt-4o-mini'</code> Source code in <code>zyx/resources/stores/memory.py</code> <pre><code>def __init__(\n    self,\n    collection_name: str = \"my_collection\",\n    model_class: Optional[Type[BaseModel]] = None,\n    embedding_api_key: Optional[str] = None,\n    location: Union[Literal[\":memory:\"], str] = \":memory:\",\n    persist_directory: str = \"chroma_db\",\n    chunk_size: int = 512,\n    model: str = \"gpt-4o-mini\",\n):\n    \"\"\"\n    Class for storing and retrieving data using Chroma.\n\n    Args:\n        collection_name (str): The name of the collection.\n        model_class (Type[BaseModel], optional): Model class for storing data.\n        embedding_api_key (str, optional): API key for embedding model.\n        location (str): \":memory:\" for in-memory database or a string path for persistent storage.\n        persist_directory (str): Directory for persisting Chroma database (if not using in-memory storage).\n        chunk_size (int): Size of chunks for text splitting.\n        model (str): Model name for text summarization.\n    \"\"\"\n\n    self.collection_name = collection_name\n    self.embedding_api_key = embedding_api_key\n    self.model_class = model_class\n    self.location = location\n    self.persist_directory = persist_directory\n    self.chunk_size = chunk_size\n    self.model = model\n\n    self.client = self._initialize_client()\n    self.collection = self._create_or_get_collection()\n</code></pre>"},{"location":"examples/document-ai/multi-document-rag/#zyx.resources.stores.memory.Memory.add","title":"<code>add(data, metadata=None)</code>","text":"<p>Add documents or data to Chroma.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, List[str], Document, List[Document]]</code> <p>The data to add to Chroma.</p> required <code>metadata</code> <code>Optional[dict]</code> <p>The metadata to add to the data.</p> <code>None</code> Source code in <code>zyx/resources/stores/memory.py</code> <pre><code>def add(\n    self,\n    data: Union[str, List[str], Document, List[Document]],\n    metadata: Optional[dict] = None,\n):\n    \"\"\"Add documents or data to Chroma.\n\n    Args:\n        data (Union[str, List[str], Document, List[Document]]): The data to add to Chroma.\n        metadata (Optional[dict]): The metadata to add to the data.\n    \"\"\"\n    if isinstance(data, str):\n        data = [data]\n    elif isinstance(data, Document):\n        data = [data]\n\n    ids, embeddings, texts, metadatas = [], [], [], []\n\n    for item in data:\n        try:\n            if isinstance(item, Document):\n                text = item.content\n                metadata = item.metadata\n            else:\n                text = item\n\n            # Chunk the content\n            chunks = chunk(text, chunk_size=self.chunk_size, model=self.model)\n\n            for chunk_text in chunks:\n                embedding_vector = self._get_embedding(chunk_text)\n                ids.append(str(uuid.uuid4()))\n                embeddings.append(embedding_vector)\n                texts.append(chunk_text)\n                chunk_metadata = metadata.copy() if metadata else {}\n                chunk_metadata['chunk'] = True\n                metadatas.append(chunk_metadata)\n        except Exception as e:\n            logger.error(f\"Error processing item: {item}. Error: {e}\")\n\n    if embeddings:\n        try:\n            # Ensure metadatas is not empty\n            metadatas = [m if m else {\"default\": \"empty\"} for m in metadatas]\n            self.collection.add(\n                ids=ids, embeddings=embeddings, metadatas=metadatas, documents=texts\n            )\n            logger.info(f\"Successfully added {len(embeddings)} chunks to the collection.\")\n        except Exception as e:\n            logger.error(f\"Error adding points to collection: {e}\")\n    else:\n        logger.warning(\"No valid embeddings to add to the collection.\")\n</code></pre>"},{"location":"examples/document-ai/multi-document-rag/#zyx.resources.stores.memory.Memory.completion","title":"<code>completion(messages=None, model=None, top_k=5, tools=None, run_tools=True, response_model=None, mode='tool_call', base_url=None, api_key=None, organization=None, top_p=None, temperature=None, max_tokens=None, max_retries=3, verbose=False)</code>","text":"<p>Perform completion with context from Chroma.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, List[dict]]</code> <p>The messages to use for the completion.</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>The model to use for the completion.</p> <code>None</code> <code>top_k</code> <code>Optional[int]</code> <p>The number of results to return from the search.</p> <code>5</code> <code>tools</code> <code>Optional[List[Union[Callable, dict, BaseModel]]]</code> <p>The tools to use for the completion.</p> <code>None</code> <code>run_tools</code> <code>Optional[bool]</code> <p>Whether to run the tools for the completion.</p> <code>True</code> <code>response_model</code> <code>Optional[BaseModel]</code> <p>The response model to use for the completion.</p> <code>None</code> <code>mode</code> <code>Optional[InstructorMode]</code> <p>The mode to use for the completion.</p> <code>'tool_call'</code> <code>base_url</code> <code>Optional[str]</code> <p>The base URL to use for the completion.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key to use for the completion.</p> <code>None</code> <code>organization</code> <code>Optional[str]</code> <p>The organization to use for the completion.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>The top p to use for the completion.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>The temperature to use for the completion.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens to generate.</p> <code>None</code> <code>max_retries</code> <code>Optional[int]</code> <p>The maximum number of retries to use for the completion.</p> <code>3</code> <code>verbose</code> <code>Optional[bool]</code> <p>Whether to print the messages to the console.</p> <code>False</code> Source code in <code>zyx/resources/stores/memory.py</code> <pre><code>def completion(\n    self,\n    messages: Union[str, List[dict]] = None,\n    model: Optional[str] = None,\n    top_k: Optional[int] = 5,\n    tools: Optional[List[Union[Callable, dict, BaseModel]]] = None,\n    run_tools: Optional[bool] = True,\n    response_model: Optional[BaseModel] = None,\n    mode: Optional[InstructorMode] = \"tool_call\",\n    base_url: Optional[str] = None,\n    api_key: Optional[str] = None,\n    organization: Optional[str] = None,\n    top_p: Optional[float] = None,\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    max_retries: Optional[int] = 3,\n    verbose: Optional[bool] = False,\n):\n    \"\"\"Perform completion with context from Chroma.\n\n    Args:\n        messages (Union[str, List[dict]]): The messages to use for the completion.\n        model (Optional[str]): The model to use for the completion.\n        top_k (Optional[int]): The number of results to return from the search.\n        tools (Optional[List[Union[Callable, dict, BaseModel]]]): The tools to use for the completion.\n        run_tools (Optional[bool]): Whether to run the tools for the completion.\n        response_model (Optional[BaseModel]): The response model to use for the completion.\n        mode (Optional[InstructorMode]): The mode to use for the completion.\n        base_url (Optional[str]): The base URL to use for the completion.\n        api_key (Optional[str]): The API key to use for the completion.\n        organization (Optional[str]): The organization to use for the completion.\n        top_p (Optional[float]): The top p to use for the completion.\n        temperature (Optional[float]): The temperature to use for the completion.\n        max_tokens (Optional[int]): The maximum number of tokens to generate.\n        max_retries (Optional[int]): The maximum number of retries to use for the completion.\n        verbose (Optional[bool]): Whether to print the messages to the console.\n    \"\"\"\n    logger.info(f\"Initial messages: {messages}\")\n\n    if isinstance(messages, str):\n        messages = [{\"role\": \"user\", \"content\": messages}]\n    elif isinstance(messages, list):\n        messages = [{\"role\": \"user\", \"content\": m} if isinstance(m, str) else m for m in messages]\n\n    query = messages[-1].get(\"content\", \"\") if messages else \"\"\n\n    try:\n        results = self.search(query, top_k=top_k)\n        summarized_results = self._summarize_results(results.results)\n    except Exception as e:\n        logger.error(f\"Error during search or summarization: {e}\")\n        summarized_results = \"\"\n\n    if messages:\n        if not any(message.get(\"role\", \"\") == \"system\" for message in messages):\n            system_message = {\n                \"role\": \"system\",\n                \"content\": f\"Relevant information retrieved: \\n {summarized_results}\",\n            }\n            messages.insert(0, system_message)\n        else:\n            for message in messages:\n                if message.get(\"role\", \"\") == \"system\":\n                    message[\"content\"] += f\"\\nAdditional context: {summarized_results}\"\n\n    try:\n        result = completion(\n            messages=messages,\n            model=model or self.model,\n            tools=tools,\n            run_tools=run_tools,\n            response_model=response_model,\n            mode=mode,\n            base_url=base_url,\n            api_key=api_key,\n            organization=organization,\n            top_p=top_p,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            max_retries=max_retries,\n        )\n\n        if verbose:\n            logger.info(f\"Completion result: {result}\")\n\n        return result\n    except Exception as e:\n        logger.error(f\"Error during completion: {e}\")\n        raise\n</code></pre>"},{"location":"examples/document-ai/multi-document-rag/#zyx.resources.stores.memory.Memory.search","title":"<code>search(query, top_k=5)</code>","text":"<p>Search in Chroma collection.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to search for.</p> required <code>top_k</code> <code>int</code> <p>The number of results to return.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>SearchResponse</code> <code>SearchResponse</code> <p>The search results.</p> Source code in <code>zyx/resources/stores/memory.py</code> <pre><code>def search(self, query: str, top_k: int = 5) -&gt; SearchResponse:\n    \"\"\"Search in Chroma collection.\n\n    Args:\n        query (str): The query to search for.\n        top_k (int): The number of results to return.\n\n    Returns:\n        SearchResponse: The search results.\n    \"\"\"\n    try:\n        query_embedding = self._get_embedding(query)\n        search_results = self.collection.query(query_embeddings=[query_embedding], n_results=top_k)\n\n        nodes = []\n        for i in range(len(search_results[\"ids\"][0])):  # Note the [0] here\n            node = ChromaNode(\n                id=search_results[\"ids\"][0][i],\n                text=search_results[\"documents\"][0][i],\n                embedding=query_embedding,\n                metadata=search_results[\"metadatas\"][0][i] if search_results[\"metadatas\"] else {}\n            )\n            nodes.append(node)\n        return SearchResponse(query=query, results=nodes)\n    except Exception as e:\n        logger.error(f\"Error during search: {e}\")\n        return SearchResponse(query=query)  # Return empty results on error\n</code></pre>"},{"location":"examples/document-ai/single-document-qa/","title":"Single Document QA","text":"<p>The simplest form of <code>RAG</code> or Retrieval-Augmented Generation is single document QA. <code>zyx</code> provides a simple interface for interacting with documents for LLM interpretation.</p>"},{"location":"examples/document-ai/single-document-qa/#example-the-large-language-monkeys-paper","title":"Example - The Large Language Monkeys Paper","text":"<p>Lets begin by loading the Large Language Monkeys paper from ARXIV, using the <code>read()</code> function. <code>read()</code> is able to parse most document formats, from both local files &amp; file URLs.</p> <p>PDF Image &amp; Table extraction is currently in development.</p> <pre><code>import zyx\n\n# Load the Large Language Monkeys paper\npaper = zyx.read(\"https://arxiv.org/pdf/2407.21787\")\n</code></pre> <p></p> <p>The read function returns a <code>Document</code> object by default, lets inspect this document now.</p> <pre><code>print(\"Metadata:\", paper.metadata)\nprint(\"Content:\", paper.content[:200])\n</code></pre> <pre><code>Metadata:\n    {'file_name': '2407.21787', 'file_type': 'application/pdf', 'file_size': 955583}\n\nContent: Large Language Monkeys: Scaling Inference Compute\n    with Repeated Sampling\n    Bradley Brown\u2217\u2020\u2021, Jordan Juravsky\u2217\u2020, Ryan Ehrlich\u2217\u2020, Ronald Clark\u2021, Quoc V. Le\u00a7,\n    Christopher R\u00b4 e\u2020, and Azalia Mirhoseini\u2020\u00a7\n    \u2020De\n</code></pre> <p></p> <p>The <code>Document</code> object can now easily be queried, achieving our goal of single document QA.</p> <pre><code>result = document.query(\"What is this paper about?\", model = \"gpt-4o-mini\")\n</code></pre> Output <pre><code>ChatCompletion(\n    id='chatcmpl-ACk3yliGJKL5ueVazlySEHB8peRSa',\n    choices=[\n        Choice(\n            finish_reason='stop',\n            index=0,\n            logprobs=None,\n            message=ChatCompletionMessage(\n                content='The paper titled \"Large Language Monkeys: Scaling Inference Compute with Repeated Sampling\" investigates the effectiveness of using\nmultiple samples during inference to improve the performance of large language models (LLMs) in solving various tasks. Here are the main points the paper\ncovers:\\n\\n1. **Scaling Inference Compute**: While significant advances in LLM capabilities have been achieved through training larger models, the authors argue\nthat inference can also be improved by increasing the number of samples generated for solving a problem, rather than limiting it to a single attempt.\\n\\n2. **Key\nParameters**: The research focuses on two primary factors:\\n   - **Coverage**: The proportion of problems solved by at least one of the generated samples.\\n   -\n**Precision**: The ability to identify correct solutions from multiple generated samples.\\n\\n3. **Experimental Results**: The authors conducted experiments across\nmultiple tasks, including coding and formal proofs, and demonstrated that increasing the number of samples can substantially improve coverage. For instance, in\ncoding challenges, the coverage improved from 15.9% to 56% when increasing the number of samples from one to 250, surpassing state-of-the-art models that use\nsingle attempts.\\n\\n4. **Cost-Effectiveness**: The paper also finds that using repeated sampling with less expensive models can sometimes be more cost-effective\nthan using fewer samples from more powerful models.\\n\\n5. **Inference Scaling Laws**: The relationship between sample size and coverage appears to follow a\nlog-linear trend, suggesting that there are scaling laws for inference similar to those observed in training.\\n\\n6. **Challenges**: While repeated sampling shows\npromise, the authors note that effective mechanisms for identifying correct solutions from many samples remain a challenge, particularly in domains without\nautomatic verification methods.\\n\\n7. **Future Directions**: Suggestions for further research include improving sample verification methods, exploring multi-turn\ninteractions for feedback, and leveraging previous attempts in generating new samples.\\n\\nOverall, the paper emphasizes the potential of repeated sampling as a\nstrategy to enhance the problem-solving capabilities of LLMs during inference and highlights both its advantages and areas for future exploration.',\n                refusal=None,\n                role='assistant',\n                function_call=None,\n                tool_calls=None\n            )\n        )\n    ],\n    created=1727600366,\n    model='gpt-4o-mini-2024-07-18',\n    object='chat.completion',\n    service_tier=None,\n    system_fingerprint='fp_f85bea6784',\n    usage=CompletionUsage(completion_tokens=417, prompt_tokens=20407, total_tokens=20824, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0))\n)\n</code></pre>"},{"location":"examples/document-ai/utils/","title":"Document AI Utilities","text":"<p><code>zyx</code> provides a couple of utilities for working with documents &amp; long text a little easier.</p>"},{"location":"examples/document-ai/utils/#chunking","title":"Chunking","text":"<p>Utilize the <code>chunk</code> function for quick semantic chunking, with optional parallelization.</p> <pre><code>from zyx import chunk\n\nchunk(\"Hello, world!\")\n# [\"Hello, world!\"]\n</code></pre>"},{"location":"examples/document-ai/utils/#api-reference","title":"API Reference","text":"<p>Takes a string, Document, or a list of strings/Document models and returns the chunked content.</p> Example <pre><code>chunk(\"Hello, world!\")\n# [\"Hello, world!\"]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Document, List[Union[str, Document]]]</code> <p>Union[str, Document, List[Union[str, Document]]]: The input to chunk.</p> required <code>chunk_size</code> <code>int</code> <p>int: The size of the chunks to return.</p> <code>512</code> <code>model</code> <code>str</code> <p>str: The model to use for chunking.</p> <code>'gpt-4'</code> <code>processes</code> <code>int</code> <p>int: The number of processes to use for chunking.</p> <code>1</code> <code>memoize</code> <code>bool</code> <p>bool: Whether to memoize the chunking process.</p> <code>True</code> <code>progress</code> <code>bool</code> <p>bool: Whether to show a progress bar.</p> <code>False</code> <code>max_token_chars</code> <code>int</code> <p>int: The maximum number of characters to use for chunking.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[List[str], List[List[str]]]</code> <p>Union[List[str], List[List[str]]]: The chunked content.</p> Source code in <code>zyx/resources/data/chunk.py</code> <pre><code>def chunk(\n    inputs: Union[str, Document, List[Union[str, Document]]],\n    chunk_size: int = 512,\n    model: str = \"gpt-4\",\n    processes: int = 1,\n    memoize: bool = True,\n    progress: bool = False,\n    max_token_chars: int = None,\n) -&gt; Union[List[str], List[List[str]]]:\n    \"\"\"\n    Takes a string, Document, or a list of strings/Document models and returns the chunked content.\n\n    Example:\n        ```python\n        chunk(\"Hello, world!\")\n        # [\"Hello, world!\"]\n        ```\n\n    Args:\n        inputs: Union[str, Document, List[Union[str, Document]]]: The input to chunk.\n        chunk_size: int: The size of the chunks to return.\n        model: str: The model to use for chunking.\n        processes: int: The number of processes to use for chunking.\n        memoize: bool: Whether to memoize the chunking process.\n        progress: bool: Whether to show a progress bar.\n        max_token_chars: int: The maximum number of characters to use for chunking.\n\n    Returns:\n        Union[List[str], List[List[str]]]: The chunked content. \n    \"\"\"\n    try:\n        tokenizer = tiktoken.encoding_for_model(model)\n        chunker = semchunk.chunkerify(\n            tokenizer,\n            chunk_size=chunk_size,\n            max_token_chars=max_token_chars,\n            memoize=memoize,\n        )\n\n        # Handle single input case (str or Document)\n        if isinstance(inputs, (str, Document)):\n            inputs = [inputs]  # Convert to list for uniform handling\n\n        if not isinstance(inputs, list):\n            raise TypeError(\n                \"inputs must be a string, Document, or a list of strings/Documents\"\n            )\n\n        texts = []\n        for item in inputs:\n            # Handle Document content\n            if isinstance(item, Document):\n                content = item.content\n                # Convert non-string content (e.g., lists from CSV/XLSX) to string\n                if isinstance(content, list):\n                    content = \"\\n\".join([\" | \".join(map(str, row)) for row in content])\n                elif not isinstance(content, str):\n                    raise TypeError(\n                        f\"Document content must be a string or list of strings, found {type(content)}\"\n                    )\n                texts.append(content)\n            # Handle string input directly\n            elif isinstance(item, str):\n                texts.append(item)\n            else:\n                raise TypeError(f\"Unsupported input type: {type(item)}\")\n\n        # Chunk the content, using processes and progress bar as needed\n        if len(texts) == 1:\n            return chunker(texts[0])  # Single input, return the chunked result\n        else:\n            return chunker(\n                texts, processes=processes, progress=progress\n            )  # Multiple inputs\n\n    except Exception as e:\n        # Detailed error logging\n        print(f\"Error in chunk function: {str(e)}\")\n        raise e\n</code></pre>"},{"location":"examples/document-ai/utils/#reading","title":"Reading","text":"<p>Utilize the <code>read</code> function for quick reading of most document types from both local file systems &amp; the web. Able to injest many documents at once, and return a list of <code>Document</code> models.</p> <pre><code>from zyx import read\n\nread(\"path/to/file.pdf\")\n# Document(content=\"...\", metadata={\"file_name\": \"file.pdf\", \"file_type\": \"application/pdf\", \"file_size\": 123456})\n</code></pre>"},{"location":"examples/document-ai/utils/#api-reference_1","title":"API Reference","text":"<p>Reads either a file, a directory, or a list of files and returns the content.</p> Example <pre><code>read(\"path/to/file.pdf\")\n# Document(content=\"...\", metadata={\"file_name\": \"file.pdf\", \"file_type\": \"application/pdf\", \"file_size\": 123456})\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path, List[Union[str, Path]]]</code> <p>Union[str, Path, List[Union[str, Path]]]: The path to read.</p> required <code>output</code> <code>Union[Type[str], OutputFormat]</code> <p>Union[Type[str], OutputFormat]: The output format.</p> <code>'document'</code> <code>target</code> <code>OutputType</code> <p>OutputType: The output type.</p> <code>'text'</code> <code>verbose</code> <code>bool</code> <p>bool: Whether to print verbose output.</p> <code>False</code> <code>workers</code> <code>Optional[int]</code> <p>Optional[int]: The number of workers to use for reading.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Document, List[Document], str]</code> <p>Union[Document, List[Document], str]: The content.</p> Source code in <code>zyx/resources/data/reader.py</code> <pre><code>def read(\n    path: Union[str, Path, List[Union[str, Path]]],\n    output: Union[Type[str], OutputFormat] = \"document\",\n    target: OutputType = \"text\",\n    verbose: bool = False,\n    workers: Optional[int] = None,\n) -&gt; Union[Document, List[Document], str]:\n    \"\"\"\n    Reads either a file, a directory, or a list of files and returns the content.\n\n    Example:\n        ```python\n        read(\"path/to/file.pdf\")\n        # Document(content=\"...\", metadata={\"file_name\": \"file.pdf\", \"file_type\": \"application/pdf\", \"file_size\": 123456})\n        ```\n\n    Args:\n        path: Union[str, Path, List[Union[str, Path]]]: The path to read.\n        output: Union[Type[str], OutputFormat]: The output format.\n        target: OutputType: The output type.\n        verbose: bool: Whether to print verbose output.\n        workers: Optional[int]: The number of workers to use for reading.\n\n    Returns:\n        Union[Document, List[Document], str]: The content.\n    \"\"\"\n    if isinstance(path, list):\n        paths = [_download_if_url(p) for p in path]\n    else:\n        paths = [_download_if_url(path)]\n\n    paths = [Path(p) for p in paths]\n\n    try:\n        if len(paths) == 1 and paths[0].is_file():\n            return _read_single_file(\n                path=paths[0], output=output, target=target, verbose=verbose\n            )\n        else:\n            with ThreadPoolExecutor(max_workers=workers or mp.cpu_count()) as executor:\n                futures = [\n                    executor.submit(_read_single_file, file, output, target, verbose)\n                    for p in paths\n                    for file in (p.glob(\"*\") if p.is_dir() else [p])\n                    if file.is_file()\n                ]\n                results = [future.result() for future in futures]\n            return [result for result in results if result is not None]\n    finally:\n        # Cleanup temporary files\n        for p in paths:\n            if str(p).startswith(\"/tmp/\") and p.is_file():\n                try:\n                    os.remove(p)\n                except Exception as e:\n                    if verbose:\n                        logger.error(f\"Error removing temporary file {p}: {str(e)}\")\n</code></pre>"},{"location":"examples/llm-completions/llm-completions/","title":"LLM Completions","text":""},{"location":"examples/llm-completions/llm-completions/#standard-llm-completions","title":"Standard LLM Completions","text":"<p>The base <code>completion()</code> function runs the <code>zyx</code> library, and provides a simple API for generating</p> <ul> <li>LLM completions for any LiteLLM compatible model</li> <li>Structured outputs using Pydantic models</li> <li>Tool Calling &amp; Automatic Tool Execution</li> </ul> <pre><code>import zyx\n\ndef my_favorite_food(name : str) -&gt; str:\n    \"\"\"Returns the user's favorite food.\"\"\"\n    return \"pizza\"\n\nzyx.completion(\n    \"What is my favorite food?\",\n    model = \"gpt-4o-mini\", # Any LiteLLM model is compatible\n    tools = [my_favorite_food], # Any function, basemodel or openai tool\n    run_tools = True # Optional: automatically executes tools\n    # response_model = BaseModel # Optional: Pydantic model for structured outputs\n    # mode = \"tool_call\" # Optional: Instructor completion mode (refer to instructor docs)\n)\n\n# \"Your favorite food is pizza!\"\n</code></pre>"},{"location":"examples/llm-completions/llm-completions/#passing-messages","title":"Passing Messages","text":"<p>The <code>completion()</code> function can pass messages as both a dictionary &amp; a string.</p> <pre><code>zyx.completion(\n    \"Hello\"\n)\n\n# or\n\nzyx.completion(\n    messages = [\n        { \"role\" : \"system\", \"message\" : \"You are a helpful assistant.\" },\n        { \"role\" : \"user\", \"message\" : \"Hello\" }\n    ]\n)\n</code></pre>"},{"location":"examples/llm-completions/llm-completions/#api-reference-completion","title":"API Reference - <code>completion()</code>","text":"<p>Runs an LLM completion, with tools, streaming or Pydantic structured outputs.</p> <p>Example:</p> <pre><code>```python\ncompletion(\n    messages = messages,\n    model = model,\n    api_key = api_key,\n    base_url = base_url,\n    organization = organization,\n)\n```\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, list[dict]]</code> <p>Union[str, list[dict]]: The messages to complete.</p> <code>None</code> <code>model</code> <code>str</code> <p>str: The model to use.</p> <code>'gpt-4o'</code> <code>api_key</code> <p>Optional[str]: The API key to use.</p> <code>None</code> <code>base_url</code> <p>Optional[str]: The base URL to use.</p> <code>None</code> <code>organization</code> <p>Optional[str]: The organization to use.</p> <code>None</code> <code>response_model</code> <code>Optional[Type[BaseModel]]</code> <p>Optional[Type[BaseModel]]: The Pydantic model to use.</p> <code>None</code> <code>mode</code> <code>Optional[InstructorMode]</code> <p>Optional[InstructorMode]: The Instructor mode to use.</p> <code>'tool_call'</code> <code>max_retries</code> <code>Optional[int]</code> <p>Optional[int]: The maximum number of retries to use.</p> <code>3</code> <code>run_tools</code> <code>Optional[bool]</code> <p>Optional[bool]: Whether to run tools.</p> <code>True</code> <code>tools</code> <code>Optional[List[ToolType]]</code> <p>Optional[List[ToolType]]: The tools to use.</p> <code>None</code> <code>parallel_tool_calls</code> <code>Optional[bool]</code> <p>Optional[bool]: Whether to run tool calls in parallel.</p> <code>False</code> <code>tool_choice</code> <code>Optional[Literal['none', 'auto', 'required']]</code> <p>Optional[Literal[\"none\", \"auto\", \"required\"]]: Whether to run tool calls in parallel.</p> <code>'auto'</code> <code>max_tokens</code> <code>Optional[int]</code> <p>Optional[int]: The maximum number of tokens to use.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>Optional[float]: The temperature to use.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Optional[float]: The top p to use.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Optional[float]: The frequency penalty to use.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Optional[float]: The presence penalty to use.</p> <code>None</code> <code>stop</code> <code>Optional[List[str]]</code> <p>Optional[List[str]]: The stop to use.</p> <code>None</code> <code>stream</code> <code>Optional[bool]</code> <p>Optional[bool]: Whether to stream the completion.</p> <code>False</code> <code>provider</code> <p>Optional[Literal[\"openai\", \"litellm\"]]: The provider to use.</p> required <code>verbose</code> <p>bool: Whether to print verbose output.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>CompletionResponse</code> <code>CompletionResponse</code> <p>The completion response.</p> Source code in <code>zyx/client.py</code> <pre><code>def completion(\n        messages: Union[str, list[dict]] = None,\n        model: str = \"gpt-4o\",\n        api_key : Optional[str] = None,\n        base_url : Optional[str] = None,\n        organization : Optional[str] = None,\n        response_model: Optional[Type[BaseModel]] = None,\n        mode: Optional[InstructorMode] = \"tool_call\",\n        max_retries: Optional[int] = 3,\n        run_tools: Optional[bool] = True,\n        tools: Optional[List[ToolType]] = None,\n        parallel_tool_calls: Optional[bool] = False,\n        tool_choice: Optional[Literal[\"none\", \"auto\", \"required\"]] = \"auto\",\n        max_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        stop: Optional[List[str]] = None,\n        stream: Optional[bool] = False,\n        client : Optional[Literal[\"openai\", \"litellm\"]] = None,\n        verbose : bool = False\n) -&gt; CompletionResponse:\n\n    \"\"\"Runs an LLM completion, with tools, streaming or Pydantic structured outputs.\n\n    Example:\n\n        ```python\n        completion(\n            messages = messages,\n            model = model,\n            api_key = api_key,\n            base_url = base_url,\n            organization = organization,\n        )\n        ```\n\n    Args:\n        messages: Union[str, list[dict]]: The messages to complete.\n        model: str: The model to use.\n        api_key : Optional[str]: The API key to use.\n        base_url : Optional[str]: The base URL to use.\n        organization : Optional[str]: The organization to use.\n        response_model: Optional[Type[BaseModel]]: The Pydantic model to use.\n        mode: Optional[InstructorMode]: The Instructor mode to use.\n        max_retries: Optional[int]: The maximum number of retries to use.\n        run_tools: Optional[bool]: Whether to run tools.\n        tools: Optional[List[ToolType]]: The tools to use.\n        parallel_tool_calls: Optional[bool]: Whether to run tool calls in parallel.\n        tool_choice: Optional[Literal[\"none\", \"auto\", \"required\"]]: Whether to run tool calls in parallel.\n        max_tokens: Optional[int]: The maximum number of tokens to use.\n        temperature: Optional[float]: The temperature to use.\n        top_p: Optional[float]: The top p to use.\n        frequency_penalty: Optional[float]: The frequency penalty to use.\n        presence_penalty: Optional[float]: The presence penalty to use.\n        stop: Optional[List[str]]: The stop to use.\n        stream: Optional[bool]: Whether to stream the completion.\n        provider : Optional[Literal[\"openai\", \"litellm\"]]: The provider to use.\n        verbose : bool: Whether to print verbose output.\n\n    Returns:\n        CompletionResponse: The completion response.\n    \"\"\"\n\n    provider = client\n\n    return Client._completion(\n        messages = messages,\n        model = model,\n        api_key = api_key,\n        base_url = base_url,\n        organization = organization,\n        response_model = response_model,\n        mode = mode,\n        max_retries = max_retries,\n        run_tools = run_tools,\n        tools = tools,\n        parallel_tool_calls = parallel_tool_calls,\n        tool_choice = tool_choice,\n        max_tokens = max_tokens,\n        temperature = temperature,\n        top_p = top_p,\n        frequency_penalty = frequency_penalty,\n        presence_penalty = presence_penalty,\n        stop = stop,\n        stream = stream,\n        provider = provider,\n        verbose = verbose\n    )\n</code></pre>"},{"location":"examples/llm-completions/pydantic-outputs/","title":"Pydantic Outputs","text":""},{"location":"examples/llm-completions/pydantic-outputs/#working-with-pydantic-models","title":"Working with Pydantic Models","text":"<p>The <code>completion()</code> function utilizes the Instructor library to provide a quick option to generate structured outputs using Pydantic.</p> <pre><code>import zyx\nfrom pydantic import BaseModel\n\nclass Character(BaseModel):\n    superhero : str\n    secret_identity : str\n\nzyx.completion(\n    \"Who is spiderman?\",\n    model = \"ollama/llama3.2\",\n    response_model = Character\n)\n</code></pre>"},{"location":"examples/llm-completions/pydantic-outputs/#using-the-basemodel-subclass","title":"Using the BaseModel Subclass","text":"<p><code>zyx</code> has a subclass of Pydantic's <code>BaseModel</code> that has an additional <code>generate()</code> method, specifically built for generating quicker structured outputs with LLMs.</p> <pre><code>from zyx import BaseModel # Subclass of the Pydantic BaseModel\n\nclass Character(BaseModel):\n    superhero : str\n    secret_identity : str\n\nCharacter.generate(\"Who is batman?\")\n\n# Character(superhero=\"Batman\", secret_identity=\"Bruce Wayne\")\n</code></pre>"},{"location":"examples/llm-completions/pydantic-outputs/#generating-batch-synthetic-data","title":"Generating Batch Synthetic Data","text":"<p>You can also generate batch synthetic data using the <code>generate</code> method. Also, instructions are not required when using the generate method. The LLM will be automatically prompted to create synthetic &amp; diverse data.</p> <pre><code>class SyntheticData(BaseModel):\n    food : str\n    chemical_compounds : list[str]\n\nSyntheticData.generate(\n    n=10,\n)\n\n# SyntheticData(\n#     food='Apple',\n#     chemical_compounds=['Quercetin', 'Cyanidin', 'Chlorogenic acid']\n# ),\n# SyntheticData(\n#     food='Banana',\n#     chemical_compounds=['Dopamine', 'Serotonin', 'Catecholamines']\n# ),\n# SyntheticData(\n#     food='Carrot',\n#     chemical_compounds=['Beta-carotene', 'Lutein', 'Zeaxanthin']\n# ), ....\n</code></pre>"},{"location":"examples/llm-completions/pydantic-outputs/#chain-of-thought-with-pydantic-models","title":"Chain of Thought with Pydantic Models","text":"<pre><code>from zyx import BaseModel\n\nclass LearningPlanWeek(BaseModel):\n    tasks : list[str]\n\nclass LearningPlan(BaseModel):\n    goal : str\n    week_1 : LearningPlanWeek\n    week_2 : LearningPlanWeek\n    week_3 : LearningPlanWeek\n\nLearningPlan.generate(\n    \"Generate a cohesive 3 week plan for learning python\",\n    process = \"sequential\" # This generates each field sequentially, emulating chain of thought\n)\n\n\n# [\n#     LearningPlan(\n#         goal='Achieve a 20% increase in sales by the end of Q4.',\n#         week_1=LearningPlanWeek(\n#             tasks=[\n#                 'Conduct market research to identify potential customer segments.',\n#                 'Develop a targeted marketing campaign to reach new customers.',\n#                 'Train the sales team on new sales techniques and product knowledge.',\n#                 'Set weekly sales targets and monitor progress.',\n#                 'Review and analyze sales data to adjust strategies as needed.'\n#             ]\n#         ),\n#         week_2=LearningPlanWeek(\n#             tasks=[\n#                 'Implement the marketing campaign and track its effectiveness.',\n#                 'Host a webinar to showcase products to potential customers.',\n#                 'Follow up with leads generated from the marketing efforts.',\n#                 'Conduct a sales team meeting to discuss progress and challenges.',\n#                 'Gather feedback from the sales team on customer interactions and adjust\n# strategies accordingly.'\n#             ]\n#         ),\n#         week_3=LearningPlanWeek(\n#             tasks=[\n#                 'Analyze the results of the marketing campaign and sales performance.',\n#                 'Identify areas for improvement based on customer feedback and sales\n# data.',\n#                 'Refine the sales strategy to better target high-potential customer\n# segments.',\n#                 'Conduct additional training sessions for the sales team based on\n# identified gaps.',\n#                 'Prepare a report summarizing progress towards the sales goal and next\n# steps.'\n#             ]\n#         )\n#     )\n# ]\n</code></pre>"},{"location":"examples/llm-functions/code-generators/","title":"Code Generators","text":"<p>The <code>function()</code> and <code>code()</code> functions are code generation &amp; execution modules that can be used for generating working python functions and objects from a natural language input. The functions provide the ability to return either a mock or real implementation of the response.</p>"},{"location":"examples/llm-functions/code-generators/#using-generated-code","title":"Using Generated Code","text":"<p>The code function is used to quickly generate python code from a natural language input. The function can be used to generate code snippets, functions, or even entire classes.</p> <pre><code>import zyx\n\nlogger = zyx.code(\"A logger named my logger\", model = \"gpt-3.5-turbo\")\n\nlogger.info(\"Hello world!\")\n</code></pre> <pre><code>2024-09-29 13:17:25,573 - my_logger - INFO - Hello world!\n</code></pre>"},{"location":"examples/llm-functions/code-generators/#generating-functions","title":"Generating Functions","text":"<p>The <code>function()</code> decorator is used for slightly more complex use cases; and executes functions themsevles. Functions must be defined with a docstring.</p> <p>For this example lets begin by creating some data.</p> <pre><code>import zyx\n\n# Lets define some data for this example\ndata = {\n    \"Names\" : ['John', 'Doe', 'Jane'],\n    \"Ages\" : [23, 45, 32],\n    \"Locations\" : ['USA', 'UK', 'Canada'],\n    \"Occupations\" : ['Doctor', 'Engineer', 'Teacher']\n}\n</code></pre> <p>Now lets define a function that will convert this data into a pandas dataframe</p> <pre><code>@zyx.function(verbose = True)\ndef build_names_into_dataframe(data : dict):\n    \"\"\"Creates a proper pandas dataframe from given dictionary data\"\"\"\n\n\ndf = build_names_into_dataframe(data)\n</code></pre> <pre><code>Names  Ages Locations Occupations\n0  John    23       USA      Doctor\n1   Doe    45        UK    Engineer\n2  Jane    32    Canada     Teacher\n</code></pre>"},{"location":"examples/llm-functions/entity-extraction/","title":"Entity Extraction","text":"<p>The <code>extract()</code> function can be used to extract any kind of textual information from a given source. As the LLM is the one performing the task, extraction is semantically aware.</p>"},{"location":"examples/llm-functions/entity-extraction/#using-the-extract-function","title":"Using the <code>extract()</code> function","text":"<pre><code>import zyx\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nzyx.extract(User, \"John is 20 years old\")\n</code></pre> <pre><code>User(name='John', age=20)\n</code></pre>"},{"location":"examples/llm-functions/entity-extraction/#api-reference","title":"API Reference","text":"<p>An LLM abstraction for extracting structured information from text.</p> Example <pre><code>import zyx\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nzyx.extract(User, \"John is 20 years old\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Type[BaseModel]</code> <p>The Pydantic model to extract information into.</p> required <code>text</code> <code>Union[str, List[str]]</code> <p>The text to extract information from.</p> required <code>client</code> <code>Literal['litellm', 'openai']</code> <p>The client to use for extraction. Defaults to \"openai\".</p> <code>'openai'</code> <code>model</code> <code>str</code> <p>The model to use for extraction. Defaults to \"gpt-4o-mini\".</p> <code>'gpt-4o-mini'</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key to use for OpenAI. Defaults to None.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>The base URL for the OpenAI API. Defaults to None.</p> <code>None</code> <code>organization</code> <code>Optional[str]</code> <p>The organization to use for OpenAI. Defaults to None.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens to use for extraction. Defaults to None.</p> <code>None</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries to attempt. Defaults to 3.</p> <code>3</code> <code>temperature</code> <code>float</code> <p>The temperature to use for extraction. Defaults to 0.</p> <code>0</code> <code>mode</code> <code>InstructorMode</code> <p>The mode to use for extraction. Defaults to \"markdown_json_mode\".</p> <code>'markdown_json_mode'</code> <code>process</code> <code>Literal['single', 'batch']</code> <p>The process to use for extraction. Defaults to \"single\".</p> <code>'single'</code> <code>batch_size</code> <code>int</code> <p>The number of texts to extract information from at a time. Defaults to 3.</p> <code>3</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[BaseModel, List[BaseModel]]</code> <p>Union[BaseModel, List[BaseModel]]: The extracted information.</p> Source code in <code>zyx/resources/completions/base/extract.py</code> <pre><code>def extract(\n    target: Type[BaseModel],\n    text: Union[str, List[str]],\n    client: Literal[\"litellm\", \"openai\"] = \"openai\",\n    model: str = \"gpt-4o-mini\",\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    organization: Optional[str] = None,\n    max_tokens: Optional[int] = None,\n    max_retries: int = 3,\n    temperature: float = 0,\n    mode: InstructorMode = \"markdown_json_mode\",\n    process: Literal[\"single\", \"batch\"] = \"single\",\n    batch_size: int = 3,\n    verbose: bool = False,\n) -&gt; Union[BaseModel, List[BaseModel]]:\n    \"\"\"An LLM abstraction for extracting structured information from text.\n\n    Example:\n        ```python\n        import zyx\n\n        class User(BaseModel):\n            name: str\n            age: int\n\n        zyx.extract(User, \"John is 20 years old\")\n        ```\n\n    Args:\n        target (Type[BaseModel]): The Pydantic model to extract information into.\n        text (Union[str, List[str]]): The text to extract information from.\n        client (Literal[\"litellm\", \"openai\"]): The client to use for extraction. Defaults to \"openai\".\n        model (str): The model to use for extraction. Defaults to \"gpt-4o-mini\".\n        api_key (Optional[str]): The API key to use for OpenAI. Defaults to None.\n        base_url (Optional[str]): The base URL for the OpenAI API. Defaults to None.\n        organization (Optional[str]): The organization to use for OpenAI. Defaults to None.\n        max_tokens (Optional[int]): The maximum number of tokens to use for extraction. Defaults to None.\n        max_retries (int): The maximum number of retries to attempt. Defaults to 3.\n        temperature (float): The temperature to use for extraction. Defaults to 0.\n        mode (InstructorMode): The mode to use for extraction. Defaults to \"markdown_json_mode\".\n        process (Literal[\"single\", \"batch\"]): The process to use for extraction. Defaults to \"single\".\n        batch_size (int): The number of texts to extract information from at a time. Defaults to 3.\n        verbose (bool): Whether to print verbose output. Defaults to False.\n\n    Returns:\n        Union[BaseModel, List[BaseModel]]: The extracted information.\n    \"\"\"\n\n    if isinstance(text, str):\n        text = [text]\n\n    if verbose:\n        logger.info(f\"Extracting information from {len(text)} text(s) into {target.__name__} model.\")\n        logger.info(f\"Using model: {model}\")\n        logger.info(f\"Batch size: {batch_size}\")\n        logger.info(f\"Process: {process}\")\n\n    system_message = f\"\"\"\n    You are an information extractor. Your task is to extract relevant information from the given text \n    and fit it into the following Pydantic model:\n\n    {target.model_json_schema()}\n\n    Instructions:\n    - Only Extract information from the text and fit it into the given model.\n    - Do not infer or generate any information that is not present in the input text.\n    - If a required field cannot be filled with information from the text, leave it as None or an empty string as appropriate.\n    \"\"\"\n\n    completion_client = Client(\n        api_key=api_key,\n        base_url=base_url,\n        organization=organization,\n        provider=client,\n        verbose=verbose\n    )\n\n    results = []\n\n    if process == \"single\":\n        response_model = target\n\n        for i in range(0, len(text), batch_size):\n            batch = text[i:i+batch_size]\n            user_message = \"Extract information from the following text(s) and fit it into the given model:\\n\\n\"\n            for idx, t in enumerate(batch, 1):\n                user_message += f\"{idx}. {t}\\n\\n\"\n\n            result = completion_client.completion(\n                messages=[\n                    {\"role\": \"system\", \"content\": system_message},\n                    {\"role\": \"user\", \"content\": user_message}\n                ],\n                model=model,\n                response_model=response_model,\n                mode=mode,\n                max_retries=max_retries,\n                temperature=temperature,\n            )\n\n            results.append(result)\n\n        return results if len(results) &gt; 1 else results[0]\n    else:  # batch process\n        for i in range(0, len(text), batch_size):\n            batch = text[i:i+batch_size]\n            batch_message = \"Extract information from the following texts and fit it into the given model:\\n\\n\"\n            for idx, t in enumerate(batch, 1):\n                batch_message += f\"{idx}. {t}\\n\\n\"\n\n            response_model = create_model(\"ResponseModel\", items=(List[target], ...))\n\n            result = completion_client.completion(\n                messages=[\n                    {\"role\": \"system\", \"content\": system_message},\n                    {\"role\": \"user\", \"content\": batch_message}\n                ],\n                model=model,\n                response_model=response_model,\n                mode=mode,\n                max_retries=max_retries,\n                temperature=temperature,\n            )\n\n            results.extend(result.items)\n\n        return results\n</code></pre>"},{"location":"examples/llm-functions/label-classification/","title":"Label Based Classification","text":"<p>The <code>classify()</code> function is used to classify text inputs based on a set of labels. The function can be used for both single label and multi label classification.</p>"},{"location":"examples/llm-functions/label-classification/#single-label-classification","title":"Single Label Classification","text":"<pre><code>import zyx\n\ninputs = [\n    \"I love programming in Python\",\n    \"I like french fries\"\n]\n\nlabels = [\n    \"code\",\n    \"food\"\n]\n\nresult = zyx.classify(\n    inputs = inputs,\n    labels = labels,\n)\n\nprint(result)\n</code></pre> <pre><code>ClassificationResult(text='I love programming in Python', label='code')\n</code></pre>"},{"location":"examples/llm-functions/label-classification/#multi-label-classification","title":"Multi Label Classification","text":"<pre><code>from zyx import classify\n\ninputs = [\n    \"I love programming in Python and food\",\n    \"I like french fries\"\n]\n\nlabels = [\n    \"code\",\n    \"food\"\n]\n\nresult = classify(\n    inputs = inputs,\n    labels = labels,\n    classification=\"multi\",\n    model=\"openai/gpt-4o\",\n)\n\nprint(result)\n</code></pre> <pre><code>MultiClassificationResult(text='I love programming in Python and food', labels=['code', 'food']),\nMultiClassificationResult(text='I like french fries', labels=['food'])\n</code></pre>"},{"location":"examples/llm-functions/label-classification/#api-reference","title":"API Reference","text":"<p>Classifies given input(s) into one or more of the provided labels.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; classify([\"I love programming in Python\", \"I like french fries\", \"I love programming in Julia\"], [\"code\", \"food\"], classification = \"single\", batch_size = 2, verbose = True)\n[\n    ClassificationResult(text=\"I love programming in Python\", label=\"code\"),\n    ClassificationResult(text=\"I like french fries\", label=\"food\")\n]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, List[str]]</code> <p>The input text to classify.</p> required <code>labels</code> <code>List[str]</code> <p>The list of labels to classify the input text into.</p> required <code>classification</code> <code>Literal['single', 'multi']</code> <p>The type of classification to perform. Defaults to \"single\".</p> <code>'single'</code> <code>n</code> <code>int</code> <p>The number of classifications to generate. Defaults to 1.</p> <code>1</code> <code>batch_size</code> <code>int</code> <p>The number of inputs to classify at a time. Defaults to 3.</p> <code>3</code> <code>model</code> <code>str</code> <p>The model to use for classification. Defaults to \"gpt-4o-mini\".</p> <code>'gpt-4o-mini'</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key to use for OpenAI. Defaults to None.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>The base URL for the OpenAI API. Defaults to None.</p> <code>None</code> <code>organization</code> <code>Optional[str]</code> <p>The organization to use for OpenAI. Defaults to None.</p> <code>None</code> <code>mode</code> <code>InstructorMode</code> <p>The mode to use for classification. Defaults to \"tool_call\".</p> <code>'tool_call'</code> <code>temperature</code> <code>Optional[float]</code> <p>The temperature to use for classification. Defaults to None.</p> <code>None</code> <code>client</code> <code>Optional[Literal['openai', 'litellm']]</code> <p>The client to use for classification. Defaults to None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List</code> <p>List[Union[ClassificationResult, MultiClassificationResult]]: The classifications generated.</p> Source code in <code>zyx/resources/completions/base/classify.py</code> <pre><code>def classify(\n        inputs : Union[str, List[str]],\n        labels : List[str],\n        classification : Literal[\"single\", \"multi\"] = \"single\",\n        n : int = 1,\n        batch_size : int = 3,\n        model : str = \"gpt-4o-mini\",\n        api_key : Optional[str] = None,\n        base_url : Optional[str] = None,\n        organization : Optional[str] = None,\n        mode : InstructorMode = \"tool_call\",\n        temperature : Optional[float] = None,\n        client : Optional[Literal[\"openai\", \"litellm\"]] = None,\n        verbose : bool = False\n) -&gt; List:\n\n    \"\"\"\n    Classifies given input(s) into one or more of the provided labels.\n\n    Examples:\n        &gt;&gt;&gt; classify([\"I love programming in Python\", \"I like french fries\", \"I love programming in Julia\"], [\"code\", \"food\"], classification = \"single\", batch_size = 2, verbose = True)\n        [\n            ClassificationResult(text=\"I love programming in Python\", label=\"code\"),\n            ClassificationResult(text=\"I like french fries\", label=\"food\")\n        ]\n\n    Args:\n        inputs (Union[str, List[str]]): The input text to classify.\n        labels (List[str]): The list of labels to classify the input text into.\n        classification (Literal[\"single\", \"multi\"]): The type of classification to perform. Defaults to \"single\".\n        n (int): The number of classifications to generate. Defaults to 1.\n        batch_size (int): The number of inputs to classify at a time. Defaults to 3.\n        model (str): The model to use for classification. Defaults to \"gpt-4o-mini\".\n        api_key (Optional[str]): The API key to use for OpenAI. Defaults to None.\n        base_url (Optional[str]): The base URL for the OpenAI API. Defaults to None.\n        organization (Optional[str]): The organization to use for OpenAI. Defaults to None.\n        mode (InstructorMode): The mode to use for classification. Defaults to \"tool_call\".\n        temperature (Optional[float]): The temperature to use for classification. Defaults to None.\n        client (Optional[Literal[\"openai\", \"litellm\"]]): The client to use for classification. Defaults to None.\n        verbose (bool): Whether to print verbose output. Defaults to False.\n\n    Returns:\n        List[Union[ClassificationResult, MultiClassificationResult]]: The classifications generated.\n    \"\"\"\n\n    if verbose:\n        logger.info(f\"Classifying {len(inputs)} inputs into {len(labels)} labels.\")\n        logger.info(f\"Using model: {model}\")\n        logger.info(f\"Batch size: {batch_size}\")\n        logger.info(f\"Classification Mode: {classification}\")\n\n    class ClassificationResult(BaseModel):\n        text : str\n        label : str\n\n    class MultiClassificationResult(BaseModel):\n        text : str\n        labels : List[str]\n\n    if classification == \"single\":\n        system_message = f\"\"\"\nYou are a world class text classifier. Your only task is to classify given text into the following categories:\n{', '.join(labels)}\n\nFor each input, you must only provide one label for classification. Each classification you generate must contain\nboth the text and the label you have classified it under.\n\"\"\"\n        if batch_size == 1:\n            response_model = ClassificationResult\n        elif batch_size &gt; 1:\n            response_model = create_model(\n            \"ClassificationResult\",\n                items = (List[ClassificationResult], ...)\n            )\n        else:\n            raise ValueError(\"Batch size must be a positive integer.\")\n    else:\n        system_message = f\"\"\"\nYou are a world class text classifier. Your only task is to classify given text into the following categories:\n{', '.join(labels)}\n\nFor each input, you must provide all labels that apply. Each classification you generate must contain\nboth the text and the label you have classified it under.\n\"\"\"\n        if batch_size == 1:\n            response_model = MultiClassificationResult\n        elif batch_size &gt; 1:\n            response_model = create_model(\n            \"ClassificationResult\",\n                items = (List[MultiClassificationResult], ...)\n            )\n        else:\n            raise ValueError(\"Batch size must be a positive integer.\")\n\n    if isinstance(inputs, str):\n        inputs = [inputs]\n\n    results = []\n\n    completion_client = Client(\n        api_key = api_key,\n        base_url = base_url,\n        organization = organization,\n        provider = client,\n        verbose = verbose\n    )\n\n    for i in range(0, len(inputs), batch_size):\n        batch = inputs[i:i+batch_size]\n\n        user_message = \"Classify the following text(s):\\n\\n\"\n        for idx, text in enumerate(batch, 1):\n            user_message += f\"{idx}. {text}\\n\\n\"\n\n        result = completion_client.completion(\n            messages=[\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": user_message}\n            ],\n            model=model,\n            response_model=response_model,\n            mode=mode,\n            temperature=temperature,\n        )\n\n        if batch_size == 1:\n            results.append(result)\n        else:\n            results.extend(result.items)\n\n    return results if len(results) &gt; 1 else results[0]\n</code></pre>"},{"location":"examples/multimodal/multimodal-generations/","title":"Multimodal Generations","text":"<p><code>zyx</code> leverages simple functions for multimodal generations. These functions have also been built in as LLM compatible tools as well, to provide multimodal tool calling agents.</p>"},{"location":"examples/multimodal/multimodal-generations/#image-generation","title":"Image Generation","text":"<p>Generate images through either the <code>OpenAI</code> or <code>FALAI</code> APIs.</p> <pre><code>from zyx import image\n\nimage(\"An astronaut riding a rainbow unicorn\")\n</code></pre> <p>Generates an image using either the FAL_AI API or OpenAI. With an optional display function to show the image in a notebook.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>str,</p> required <code>model</code> <code>ModelType</code> <p>ModelType = \"dall-e-3\",</p> <code>'dall-e-3'</code> <code>api_key</code> <code>Optional[str]</code> <p>Optional[str] = None,</p> <code>None</code> <code>image_size</code> <code>Optional[str]</code> <p>Optional[str] = \"landscape_4_3\",</p> <code>'landscape_4_3'</code> <code>num_inference_steps</code> <code>Optional[int]</code> <p>Optional[int] = 26,</p> <code>26</code> <code>guidance_scale</code> <code>Optional[float]</code> <p>Optional[float] = 3.5,</p> <code>3.5</code> <code>enable_safety_checker</code> <code>Optional[bool]</code> <p>Optional[bool] = False,</p> <code>False</code> <code>size</code> <code>Optional[str]</code> <p>Optional[str] = \"1024x1024\",</p> <code>'1024x1024'</code> <code>quality</code> <code>Optional[str]</code> <p>Optional[str] = \"standard\",</p> <code>'standard'</code> <code>n</code> <code>Optional[int]</code> <p>Optional[int] = 1,</p> <code>1</code> <code>display</code> <code>Optional[bool]</code> <p>Optional[bool] = False,</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[str, Any]</code> <p>str or Any: The generated image or an error message.</p> Source code in <code>zyx/resources/ext/multimodal.py</code> <pre><code>def image(\n    prompt: str,\n    model: ModelType = \"dall-e-3\",\n    api_key: Optional[str] = None,\n    image_size: Optional[str] = \"landscape_4_3\",\n    num_inference_steps: Optional[int] = 26,\n    guidance_scale: Optional[float] = 3.5,\n    enable_safety_checker: Optional[bool] = False,\n    size: Optional[str] = \"1024x1024\",\n    quality: Optional[str] = \"standard\",\n    n: Optional[int] = 1,\n    display: Optional[bool] = False,\n    optimize_prompt: Optional[bool] = False,\n    optimize_prompt_model: Optional[str] = \"openai/gpt-4o-mini\",\n) -&gt; Union[str, Any]:\n    \"\"\"Generates an image using either the FAL_AI API or OpenAI. With an\n    optional display function to show the image in a notebook.\n\n    Parameters:\n        prompt: str,\n        model: ModelType = \"dall-e-3\",\n        api_key: Optional[str] = None,\n        image_size: Optional[str] = \"landscape_4_3\",\n        num_inference_steps: Optional[int] = 26,\n        guidance_scale: Optional[float] = 3.5,\n        enable_safety_checker: Optional[bool] = False,\n        size: Optional[str] = \"1024x1024\",\n        quality: Optional[str] = \"standard\",\n        n: Optional[int] = 1,\n        display: Optional[bool] = False,\n\n    Returns:\n        str or Any: The generated image or an error message.\n    \"\"\"\n    model_config = _get_model_config(model)\n\n    if model_config[\"provider\"] == \"openai\":\n        from openai import OpenAI\n\n        try:\n            client = OpenAI(api_key=api_key)\n        except Exception as e:\n            return e\n        try:\n            response = client.images.generate(\n                model=model_config[\"model\"],\n                prompt=prompt,\n                size=size,\n                quality=quality,\n                n=n,\n            )\n        except Exception as e:\n            return e\n        if display:\n            try:\n                from IPython.display import display, Image\n            except ImportError:\n                from .. import logger\n\n                logger.critical(\n                    \"The display function requires IPython, which is not included in the base 'zyx' package. Please install it with `pip install ipython`.\"\n                )\n                prompt_cli_install(\"IPython\")\n\n            url = response.data[0].url\n            display(Image(url=url))\n        return response\n\n    elif model_config[\"provider\"] == \"fal\":\n        try:\n            import fal_client\n        except ImportError:\n            from .. import logger\n\n            logger.critical(\n                \"The FAL_AI API requires the 'fal-client' package. Please install it with `pip install fal-client`.\"\n            )\n            prompt_cli_install(\"fal-client\")\n\n        if optimize_prompt:\n            from ... import completion\n            from pydantic import BaseModel\n\n            class OptimizedPrompt(BaseModel):\n                prompt: str\n\n            optimized_prompt = completion(\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": f\"\"\"\n                 ## CONTEXT ## \\n\n                 You are a world class image description optimizer. You enhance descriptions of images at incredible quality, detail, but with a focus on being concise. You define your descritpions\n                 in a comma list of 2-3 word phrases. \\n\\n\n\n                 ## INSTRUCTIONS ## \\n\n                 - You will be given a description of an image\n                 - Reason about the image as a whole, and descriptions the user has provided\n                 - Optimize the prompt for use in image generation\n                 - Ensure that the optimized prompt is a concise, detailed list of 2-3 word phrases.\n\n                 ## EXAMPLE ## \\n\n                 Original Prompt : [ A beautiful landscape painting of a sunset over the ocean. ] \\n\n                 Optimized Prompt : [ A beautiful painting, pink vibrant sunset, dynamic ocean waves, vibrant art, 4k, brush strokes ]\n\n                 \"\"\",\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"Optimize this image description for use in image generation. The original prompt is : [ {prompt} ]\",\n                    },\n                ],\n                model=optimize_prompt_model,\n                response_model=OptimizedPrompt,\n            )\n\n            prompt = optimized_prompt.prompt\n\n        try:\n            handler = fal_client.submit(\n                application=model_config[\"application\"],\n                arguments={\n                    \"prompt\": prompt,\n                    \"image_size\": image_size,\n                    \"num_inference_steps\": num_inference_steps,\n                    \"guidance_scale\": guidance_scale,\n                    \"enable_safety_checker\": enable_safety_checker,\n                    \"num_images\": n,\n                },\n            )\n            result = handler.get()\n            if display:\n                try:\n                    from IPython.display import display, Image\n                except ImportError:\n                    from .. import logger\n\n                    logger.critical(\n                        \"The display function requires IPython, which is not included in the base 'zyx' package. Please install it with `pip install ipython`.\"\n                    )\n                    prompt_cli_install(\"IPython\")\n\n                url = result[\"images\"][0][\"url\"]\n                display(Image(url=url))\n        except Exception as e:\n            result = e\n        return result\n</code></pre>"},{"location":"examples/multimodal/multimodal-generations/#audio-generation","title":"Audio Generation","text":"<p>Use the <code>audio()</code> function to generate audio. This is a direct text -&gt; speech.</p> <pre><code>from zyx import audio\n\naudio(\"Hello, my name is john!\")\n</code></pre> <p>API Reference</p> <p>Generates an audio file from text, through the openai API.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>str,</p> required <code>model</code> <code>OPENAI_TTS_MODELS</code> <p>OPENAI_TTS_MODELS = \"tts-1\",</p> <code>'tts-1'</code> <code>voice</code> <code>OPENAI_TTS_VOICES</code> <p>OPENAI_TTS_VOICES = \"alloy\",</p> <code>'alloy'</code> <code>api_key</code> <code>Optional[str]</code> <p>Optional[str] = None,</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>Optional[str] = None,</p> <code>None</code> <code>filename</code> <code>Optional[str]</code> <p>Optional[str] = None,</p> <code>None</code> <code>play</code> <code>bool</code> <p>bool = False,</p> <code>False</code> <p>Returns:</p> Type Description <p>str or Any: The generated audio file or an error message.</p> Source code in <code>zyx/resources/ext/multimodal.py</code> <pre><code>def audio(\n    prompt: str,\n    model: OPENAI_TTS_MODELS = \"tts-1\",\n    voice: OPENAI_TTS_VOICES = \"alloy\",\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    filename: Optional[str] = None,\n    play: bool = False,\n):\n    \"\"\"Generates an audio file from text, through the openai API.\n\n    Parameters:\n        prompt: str,\n        model: OPENAI_TTS_MODELS = \"tts-1\",\n        voice: OPENAI_TTS_VOICES = \"alloy\",\n        api_key: Optional[str] = None,\n        base_url: Optional[str] = None,\n        filename: Optional[str] = None,\n        play: bool = False,\n\n    Returns:\n        str or Any: The generated audio file or an error message.\n    \"\"\"\n    from openai import OpenAI\n    import io\n\n    try:\n        import sounddevice as sd\n        import soundfile as sf\n    except ImportError:\n        from .. import logger\n\n        logger.critical(\n            \"The [italic]speak[/italic] function requires sounddevice and soundfile, which are not included in the base 'zyx' package. Please install them with [bold]`pip install sounddevice soundfile`[/bold].\"\n        )\n        prompt_cli_install(\"sounddevice soundfile\")\n\n    client = OpenAI(api_key=api_key, base_url=base_url)\n    try:\n        response = client.audio.audio.create(input=prompt, model=model, voice=voice)\n        audio_data = response.read()\n\n        with io.BytesIO(audio_data) as audio_buffer:\n            audio_array, sample_rate = sf.read(audio_buffer)\n\n        if filename:\n            file_endings = [\".wav\", \".mp3\", \".m4a\"]\n            if not filename.endswith(tuple(file_endings)):\n                raise ValueError(\n                    f\"Filename must end with one of the following: {', '.join(file_endings)}\"\n                )\n\n            sf.write(filename, audio_array, sample_rate)\n\n        if play:\n            try:\n                from IPython.display import Audio\n            except ImportError:\n                from .. import logger\n\n                logger.critical(\n                    \"The [italic]play[/italic] function requires IPython, which is not included in the base 'zyx' package. Please install it with [bold]`pip install ipython`[/bold].\"\n                )\n                prompt_cli_install(\"IPython\")\n            # Play audio using sounddevice\n            sd.play(audio_array, sample_rate)\n            sd.wait()\n\n            # For Jupyter notebook, also return IPython audio widget\n            return Audio(audio_array, rate=sample_rate, autoplay=True)\n        else:\n            return audio_array, sample_rate\n\n    except Exception as e:\n        return str(e)\n</code></pre>"},{"location":"examples/multimodal/multimodal-generations/#audio-trancription","title":"Audio Trancription","text":"<p>Use the <code>transcribe()</code> function to convert audio files into text.</p> <pre><code>from zyx import transcribe\n\ntranscribe(\"path/to/audio.mp3\")\n</code></pre> <p>API Reference</p> <p>Transcribes an audio file into text, through the openai API.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>str = \"whisper-1\",</p> <code>'whisper-1'</code> <code>api_key</code> <code>Optional[str]</code> <p>Optional[str] = None,</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>Optional[str] = None,</p> <code>None</code> <code>organization</code> <code>Optional[str]</code> <p>Optional[str] = None,</p> <code>None</code> <code>file</code> <code>Optional[str]</code> <p>Optional[str] = None,</p> <code>None</code> <code>record</code> <code>bool</code> <p>bool = False,</p> <code>False</code> <code>duration</code> <code>int</code> <p>int = 5,</p> <code>5</code> <p>Returns:</p> Type Description <p>str or Any: The transcribed text or an error message.</p> Source code in <code>zyx/resources/ext/multimodal.py</code> <pre><code>def transcribe(\n    model: str = \"whisper-1\",\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    organization: Optional[str] = None,\n    file: Optional[str] = None,\n    record: bool = False,\n    duration: int = 5,\n):\n    \"\"\"Transcribes an audio file into text, through the openai API.\n\n    Parameters:\n        model: str = \"whisper-1\",\n        api_key: Optional[str] = None,\n        base_url: Optional[str] = None,\n        organization: Optional[str] = None,\n        file: Optional[str] = None,\n        record: bool = False,\n        duration: int = 5,\n\n    Returns:\n        str or Any: The transcribed text or an error message.\n    \"\"\"\n    from openai import OpenAI\n\n    try:\n        import sounddevice as sd\n        import soundfile as sf\n    except ImportError:\n        from .. import logger\n\n        logger.critical(\n            \"The [italic]speak[/italic] function requires sounddevice and soundfile, which are not included in the base 'zyx' package. Please install them with [bold]`pip install sounddevice soundfile`[/bold].\"\n        )\n        prompt_cli_install(\"sounddevice soundfile\")\n\n    import io\n\n    client = OpenAI(api_key=api_key, base_url=base_url, organization=organization)\n\n    if record:\n        print(f\"Recording for {duration} seconds...\")\n        audio_data = sd.rec(int(duration * 44100), samplerate=44100, channels=1)\n        sd.wait()\n        print(\"Recording finished.\")\n\n        with io.BytesIO() as buffer:\n            sf.write(buffer, audio_data, 44100, format=\"wav\")\n            buffer.seek(0)\n            audio_file = buffer\n    elif file:\n        if not file.endswith((\".mp3\", \".wav\", \".m4a\")):\n            raise ValueError(\"File must be a .mp3, .wav, or .m4a file\")\n        audio_file = open(file, \"rb\")\n    else:\n        raise ValueError(\n            \"Either 'file' must be provided or 'record' must be set to True\"\n        )\n\n    try:\n        transcription = client.audio.transcriptions.create(\n            model=model, file=audio_file, response_format=\"text\"\n        )\n        return transcription\n    except Exception as e:\n        return str(e)\n</code></pre>"}]}