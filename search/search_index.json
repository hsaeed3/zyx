{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"zyx","text":"<pre><code>pip install zyx --upgrade\n\nzyx\n</code></pre> <p>zyx 0.3.00 - the first 'release' of the library is out!  All CrewAI &amp; other obstructions have been removed, and the library is back to being lightweight.</p> <p><code>zyx</code> is a hyper-fast, fun, &amp; ease-of-use focused Python library for using LLMs.  It was created on top of <code>Instructor</code> and <code>LiteLLM</code>, and focuses to provide an abstraction free framework.  The library uses methods such as lazy-loading to provide a single import for all its features. This library is not meant to be used as a production-ready solution, but rather as a tool to quickly &amp; easily experiment with LLMs. </p> <p>Some of the key features of <code>zyx</code> include:</p> <ul> <li>Universal Completion Client : A singular function that handles all LiteLLM compatible models, Pydantic structured outputs, tool calling &amp; execution, prompt optimization, streaming &amp; vision support.</li> <li>A Large Collection of LLM Powered Functions : This library is inspired by <code>MarvinAI</code>, and it's quick LLM function style framework and has built upon it vastly.</li> <li>Easy to Use Memory (Rag) : A <code>Qdrant</code> wrapper, built to support easy store creation, text/document/pydantic model data insertion, universal embedding provider support, LLM completions for RAG &amp; more.</li> <li>Multimodel Generations : Supports generations for images, audio &amp; speech transcription.</li> <li>Functional / Easy Access Terminal Client : A terminal client built using <code>textual</code> to allow for easy access to <code>zyx</code> features.</li> <li>New Experimental Conversational Multi-Agent Framework : Built from the ground up using <code>Instructor</code>, the agentic framework provides a solution towards conversationally state managed agents, with task creation, custom tool use, artifact creation &amp; more.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#links","title":"Links","text":"<ul> <li>Getting Started</li> </ul>"},{"location":"examples/agents/","title":"Conversational Agents | Very Experimental","text":"<p>New in zyx is a newly created multi-agent framework, with support for tools, planning, supervision and more. </p> <ul> <li>This framework follows the zyx architecture, and is built on top of the <code>LiteLLM</code> &amp; <code>Instructor</code> completion module. </li> <li>This framework presents a new way of looking at a multi-agent system, to be used as a state managed, conversational orchestration. </li> </ul>"},{"location":"examples/agents/#current-features","title":"Current Features:","text":"<pre><code>- ***LLM Based Intent Classification***\n- ***Dynamically Generated Workflows &amp; Tasks***\n- ***Artifact Generation &amp; Guidance (Inspired by Claude)***\n- ***Custom Tool Usage***\n- ***Dynamic Short-Term Memory Management***\n- ***Supervision of Workflows***\n- ***Reflective Analysis of Tools &amp; Workflows***\n</code></pre>"},{"location":"examples/agents/#planned-features","title":"Planned Features:","text":"<pre><code>- ***Long-Term Memory Management (Enter Old Workflows)***\n- ***Add More 'Worker Agents'***\n- ***Graph Based Tool Classification (To allow 'unlimited' tools) (Very Soon)***\n- ***Chat Response Customization (Very Soon)***\n</code></pre> <p>Additional &amp; Complex Customization and Modularity will be introduced in time, dont worry.</p> <p>Below is a high level flowchart of the implementation of this framework.</p> <pre><code>flowchart TD\n    A[on_message] --&gt; B{Workflow state?}\n    B --&gt;|IDLE| C[Handle Chat]\n    B --&gt;|Active| D[Classify Intent]\n\n    D --&gt; E{Tool or Retrieve?}\n    E --&gt;|Tool| F[Use Tool]\n    E --&gt;|Retrieve| G[Retrieve Info]\n    E --&gt;|Neither| H{Stream?}\n\n    H --&gt;|Yes| I[Stream Response]\n    H --&gt;|No| J[Handle Intent]\n\n    J --&gt; K{Intent Type}\n    K --&gt;|Chat| C\n    K --&gt;|Plan| L[Plan]\n    K --&gt;|Execute| M[Execute]\n    K --&gt;|Evaluate| N[Evaluate]\n    K --&gt;|Generate| O[Generate]\n    K --&gt;|Reflect| P[Reflect]\n\n    C --&gt; Q[Update Memory]\n    F --&gt; Q\n    G --&gt; Q\n    I --&gt; Q\n    L --&gt; Q\n    M --&gt; Q\n    N --&gt; Q\n    O --&gt; Q\n    P --&gt; Q\n\n    Q --&gt; R{Generate Artifact?}\n    R --&gt;|Yes| S[Generate Artifact]\n    R --&gt;|No| T[Return Response]\n    S --&gt; T\n\n    F --&gt; U[Select Tool]\n    U --&gt; V[Get Tool Args]\n    V --&gt; W[Execute Tool]\n    W --&gt; Q\n\n    G --&gt; X[Use Retriever Tool]\n    X --&gt; Y[Summarize Retrieved Info]\n    Y --&gt; Q\n\n    classDef default fill:#f9f,stroke:#333,stroke-width:2px;\n    classDef process fill:#bbf,stroke:#333,stroke-width:2px,color:#000,stroke-dasharray: 5 5;\n    classDef decision fill:#ff7,stroke:#333,stroke-width:2px;\n    classDef io fill:#beb,stroke:#333,stroke-width:2px;\n\n    class A,C,F,G,I,L,M,N,O,P,S,U,V,W,X,Y process;\n    class B,E,H,J,K,R decision;\n    class Q io;\n    class T io;</code></pre>"},{"location":"examples/agents/#why-conversational-agents","title":"Why Conversational Agents?","text":"<p>Creating conversational agents, with an ability to save complex state &amp; workflows, allows for an organic way of interacting with LLMs. Currently, most agentic frameworks are focused on single or few shot workflows, diminishing the customization or tweakable ability of the user's desired outcome.</p>"},{"location":"examples/agents/#creating-the-agentic-workflow","title":"Creating the Agentic Workflow","text":"<p>Lets create an agentic pipeline. </p> <p>Tools can be added in two ways, either by the <code>tools=[]</code> argument, or with the optimized <code>retriever_tools=[]</code> argument if tools are retrievers, query engines, etc.</p> <pre><code>import zyx\n\n# Create the workflow\nagents = zyx.Agents(\n    model=\"gpt-4o-mini\",\n    # Lets use some prebuilt zyx tools\n    tools=[\n        zyx.tools.execute_code,  # Code Executor\n        zyx.tools.write_file,  # File Writer\n    ],\n    verbose=True\n)\n</code></pre> <pre><code># OUTPUT\n2024-09-08 02:04:12.779 | INFO     | zyx._client.memory:_create_collection:73 - Collection 'agents_memory' does not exist. Creating it now.\n2024-09-08 02:04:12.779 | INFO     | zyx._client.memory:_create_collection:82 - Collection 'agents_memory' created successfully.\n</code></pre>"},{"location":"examples/agents/#sending-a-message","title":"Sending a Message","text":"<p>Messages will run through an intent classification step, to determine the next action. This allows for the creation of <code>Artifacts</code> &amp; <code>Dynamic Workflows</code>.</p> <pre><code>response = agents.completion(\"Hi\")\n\nprint(response)\n</code></pre> <pre><code>2024-09-08 02:04:13.801 | INFO     | zyx._client.memory:add:156 - Successfully added 1 points to the collection.\n2024-09-08 02:04:14.667 | INFO     | zyx._client.memory:add:156 - Successfully added 1 points to the collection.\n\nHello! How can I assist you today?\n</code></pre> <p>The response was short and simple, as it was classified as a simple chat intent. Lets start asking the agents to help us write some code.</p>"},{"location":"examples/agents/#creating-goals-artifacts","title":"Creating Goals &amp; Artifacts","text":"<p>Goals are optimized towards the type of tools presented to the agents. The framework will reason differently based on the toolset it has access to. </p> <p>In a soon update, the concept behind contextual memory will be implemented for tools, allowing 'infinite' tools to be used.</p> <pre><code>response = agents.completion(\n    \"I want to start writing an HTML page, that has a nice title in a box.\"\n)\n\nprint(response)\n</code></pre> <p>An artifact file was created for us, with our newly created HTML code. Lets view the file.</p> My Nice Title          My Nice Title"},{"location":"examples/agents/#artifact-reflection-regeneration","title":"Artifact Reflection &amp; Regeneration","text":"<p>Lets change the color of our artifact.</p> <pre><code>response = agents.completion(\"Make the title red\")\n\nprint(response)\n</code></pre> <p>Now lets view the file again.</p> My Nice Title          My Nice Title      <p>The agentic framework was able to capture our intent and reflect the changes in our artfiact. This is a specific example, and the framework will adapt to the specific list of tools presented to it.</p>"},{"location":"examples/agents/#lets-change-the-topic","title":"Lets Change the Topic","text":"<pre><code>response = agents.completion(\"I want to get started learning python\")\n\nprint(response)\n</code></pre> <pre><code>2024-09-08 05:34:36.361 | INFO     | zyx._client.memory:add:156 - Successfully added 1 points to the collection.\n2024-09-08 05:34:41.562 | INFO     | zyx._client.memory:add:156 - Successfully added 1 points to the collection.\nTo get started learning Python, you can follow these steps:\n\n1. **Install Python**: Download and install the latest version of Python from the official website \n(https://www.python.org/downloads/).\n\n2. **Set Up an IDE**: Choose an Integrated Development Environment (IDE) to write your code. Popular options \ninclude PyCharm, Visual Studio Code, or even simple text editors like Sublime Text.\n\n3. **Learn the Basics**: Familiarize yourself with the basic syntax, data types, and control structures in Python. \nResources like \"Automate the Boring Stuff with Python\" and Codecademy's Python course are great starting points.\n\n4. **Practice Coding**: Start solving problems on platforms like LeetCode, HackerRank, or Codewars to improve your \ncoding skills.\n\n5. **Build Projects**: Work on small projects that interest you, such as a calculator, web scraper, or simple game,\nto apply what you've learned.\n\n6. **Join a Community**: Engage with other learners by joining forums like Stack Overflow or Reddit's r/learnpython\nto ask questions and share knowledge.\n\n7. **Explore Advanced Topics**: Once you're comfortable with the basics, explore more advanced topics like web \ndevelopment with Flask or Django, data analysis with Pandas, or machine learning with TensorFlow.\n\nRemember, practice and consistency are key to becoming proficient in Python!\n</code></pre>"},{"location":"examples/agents/#now-lets-go-back-to-our-original-artifact","title":"Now lets go back to our original artifact","text":"<p>We'll send a broad message, to test the agent.</p> <pre><code>response = agents.completion(\"Change the title to blue and name is 'My Website!'\")\n\nprint(response)\n</code></pre> <p>Lets view the file again.</p> My Website!          My Website!"},{"location":"examples/completions/","title":"Generating Completions","text":"<p>The primary module of zyx, is the universal <code>.completion()</code> function. This module is an extensive wrapper around the <code>litellm .completion()</code> function, as well as the Instructor library. </p> <p>The <code>.completion()</code> function is capable of</p> <ul> <li>Generations with any LiteLLM compatible model<ul> <li>Ollama, OpenAI, Anthropic, Groq, Mistral, and more!</li> </ul> </li> <li>Direct Instructor Pydantc structured outputs</li> <li>Tool calling &amp; execution support. (Get a tool interpretation with one function)<ul> <li>zyx provides a few prebuilt tools out of the box</li> <li>Can take in a list of Python functions, OpenAI dictionaries, or Pydantic models as tools!</li> <li>Automatic tool execution if a tool is called through the <code>run_tools</code> parameter</li> </ul> </li> <li>Streaming</li> <li>New Vision support <ul> <li>Pass in a list of urls</li> <li>Currently uses multi shot prompting if a response model or tools were also passed.</li> </ul> </li> <li>New Prompt optimization <ul> <li>Creates or optimizes a task tuned system prompt using either the COSTAR or TIDD-EC frameworks automatically.</li> </ul> </li> </ul>"},{"location":"examples/completions/#standard-completion","title":"Standard Completion","text":"<pre><code># Simplest Way to Generate\n# Defaults to \"gpt-4o-mini\" if no model is provided\nfrom zyx import completion\n\nresponse = completion(\"Hi, how are you?\")\n\n# Returns a standard OpenAI style response object\nprint(response.choices[0].message.content)\n</code></pre> <pre><code># Obviously takes in a normal list of messages as well\nresponse = completion([\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n    {\"role\": \"user\", \"content\": \"Hi, how are you?\"}\n])\n</code></pre> <pre><code># OUTPUT\nHello! I'm just a program, but I'm here and ready to help you. How can I assist you today?\n</code></pre>"},{"location":"examples/completions/#instructor-output","title":"Instructor Output","text":"<pre><code>import zyx\nfrom pydantic import BaseModel\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n# We can pass in a system prompt to change the behavior of the assistant\nresponse = zyx.completion(\n    \"Create a mock person\",\n    response_model = Person,\n    mode = \"md_json\" # Change instructor parsing mode with this (default is md_json)\n)\n\n# Lets print the full response object\nprint(response)\n\nprint(f\"Person Name: '{response.name}', Person Age: '{response.age}'\")\n</code></pre> <pre><code># OUTPUT\nPerson(name='John Doe', age=30)\nPerson Name: 'John Doe', Person Age: '30'\n</code></pre>"},{"location":"examples/completions/#tool-calling-execution","title":"Tool Calling &amp; Execution","text":"<pre><code># Lets return a tool call\nimport zyx\n\n# Lets use the prebuilt web search tool!\nresponse = zyx.completion(\n    \"Who won the 2024 Euro Cup Final?\",\n    tools = [zyx.tools.web_search],\n    run_tools = True # Set to true to execute tool calls\n)\n\nprint(response.choices[0].message.content)\n</code></pre> <pre><code># OUTPUT\nSpain won the 2024 Euro Cup Final, defeating England 2-1. The decisive goal was scored by substitute Mikel \nOyarzabal in the 86th minute. This victory marked Spain's fourth European championship title. You can find more \ndetails about the match (https://en.wikipedia.org/wiki/UEFA_Euro_2024_Final).\n</code></pre>"},{"location":"examples/completions/#prompt-optimization","title":"Prompt Optimization","text":"<pre><code># No Optimization\nfrom zyx import completion\n\nresponse = completion(\n    \"Write a story about a man who travels through time to save his family from a deadly disease.\"\n)\n\nprint(response.choices[0].message.content)\n</code></pre> Output <pre><code># OUTPUT\nIn the small town of Eldridge, nestled between rolling hills and shimmering rivers, lived Ethan Forrester, a mild-mannered librarian with an \nextraordinary obsession: time travel. By day, he cataloged dusty old tomes and helped patrons uncover forgotten stories; by night, he scoured \nthe internet and ancient texts for clues that could unlock the secrets of time.\n\nThen came the news that shattered his world. His beloved wife, Clara, and their two children, Lucy and Sam, had contracted a mysterious and \ndeadly disease that was sweeping through the town like wildfire. Despite the doctors\u2019 best efforts and the hospital\u2019s desperate measures, the \nprognosis was grim. Ethan learned that if he did not act fast, he would lose the most precious people in his life.\n\nOne stormy evening, as thunder rumbled ominously outside, Ethan discovered a brittle manuscript hidden behind the row of encyclopedias. It \nspoke of a time-traveling device, built by a forgotten inventor named Archimedes Blake\u2014an intricate metal orb that pulsed with a strange \nenergy. The instructions were hazy, but Ethan understood one thing: he had to find it.\n\nHe scoured every corner of the library for weeks, until, finally, he found an ancient map hidden in the spine of a long-neglected book. The \nmap contained clues leading to Archimedes\u2019s laboratory, which lay abandoned in a remote part of the nearby woods. Driven by desperation, Ethan\npacked supplies and set off into the forest that night.\n\nThe air was thick with mist as he crossed over fallen branches and brambles. Guided by the map\u2019s scribbled notes and an adrenaline-fueled \ndetermination, he reached a tumbledown cottage shrouded in vines and shadows. Inside lay the remnants of Archimedes\u2019s past\u2014a jumble of \nbeakers, gears, and sketches. In the center, on a makeshift table, rested the metal orb, its surface gleaming like a star shrouded in dust.\n\nAfter hours of tinkering and curse-laden frustration, Ethan managed to grasp the orb correctly, recalling the few instructions he had \ndeciphered. His heart raced as he envisioned where he needed to go: the moment before the disease had struck the town; the point where he \ncould warn his family and seek treatment before it ensnared them.\n\nWith a deep breath, Ethan activated the orb. Blinding light enveloped him, and he felt a strange sensation, as if he were being pulled apart \nand stitched back together again. Moments later, he collapsed onto the ground\u2014different leaves under him, a new sense of calm in the air. It \nwas late summer, two months earlier. The sun bathed the town of Eldridge in a warm glow, and he could hear the distant laughter of children \nplaying in the park.\n\nEthan's heart swelled with hope. He set out for home, each step quickening in urgency. He burst through the front door, breathless and \nfrazzled. Clara looked up from the kitchen, her smile melting into confusion.\n\n\u201cEthan? You\u2019re home early!\u201d she exclaimed, wiping her hands on a dish towel.\n\n\u201cClara, we need to talk! The kids\u2014Lucy and Sam\u2014they need to be checked by a doctor. There\u2019s something terrible coming.\u201d\n\nClara raised an eyebrow, bemused by Ethan\u2019s intensity, but he could see the flicker of concern in her eyes. He explained everything\u2014the \ndisease, the timeline, the frantic search for a cure\u2014all while Lucy and Sam played, oblivious to the urgency.\n\nThe following days were a whirlwind. Ethan persuaded Clara to take the children for a full checkup and lab tests. As they waited, Ethan shared\nwith Clara snippets from the future\u2014their family struggles, the joys, and sorrows. He watched as realization dawned on her face about the \nfragility of life. \n\nA week later, the tests came back. Bit by bit, the pieces started to fall into place. With early intervention, the family received treatment \nfrom a specialist flown in from the city. The miraculous news hit them like a fresh spring breeze\u2014the strain had been captured, and a vaccine \nwas developed in record time. The disease had not yet had a chance to strike them down. \n\nAs Ethan watched them heal and grow stronger, he felt a pang of bittersweet longing. He had a profound understanding of time, of its \nrelentless march and its moments of beauty. One evening, as the sun dipped below the horizon, he returned to the woods where the orb lay \nhidden. \n\nClutching it tightly, he whispered a silent thanks for the second chance before burying it deep beneath the earth. He had come for a purpose, \nand now, with a family he cherished more than anything, he was ready to live in the present.\n\nYears later, as he read to Lucy and Sam in the sprawling garden of their home, Ethan smiled fondly at Clara. Life was fragile and precious, \nbut, as he had learned, so was hope. He vowed never to forget the lessons of time and love, believing deeply that every moment shared was the \ngreatest adventure of all.\n</code></pre> <pre><code>from zyx import completion\n\nresponse = completion(\n    \"Write a story about a man who travels through time to save his family from a deadly disease.\",\n    optimize = \"costar\" # Optimize with COSTAR\n)\n\nprint(response.choices[0].message.content)\n</code></pre> Output <pre><code># OUTPUT\n# Chapter 1: A World in Shadows\n\nIn the year 2142, the world had crumbled under the weight of a relentless epidemic. A once-vibrant society now flickered like a dying ember, \nchoked by despair and grief. Ethan was no stranger to this darkness; it haunted his every waking moment. His family\u2014his wife Megan and their \nten-year-old son Lucas\u2014had been caught in the relentless grip of a disease that science had yet to understand, let alone cure. \n\nStanding in the crumbling remnants of their home, Ethan clutched a faded photograph of the three of them\u2014smiling amidst the towering oaks of \ntheir backyard during a summer picnic, the sun breathing life into their laughter. Those moments now felt like whispers from a different life,\nswallowed by an abyss of hopelessness. Each day, he watched as the light dimmed in Megan\u2019s eyes and as Lucas succumbed to fits of coughing \nthat rattled his small frame. \n\nDesperation clawed at Ethan\u2019s chest, gnawing away at his resolve. But buried deep within him lay a flicker of an idea\u2014rumors of time travel, \nof bending the very fabric of reality to find a remedy in the past. He had to try.\n\n# Chapter 2: The Chronosphere\n\nEthan's journey began in the bowels of an ancient research facility, long abandoned yet whispered about among the few who still dared to \ndream. There, he found tales of a machine, the Chronosphere\u2014a device that could pierce the veil of time.\n\nAfter days of rummaging through tattered blueprints and decoding fragmented notes left behind by hopeless scientists, Ethan unearthed a \nworking prototype, a hissing contraption of gears and glowing screens that threatened to expel him into the unknown. Steeling himself, he \nwhispered his intentions, \u201cI need to save my family.\u201d \n\nWith a flick of a switch, the room around him dimmed. An orchestra of energy hummed, thrumming with the promise of travel as spirals of light \nenveloped him. One moment he was standing in the desolation of 2142, and the next, he was hurtling through time, the colors around him \nswirling like oil on water. \n\n# Chapter 3: The Success of the Past\n\nEthan landed in the year 1962, the air thick with the sweetness of blooming flowers and the hum of life\u2014a stark contrast to the sterile \nsilence of his home. The sun blazed in a way it hadn\u2019t done in a long time. His heart raced with both exhilaration and anxiety, but he was \nfocused\u2014his mission was to find that elusive cure.\n\nHe found solace in the bustling energy of the city, absorbing the sights and sounds that felt foreign yet familiar. Guided by flickering \nfragments of his research, he sought out a brilliant scientist, Dr. Elizabeth Hale\u2014a name that stood out in the annals of hope for a disease \ncuriously similar to the one afflicting his family.\n\nAs he approached the tall brick laboratory, he felt the weight of his desperation intensify. Would she believe him? Could she help?\n\n# Chapter 4: Connections to History\n\nEthan's heart pounded as he introduced himself, weaving a tale flirted with half-truths while holding the fragile essence of faith. Dr. Hale \nwas skeptical initially, her brow raised in doubt. Yet, as Ethan shared his knowledge of the disease\u2014its symptoms, its evolution\u2014her \nexpression softened. The allure of a mysterious visitor from the future intrigued her.\n\nDays turned into weeks as they worked side by side. The bond grew, not just a collaboration of science, but a friendship forged in shared \npurpose. They found laughter in long nights spent pouring over research, and for the first time, Ethan began to feel a flicker of hope.\n\nBut with each day, the cruel clock reminded Ethan of the ticking minutes that threatened his family\u2019s existence. He was torn between two \nworlds\u2014one where everything was a struggle and another where he could rewrite the script of fate.\n\n# Chapter 5: The Breaking Point\n\nAs the breakthroughs arose, so did challenges. There were moments where he would catch himself staring longingly at photographs of Megan and \nLucas\u2014haunting visions that circled his mind like vultures. The tension mounted, and with it came ethical dilemmas. Each decision he made had \nthe potential to alter the course of history. Could he risk changing significant events, or worse, losing his family altogether if he \nfaltered?\n\nThe weight of his choices bore down heavily, but he couldn\u2019t afford hesitation. Finally, after a whirlwind of sleepless nights and \nexperiments, they developed a serum\u2014a concoction that shimmered with promise.\n\n# Chapter 6: A Heartbreaking Choice\n\nEthan stood at the threshold of history as he prepared to return. Dr. Hale expressed her unease. \u201cYou have to understand, if you succeed in \nmaking this discovery, it could alter everything. Are you willing to let the past unfold as it is?\u201d \n\nEthan\u2019s heart ached. \u201cI have to try. My family\u2014my son needs me.\u201d The words hung in the air like a promise, heavy and unwavering.\n\nWith the serum secured, Ethan activated the Chronosphere once again, this time with the weight of a lifetime on his shoulders. He surrounded \nhimself with thoughts of Lucas and Megan, channeling every ounce of his love into the device, feeling the pull of time drawing him back to his\ncrumbled present.\n\n# Chapter 7: The Race Against Time\n\nThe whirlwind of colors enveloped him once more, the familiar sensation of being flung through space. He arrived home to a desolate silence, a\nstillness that mirrored the void in his heart. \n\nRushing through the door, he found Megan sprawled on the couch, feverish and pale\u2014the epitome of fragility. Lucas lay sleeping, his small \nchest rising and falling, the sound unsteady.\n\n\u201cPlease,\u201d Ethan whispered, his hands trembling as he prepared the serum. Would it be too late? He administered it without hesitation, pouring \nevery ounce of his fear and love into the moment. \n\nAs the seconds ticked agonizingly by, he held his breath. Slowly, Megan's eyes fluttered open, confusion etching her features. \u201cEthan?\u201d \n\n# Chapter 8: A New Dawn\n\nThe wakeful hours turned into a miracle; he watched as the color gradually returned to Megan\u2019s cheeks and, in time, Lucas awoke with his \nonce-thin voice growing stronger. Joy overrode the remnants of despair\u2014their reunited laughter echoed through the hall, worming its way into \nevery corner of their home.\n\nAs Ethan held them close, he knew things would never be the same. He had altered history, yet in this newfound brightness, he felt intense \ngratitude\u2014an understanding of love that transcended every obstacle.\n\nBut lingering in the back of his mind was an echo of Dr. Hale\u2019s caution. There was no telling what ripples his actions would cause in the \nfuture. Still, in that moment, as Lucas and Megan filled their home with light, Ethan realized his sacrifice was not for nothing. He had \ntraveled through time not just to save his family, but to awaken the unyielding strength of love.\n\nAnd in the chaotic dance of time and fate, love had authored a new chapter worth embracing.\n</code></pre>"},{"location":"examples/completions/#api-reference","title":"API Reference","text":"<p>Runs a tool calling &amp; structured output capable completion or completion chain.</p> Example <pre><code>import zyx\nzyx.completion()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, List[dict]]</code> <p>The messages to send to the model.</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>The model to use for completion.</p> <code>'gpt-4o-mini'</code> <code>response_model</code> <code>Optional[BaseModel]</code> <p>The response model to use for structured output.</p> <code>None</code> <code>mode</code> <code>Optional[ClientModeParams]</code> <p>The mode to use for completion.</p> <code>'md_json'</code> <code>optimize</code> <code>Union[Optimizer, None]</code> <p>The optimization strategy to use for completion.</p> <code>None</code> <code>tools</code> <code>Optional[List[Union[Callable, dict, BaseModel]]]</code> <p>The tools to use for completion.</p> <code>None</code> <code>run_tools</code> <code>Optional[bool]</code> <p>Whether to run the tools.</p> <code>True</code> <code>base_url</code> <code>Optional[str]</code> <p>The base URL to use for the API.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key to use for authentication.</p> <code>None</code> <code>organization</code> <code>Optional[str]</code> <p>The organization to use for the API.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>The top p value for sampling.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>The temperature value for sampling.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens to generate.</p> <code>None</code> <code>max_retries</code> <code>Optional[int]</code> <p>The maximum number of retries for the API call.</p> <code>3</code> <code>params</code> <code>Optional[ClientParams]</code> <p>Additional parameters for the completion.</p> <code>None</code> <code>verbose</code> <code>Optional[bool]</code> <p>Whether to print verbose output.</p> <code>False</code> <code>stream</code> <code>Optional[bool]</code> <p>Whether to stream the response.</p> <code>False</code> <code>image_urls</code> <code>Optional[Union[str, List[str]]]</code> <p>The image URLs to interpret.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>Response</code> <p>The completion response, or a generator if streaming is enabled.</p> Source code in <code>zyx/_client/completion.py</code> <pre><code>def completion(\n    messages: Union[str, List[dict]] = None,\n    model: Optional[str] = \"gpt-4o-mini\",\n    response_model: Optional[BaseModel] = None,\n    mode: Optional[ClientModeParams] = \"md_json\",\n    optimize: Union[Optimizer, None] = None,\n    tools: Optional[List[Union[Callable, dict, BaseModel]]] = None,\n    run_tools: Optional[bool] = True,\n    base_url: Optional[str] = None,\n    api_key: Optional[str] = None,\n    organization: Optional[str] = None,\n    top_p: Optional[float] = None,\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    max_retries: Optional[int] = 3,\n    params: Optional[ClientParams] = None,\n    verbose: Optional[bool] = False,\n    stream: Optional[bool] = False,\n    image_urls: Optional[Union[str, List[str]]] = None,\n    **kwargs,\n) -&gt; Response:\n    \"\"\"\n    Runs a tool calling &amp; structured output capable completion or completion chain.\n\n    Example:\n        ```python\n        import zyx\n        zyx.completion()\n        ```\n\n    Args:\n        messages (Union[str, List[dict]]): The messages to send to the model.\n        model (Optional[str]): The model to use for completion.\n        response_model (Optional[BaseModel]): The response model to use for structured output.\n        mode (Optional[ClientModeParams]): The mode to use for completion.\n        optimize (Union[Optimizer, None]): The optimization strategy to use for completion.\n        tools (Optional[List[Union[Callable, dict, BaseModel]]]): The tools to use for completion.\n        run_tools (Optional[bool]): Whether to run the tools.\n        base_url (Optional[str]): The base URL to use for the API.\n        api_key (Optional[str]): The API key to use for authentication.\n        organization (Optional[str]): The organization to use for the API.\n        top_p (Optional[float]): The top p value for sampling.\n        temperature (Optional[float]): The temperature value for sampling.\n        max_tokens (Optional[int]): The maximum number of tokens to generate.\n        max_retries (Optional[int]): The maximum number of retries for the API call.\n        params (Optional[ClientParams]): Additional parameters for the completion.\n        verbose (Optional[bool]): Whether to print verbose output.\n        stream (Optional[bool]): Whether to stream the response.\n        image_urls (Optional[Union[str, List[str]]]): The image URLs to interpret.\n\n    Returns:\n        Response: The completion response, or a generator if streaming is enabled.\n    \"\"\"\n    import tenacity\n\n    @tenacity.retry(\n        stop=tenacity.stop_after_attempt(2),\n        retry=tenacity.retry_if_exception_type(Exception),\n    )\n    def completion_with_retry(client, current_mode):\n        try:\n            # Run Completion\n            return client.completion(\n                messages=messages,\n                model=model,\n                tools=tools,\n                run_tools=run_tools,\n                response_model=response_model,\n                mode=current_mode,\n                base_url=base_url,\n                api_key=api_key,\n                organization=organization,\n                top_p=top_p,\n                temperature=temperature,\n                max_tokens=max_tokens,\n                max_retries=max_retries,\n                optimize=optimize,\n                params=params,\n                verbose=verbose,\n                image_urls=image_urls,\n                **kwargs,\n            )\n\n        except Exception as e:\n            # Run Tenacity Retry if Instructor Mode is used\n            if response_model:\n                if current_mode == \"md_json\":\n                    new_mode = \"tools\"\n                else:\n                    new_mode = \"md_json\"\n                print(f\"Retrying with mode: {new_mode}\")\n                return completion_with_retry(client, new_mode)\n\n            # Raise Error if Instructor Mode is not used\n            else:\n                raise e\n\n    try:\n        client = CompletionClient()\n        client_params = params or ClientParams(\n            messages=client.format_messages(messages),\n            model=model,\n            tools=client.format_tools(tools),\n            run_tools=run_tools,\n            response_model=response_model,\n            mode=mode,\n            base_url=base_url,\n            api_key=api_key,\n            organization=organization,\n            top_p=top_p,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            max_retries=max_retries,\n            kwargs=kwargs,\n        )\n        client.params = client_params\n\n        if optimize:\n            client.params.messages = client.optimize_system_prompt(\n                optimize=optimize, verbose=verbose\n            )\n\n        if image_urls:\n            image_interpretation = client.interpret_images(image_urls, model)\n            client.params.messages.append(\n                {\"role\": \"assistant\", \"content\": image_interpretation}\n            )\n\n        if stream and not response_model:\n            return client.stream_completion()\n        else:\n            return completion_with_retry(client, mode)\n\n    except Exception as e:\n        print(f\"Error in completion function: {e}\")\n        return None\n</code></pre>"},{"location":"examples/getting-started/","title":"Getting Started","text":"<p>Install the package with pip:</p> <pre><code>pip install zyx --upgrade\n</code></pre>"},{"location":"examples/getting-started/#cli-app","title":"CLI App","text":"<p>Test your <code>zyx</code> installation by running the super quick CLI model hub application, built on <code>Textual</code>. </p> <p>Run the following code in your terminal</p> <pre><code>zyx\n</code></pre> <p></p>"},{"location":"examples/memory/","title":"Memory (RAG)","text":"<p>The zyx library brings an easier way to interact with vector databases &amp; implement RAG pipelines. Using the <code>Qdrant-Client</code> library, we can easily store and query data. The <code>completion</code> client has also been built into the Memory module to allow for LLM (RAG) completions.</p>"},{"location":"examples/memory/#creating-a-store","title":"Creating a Store","text":"<p>Lets create a store to store and query data.</p> <pre><code>import zyx\nfrom pydantic import BaseModel\n\n\nclass Data(BaseModel):\n    memory: str\n    date: str\n\n\nstore = zyx.Memory(\n    collection_name = \"store\",\n    location = \":memory:\",\n    model_class = Data,  # Set the Pydantic model class to use.(Optional)\n)\n</code></pre> <pre><code># OUTPUT\n2024-09-08 00:33:13.825 | INFO     | zyx._client.memory:_create_collection:73 - Collection 'base_store' does not exist. Creating it now.\n2024-09-08 00:33:13.826 | INFO     | zyx._client.memory:_create_collection:82 - Collection 'base_store' created successfully.\n2024-09-08 00:33:13.827 | INFO     | zyx._client.memory:_create_collection:73 - Collection 'pydantic_store' does not exist. Creating it now.\n2024-09-08 00:33:13.827 | INFO     | zyx._client.memory:_create_collection:82 - Collection 'pydantic_store' created successfully.\n</code></pre>"},{"location":"examples/memory/#adding-data","title":"Adding Data","text":"<p>We can add data to the store using the <code>add</code> method.</p> <p>Lets generate some mock data to add to the store.</p> <pre><code>data = zyx.generate(Data, n=10)\n\nprint(data)\n\n# Add it to the store\nstore.add(pydantic_data)\nstore.add(\n    Data(memory=\"My bestie and I went to the park yesterday\", date=\"2023-08-01\")\n)\n</code></pre> <pre><code>[\n    Data(memory='Remember to buy groceries', date='2023-10-01'),\n    Data(memory=\"Doctor's appointment at 3 PM\", date='2023-10-02'),\n    Data(memory='Meeting with the project team', date='2023-10-03'),\n    Data(memory='Call mom', date='2023-10-04'),\n    Data(memory='Finish the report by Friday', date='2023-10-05'),\n    Data(memory='Plan weekend trip', date='2023-10-06'),\n    Data(memory='Attend yoga class', date='2023-10-07'),\n    Data(memory='Submit tax documents', date='2023-10-08'),\n    Data(memory='Grocery shopping list', date='2023-10-09'),\n    Data(memory='Prepare for the presentation', date='2023-10-10')\n]\n\n2024-09-08 00:33:23.230 | INFO     | zyx._client.memory:add:156 - Successfully added 10 points to the collection.\n2024-09-08 00:33:23.364 | INFO     | zyx._client.memory:add:156 - Successfully added 1 points to the collection.\n</code></pre>"},{"location":"examples/memory/#adding-documents","title":"Adding Documents","text":"<p>Document support is also easily available.</p> <pre><code>store.add_docs([\"doc.txt\", \"doc2.txt\"])\n</code></pre>"},{"location":"examples/memory/#querying-data","title":"Querying Data","text":"<p>We can query the store using the <code>query</code> method.</p> <pre><code>results = store.search(\"Did i do anything with my best friend?\")\n\nprint(results.query)\nfor result in results.results:\n    print(result.text)\n</code></pre> <pre><code># OUTPUT\nDid i do anything with my best friend?\nmemory='My bestie and I went to the park yesterday' date='2023-08-01'\nmemory='Call mom' date='2023-10-04'\nmemory='Plan weekend trip' date='2023-10-06'\nmemory='Meeting with the project team' date='2023-10-03'\nmemory='Attend yoga class' date='2023-10-07'\n</code></pre>"},{"location":"examples/memory/#rag-completion","title":"RAG Completion","text":"<p>We can use the <code>completion</code> client to perform RAG completions.</p> <pre><code>response = store.completion(\"Did i do anything with my best friend?\")\n\nprint(response.choices[0].message.content)\n</code></pre> <pre><code>2024-09-08 00:34:39.951 | INFO     | zyx._client.memory:completion:277 - Initial messages: Did i do anything with my best friend?\nYes, according to your memory stored under ID: 80446fe4-acd5-4235-8552-65c01531f84f, you and your \"bestie\" (best \nfriend) went to the park yesterday.\n</code></pre>"},{"location":"examples/memory/#api-reference","title":"API Reference","text":"Source code in <code>zyx/_client/memory.py</code> <pre><code>class Memory:\n    def __init__(\n        self,\n        collection_name: str = \"my_collection\",\n        vector_size: int = 1536,\n        distance: Literal[\"Cosine\", \"Euclid\", \"Dot\"] = \"Cosine\",\n        location: str = \":memory:\",\n        host: str = None,\n        port: int = 6333,\n        embedding_model: str = \"text-embedding-3-small\",\n        embedding_dimensions: Optional[int] = None,\n        embedding_api_key: Optional[str] = None,\n        embedding_api_base: Optional[str] = None,\n        embedding_api_version: Optional[str] = None,\n        model_class: Optional[Type[BaseModel]] = None,\n    ):\n        from qdrant_client.http.models import Distance, VectorParams\n        from qdrant_client import QdrantClient\n\n        self.collection_name = collection_name\n        self.vector_size = vector_size\n        self.distance = Distance(distance)\n        self.location = location\n        self.embedding_model = embedding_model\n        self.embedding_dimensions = embedding_dimensions\n        self.embedding_api_key = embedding_api_key\n        self.embedding_api_base = embedding_api_base\n        self.embedding_api_version = embedding_api_version\n        self.model_class = model_class\n\n        if location == \":memory:\" or host:\n            self.client = QdrantClient(location=location, host=host, port=port)\n        else:\n            self.client = QdrantClient(path=location)\n\n        self._create_collection()\n\n    def _create_collection(self):\n        from qdrant_client.http.models import VectorParams\n\n        try:\n            self.client.get_collection(self.collection_name)\n            logger.info(f\"Collection '{self.collection_name}' already exists.\")\n        except Exception as e:\n            logger.info(\n                f\"Collection '{self.collection_name}' does not exist. Creating it now.\"\n            )\n            self.client.create_collection(\n                collection_name=self.collection_name,\n                vectors_config=VectorParams(\n                    size=self.vector_size, distance=self.distance\n                ),\n            )\n            logger.info(f\"Collection '{self.collection_name}' created successfully.\")\n\n    def _get_embedding(self, text: str) -&gt; List[float]:\n        from litellm.main import embedding as litellm_embedding\n\n        try:\n            response = litellm_embedding(\n                model=self.embedding_model,\n                input=[text],\n                dimensions=self.embedding_dimensions,\n                api_key=self.embedding_api_key,\n                api_base=self.embedding_api_base,\n                api_version=self.embedding_api_version,\n            )\n\n            embedding_data = response.get(\"data\", None)\n            if (\n                embedding_data\n                and isinstance(embedding_data, list)\n                and len(embedding_data) &gt; 0\n            ):\n                embedding_vector = embedding_data[0].get(\"embedding\", None)\n                if isinstance(embedding_vector, list) and all(\n                    isinstance(x, float) for x in embedding_vector\n                ):\n                    return embedding_vector\n                else:\n                    raise ValueError(\n                        \"Invalid embedding format: Expected a list of floats within the 'embedding' key\"\n                    )\n            else:\n                raise ValueError(\"Embedding data is missing or improperly formatted\")\n        except Exception as e:\n            logger.error(f\"Error generating embedding: {e}\")\n            raise\n\n    def add(\n        self,\n        data: Union[str, List[str], BaseModel, List[BaseModel]],\n        metadata: Optional[dict] = None,\n    ):\n        from qdrant_client.http.models import PointStruct\n\n        if isinstance(data, str):\n            data = [data]\n        elif isinstance(data, BaseModel):\n            data = [data]\n\n        points = []\n        for item in data:\n            try:\n                if isinstance(item, BaseModel):\n                    text = json.dumps(item.dict())\n                    metadata = item.dict()\n                else:\n                    text = item\n\n                embedding_vector = self._get_embedding(text)\n                point = PointStruct(\n                    id=str(uuid.uuid4()),\n                    vector=embedding_vector,\n                    payload={\n                        \"text\": text,\n                        \"metadata\": metadata or {},\n                        \"is_model\": isinstance(item, BaseModel),\n                    },\n                )\n                points.append(point)\n            except Exception as e:\n                logger.error(f\"Error processing item: {item}. Error: {e}\")\n\n        if points:\n            try:\n                self.client.upsert(collection_name=self.collection_name, points=points)\n                logger.info(\n                    f\"Successfully added {len(points)} points to the collection.\"\n                )\n            except Exception as e:\n                logger.error(f\"Error upserting points to collection: {e}\")\n        else:\n            logger.warning(\"No valid points to add to the collection.\")\n\n    def add_docs(self, file_paths: Union[str, List[str]]):\n        from pathlib import Path\n        from semchunk import chunkerify\n\n        if isinstance(file_paths, str):\n            file_paths = [file_paths]\n\n        for file_path in file_paths:\n            path = Path(file_path)\n            if not path.is_file():\n                logger.warning(f\"'{file_path}' is not a valid file. Skipping.\")\n                continue\n\n            try:\n                with path.open(\"r\", encoding=\"utf-8\") as file:\n                    content = file.read()\n\n                # Initialize chunker\n                chunker = chunkerify(self.embedding_model, chunk_size=self.vector_size)\n                chunks = chunker(content)\n\n                document_id = str(uuid.uuid4())\n                for chunk in chunks:\n                    embedding = self._get_embedding(chunk)\n                    document = Document(\n                        document_id=document_id,\n                        text=chunk,\n                        metadata={\"file_path\": str(path)},\n                    )\n\n                    point = rest.PointStruct(\n                        id=str(uuid.uuid4()),\n                        vector=embedding,\n                        payload={\n                            \"text\": chunk,\n                            \"metadata\": document.metadata,\n                            \"document_id\": document.document_id,\n                        },\n                    )\n\n                    self.client.upsert(\n                        collection_name=self.collection_name, points=[point]\n                    )\n\n                logger.info(f\"Successfully processed and added document: {path}\")\n            except Exception as e:\n                logger.error(f\"Error processing document {path}: {e}\")\n\n    def search(self, query: str, top_k: int = 5) -&gt; SearchResponse:\n        try:\n            query_vector = self._get_embedding(query)\n            search_results = self.client.search(\n                collection_name=self.collection_name,\n                query_vector=query_vector,\n                limit=top_k,\n            )\n\n            nodes = []\n            for result in search_results:\n                payload = result.payload\n                if payload.get(\"is_model\", False) and self.model_class:\n                    # This is a stored BaseModel instance\n                    model_data = json.loads(payload.get(\"text\", \"{}\"))\n                    model_instance = self.model_class(**model_data)\n                    node = QdrantNode(\n                        id=str(result.id),\n                        text=str(model_instance),\n                        embedding=query_vector,\n                        metadata={\"model_data\": model_data},\n                    )\n                elif \"document_id\" in payload:\n                    # This is a document chunk\n                    node = QdrantNode(\n                        id=str(result.id),\n                        text=payload.get(\"text\", \"\"),\n                        embedding=query_vector,\n                        metadata={\n                            \"document_id\": payload[\"document_id\"],\n                            \"file_path\": payload[\"metadata\"][\"file_path\"],\n                        },\n                    )\n                else:\n                    # This is a regular node\n                    node = QdrantNode(\n                        id=str(result.id),\n                        text=payload.get(\"text\", \"\"),\n                        embedding=query_vector,\n                        metadata=payload.get(\"metadata\", {}),\n                    )\n                nodes.append(node)\n            return SearchResponse(query=query, results=nodes)\n        except Exception as e:\n            logger.error(f\"Error during search: {e}\")\n            raise\n\n    def completion(\n        self,\n        messages: Union[str, list[dict[str, str]]] = None,\n        model: Optional[str] = \"gpt-4o-mini\",\n        top_k: Optional[int] = 5,\n        tools: Optional[List[Union[Callable, dict, BaseModel]]] = None,\n        run_tools: Optional[bool] = True,\n        response_model: Optional[BaseModel] = None,\n        mode: Optional[ClientModeParams] = \"tools\",\n        base_url: Optional[str] = None,\n        api_key: Optional[str] = None,\n        organization: Optional[str] = None,\n        top_p: Optional[float] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        max_retries: Optional[int] = 3,\n        verbose: Optional[bool] = False,\n    ):\n        logger.info(f\"Initial messages: {messages}\")\n\n        # Unwrap the extra array if present\n        if (\n            isinstance(messages, list)\n            and len(messages) == 1\n            and isinstance(messages[0], list)\n        ):\n            messages = messages[0]\n            logger.info(f\"Unwrapped messages from extra array: {messages}\")\n\n        if isinstance(messages, str):\n            messages = [{\"role\": \"user\", \"content\": messages}]\n        elif isinstance(messages, list):\n            if not messages:\n                raise ValueError(\"Messages list is empty\")\n            # Ensure each item in the list is a dictionary\n            messages = [\n                {\"role\": \"user\", \"content\": m} if isinstance(m, str) else m\n                for m in messages\n            ]\n            if not all(\n                isinstance(m, dict) and \"role\" in m and \"content\" in m for m in messages\n            ):\n                raise ValueError(\n                    \"Invalid message format. Expected list of dicts with 'role' and 'content' keys\"\n                )\n        else:\n            raise ValueError(\"Invalid message format. Expected str or list of dicts\")\n\n        if verbose:\n            logger.info(f\"Formatted messages: {messages}\")\n\n        query = messages[-1].get(\"content\", \"\") if messages else \"\"\n\n        try:\n            results = self.search(query, top_k=top_k)\n        except Exception as e:\n            logger.error(f\"Error during search: {e}\")\n            results = SearchResponse(query=query)\n\n        results_content = []\n        for result in results.results:\n            metadata = result.metadata\n            metadata_str = \", \".join(\n                [f\"{key}: {value}\" for key, value in metadata.items()]\n            )\n            results_content.append(f\"Text: {result.text}, Metadata: {metadata_str}\")\n\n        if verbose:\n            logger.info(f\"Search results: {results_content}\")\n\n        if messages:\n            if not any(message.get(\"role\", \"\") == \"system\" for message in messages):\n                system_message = {\n                    \"role\": \"system\",\n                    \"content\": f\"You have retrieved the following relevant information. Use only if relevant {str(results_content)}\",\n                }\n                messages.insert(0, system_message)\n                if verbose:\n                    logger.info(f\"Inserted system message: {messages}\")\n            else:\n                for message in messages:\n                    if message.get(\"role\", \"\") == \"system\":\n                        message[\"content\"] += (\n                            f\" You have retrieved the following relevant information. Use only if relevant {str(results_content)}\"\n                        )\n                        if verbose:\n                            logger.info(f\"Updated system message: {messages}\")\n                        break\n\n        if verbose:\n            logger.info(f\"Final messages before ClientParams: {messages}\")\n\n        from .completion import CompletionClient\n\n        messages = CompletionClient.format_messages(messages=messages)\n\n        try:\n            from .completion import completion\n\n            result = completion(\n                messages=messages,\n                model=model,\n                tools=tools,\n                run_tools=run_tools,\n                response_model=response_model,\n                mode=mode,\n                base_url=base_url,\n                api_key=api_key,\n                organization=organization,\n                top_p=top_p,\n                temperature=temperature,\n                max_tokens=max_tokens,\n                max_retries=max_retries,\n                verbose=verbose,\n            )\n\n            if verbose:\n                logger.info(f\"Completion result: {result}\")\n\n            return result\n        except Exception as e:\n            logger.error(f\"Error during completion: {e}\")\n            logger.exception(\"Full traceback:\")\n            raise\n\n    def save(self, path: Optional[str] = None):\n        from qdrant_client.qdrant_client import QdrantClient\n        from pathlib import Path\n        import json\n\n        if self.location != \":memory:\":\n            logger.warning(\"This Memory instance is not in-memory. No need to save.\")\n            return\n\n        if path is None:\n            home_dir = Path.home()\n            path = home_dir / \".zyx\" / \"memories\"\n\n        path = Path(path)\n        path.mkdir(parents=True, exist_ok=True)\n\n        collection_path = path / self.collection_name\n        collection_path.mkdir(exist_ok=True)\n\n        try:\n            # Save collection configuration\n            config = {\n                \"vector_size\": self.vector_size,\n                \"distance\": self.distance.name,\n                \"embedding_model\": self.embedding_model,\n                \"embedding_dimensions\": self.embedding_dimensions,\n            }\n            with open(collection_path / \"config.json\", \"w\") as f:\n                json.dump(config, f)\n\n            # Save collection data\n            self.client.snapshot(\n                collection_name=self.collection_name,\n                snapshot_path=str(collection_path / \"snapshot\"),\n            )\n\n            logger.info(f\"Successfully saved Memory to {collection_path}\")\n\n            # Update the client to use the new location\n            self.location = str(collection_path)\n            self.client = QdrantClient(path=self.location)\n\n        except Exception as e:\n            logger.error(f\"Error saving Memory: {e}\")\n            raise\n\n    def get_model_instances(self, query: str, top_k: int = 5) -&gt; List[Any]:\n        if not self.model_class:\n            raise ValueError(\"No model class specified for this Memory instance\")\n\n        search_response = self.search(query, top_k)\n        model_instances = []\n\n        for result in search_response.results:\n            if \"model_data\" in result.metadata:\n                model_instance = self.model_class(**result.metadata[\"model_data\"])\n                model_instances.append(model_instance)\n\n        return model_instances\n</code></pre>"},{"location":"examples/multimodal/","title":"Multimodal Generations","text":""},{"location":"examples/multimodal/#image","title":".image()","text":"<p>Generates an image using either the FAL_AI API or OpenAI. With an optional display function to show the image in a notebook.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>str,</p> required <code>model</code> <code>ModelType</code> <p>ModelType = \"dall-e-3\",</p> <code>'dall-e-3'</code> <code>api_key</code> <code>Optional[str]</code> <p>Optional[str] = None,</p> <code>None</code> <code>image_size</code> <code>Optional[str]</code> <p>Optional[str] = \"landscape_4_3\",</p> <code>'landscape_4_3'</code> <code>num_inference_steps</code> <code>Optional[int]</code> <p>Optional[int] = 26,</p> <code>26</code> <code>guidance_scale</code> <code>Optional[float]</code> <p>Optional[float] = 3.5,</p> <code>3.5</code> <code>enable_safety_checker</code> <code>Optional[bool]</code> <p>Optional[bool] = False,</p> <code>False</code> <code>size</code> <code>Optional[str]</code> <p>Optional[str] = \"1024x1024\",</p> <code>'1024x1024'</code> <code>quality</code> <code>Optional[str]</code> <p>Optional[str] = \"standard\",</p> <code>'standard'</code> <code>n</code> <code>Optional[int]</code> <p>Optional[int] = 1,</p> <code>1</code> <code>display</code> <code>Optional[bool]</code> <p>Optional[bool] = False,</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[str, Any]</code> <p>str or Any: The generated image or an error message.</p> Source code in <code>zyx/_client/multimodal.py</code> <pre><code>def image(\n    prompt: str,\n    model: ModelType = \"dall-e-3\",\n    api_key: Optional[str] = None,\n    image_size: Optional[str] = \"landscape_4_3\",\n    num_inference_steps: Optional[int] = 26,\n    guidance_scale: Optional[float] = 3.5,\n    enable_safety_checker: Optional[bool] = False,\n    size: Optional[str] = \"1024x1024\",\n    quality: Optional[str] = \"standard\",\n    n: Optional[int] = 1,\n    display: Optional[bool] = False,\n    optimize_prompt: Optional[bool] = False,\n    optimize_prompt_model: Optional[str] = \"openai/gpt-4o-mini\",\n) -&gt; Union[str, Any]:\n    \"\"\"Generates an image using either the FAL_AI API or OpenAI. With an\n    optional display function to show the image in a notebook.\n\n    Parameters:\n        prompt: str,\n        model: ModelType = \"dall-e-3\",\n        api_key: Optional[str] = None,\n        image_size: Optional[str] = \"landscape_4_3\",\n        num_inference_steps: Optional[int] = 26,\n        guidance_scale: Optional[float] = 3.5,\n        enable_safety_checker: Optional[bool] = False,\n        size: Optional[str] = \"1024x1024\",\n        quality: Optional[str] = \"standard\",\n        n: Optional[int] = 1,\n        display: Optional[bool] = False,\n\n    Returns:\n        str or Any: The generated image or an error message.\n    \"\"\"\n    model_config = _get_model_config(model)\n\n    if model_config[\"provider\"] == \"openai\":\n        from openai import OpenAI\n\n        try:\n            client = OpenAI(api_key=api_key)\n        except Exception as e:\n            return e\n        try:\n            response = client.images.generate(\n                model=model_config[\"model\"],\n                prompt=prompt,\n                size=size,\n                quality=quality,\n                n=n,\n            )\n        except Exception as e:\n            return e\n        if display:\n            try:\n                from IPython.display import display, Image\n            except ImportError:\n                from .. import logger\n                from .utils.prompt_cli_install import prompt_cli_install\n\n                logger.critical(\n                    \"The display function requires IPython, which is not included in the base 'zyx' package. Please install it with `pip install ipython`.\"\n                )\n                prompt_cli_install(\"IPython\")\n\n            url = response.data[0].url\n            display(Image(url=url))\n        return response\n\n    elif model_config[\"provider\"] == \"fal\":\n        try:\n            import fal_client\n        except ImportError:\n            from .. import logger\n            from .utils.prompt_cli_install import prompt_cli_install\n\n            logger.critical(\n                \"The FAL_AI API requires the 'fal-client' package. Please install it with `pip install fal-client`.\"\n            )\n            prompt_cli_install(\"fal-client\")\n\n        if optimize_prompt:\n            from .completion import completion\n            from pydantic import BaseModel\n\n            class OptimizedPrompt(BaseModel):\n                prompt: str\n\n            optimized_prompt = completion(\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": f\"\"\"\n                 ## CONTEXT ## \\n\n                 You are a world class image description optimizer. You enhance descriptions of images at incredible quality, detail, but with a focus on being concise. You define your descritpions\n                 in a comma list of 2-3 word phrases. \\n\\n\n\n                 ## INSTRUCTIONS ## \\n\n                 - You will be given a description of an image\n                 - Reason about the image as a whole, and descriptions the user has provided\n                 - Optimize the prompt for use in image generation\n                 - Ensure that the optimized prompt is a concise, detailed list of 2-3 word phrases.\n\n                 ## EXAMPLE ## \\n\n                 Original Prompt : [ A beautiful landscape painting of a sunset over the ocean. ] \\n\n                 Optimized Prompt : [ A beautiful painting, pink vibrant sunset, dynamic ocean waves, vibrant art, 4k, brush strokes ]\n\n                 \"\"\",\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"Optimize this image description for use in image generation. The original prompt is : [ {prompt} ]\",\n                    },\n                ],\n                model=optimize_prompt_model,\n                response_model=OptimizedPrompt,\n            )\n\n            prompt = optimized_prompt.prompt\n\n        try:\n            handler = fal_client.submit(\n                application=model_config[\"application\"],\n                arguments={\n                    \"prompt\": prompt,\n                    \"image_size\": image_size,\n                    \"num_inference_steps\": num_inference_steps,\n                    \"guidance_scale\": guidance_scale,\n                    \"enable_safety_checker\": enable_safety_checker,\n                    \"num_images\": n,\n                },\n            )\n            result = handler.get()\n            if display:\n                try:\n                    from IPython.display import display, Image\n                except ImportError:\n                    from .. import logger\n                    from .utils.prompt_cli_install import prompt_cli_install\n\n                    logger.critical(\n                        \"The display function requires IPython, which is not included in the base 'zyx' package. Please install it with `pip install ipython`.\"\n                    )\n                    prompt_cli_install(\"IPython\")\n\n                url = result[\"images\"][0][\"url\"]\n                display(Image(url=url))\n        except Exception as e:\n            result = e\n        return result\n</code></pre>"},{"location":"examples/multimodal/#speak","title":".speak()","text":"<p>Generates an audio file from text, through the openai API.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>str,</p> required <code>model</code> <code>OPENAI_TTS_MODELS</code> <p>OPENAI_TTS_MODELS = \"tts-1\",</p> <code>'tts-1'</code> <code>voice</code> <code>OPENAI_TTS_VOICES</code> <p>OPENAI_TTS_VOICES = \"alloy\",</p> <code>'alloy'</code> <code>api_key</code> <code>Optional[str]</code> <p>Optional[str] = None,</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>Optional[str] = None,</p> <code>None</code> <code>filename</code> <code>Optional[str]</code> <p>Optional[str] = None,</p> <code>None</code> <code>play</code> <code>bool</code> <p>bool = False,</p> <code>False</code> <p>Returns:</p> Type Description <p>str or Any: The generated audio file or an error message.</p> Source code in <code>zyx/_client/multimodal.py</code> <pre><code>def speak(\n    prompt: str,\n    model: OPENAI_TTS_MODELS = \"tts-1\",\n    voice: OPENAI_TTS_VOICES = \"alloy\",\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    filename: Optional[str] = None,\n    play: bool = False,\n):\n    \"\"\"Generates an audio file from text, through the openai API.\n\n    Parameters:\n        prompt: str,\n        model: OPENAI_TTS_MODELS = \"tts-1\",\n        voice: OPENAI_TTS_VOICES = \"alloy\",\n        api_key: Optional[str] = None,\n        base_url: Optional[str] = None,\n        filename: Optional[str] = None,\n        play: bool = False,\n\n    Returns:\n        str or Any: The generated audio file or an error message.\n    \"\"\"\n    from openai import OpenAI\n    import io\n\n    try:\n        import sounddevice as sd\n        import soundfile as sf\n    except ImportError:\n        from .. import logger\n        from .utils.prompt_cli_install import prompt_cli_install\n\n        logger.critical(\n            \"The [italic]speak[/italic] function requires sounddevice and soundfile, which are not included in the base 'zyx' package. Please install them with [bold]`pip install sounddevice soundfile`[/bold].\"\n        )\n        prompt_cli_install(\"sounddevice soundfile\")\n\n    client = OpenAI(api_key=api_key, base_url=base_url)\n    try:\n        response = client.audio.speech.create(input=prompt, model=model, voice=voice)\n        audio_data = response.read()\n\n        with io.BytesIO(audio_data) as audio_buffer:\n            audio_array, sample_rate = sf.read(audio_buffer)\n\n        if filename:\n            file_endings = [\".wav\", \".mp3\", \".m4a\"]\n            if not filename.endswith(tuple(file_endings)):\n                raise ValueError(\n                    f\"Filename must end with one of the following: {', '.join(file_endings)}\"\n                )\n\n            sf.write(filename, audio_array, sample_rate)\n\n        if play:\n            try:\n                from IPython.display import Audio\n            except ImportError:\n                from .. import logger\n                from .utils.prompt_cli_install import prompt_cli_install\n\n                logger.critical(\n                    \"The [italic]play[/italic] function requires IPython, which is not included in the base 'zyx' package. Please install it with [bold]`pip install ipython`[/bold].\"\n                )\n                prompt_cli_install(\"IPython\")\n            # Play audio using sounddevice\n            sd.play(audio_array, sample_rate)\n            sd.wait()\n\n            # For Jupyter notebook, also return IPython audio widget\n            return Audio(audio_array, rate=sample_rate, autoplay=True)\n        else:\n            return audio_array, sample_rate\n\n    except Exception as e:\n        return str(e)\n</code></pre>"},{"location":"examples/multimodal/#transcribe","title":".transcribe()","text":"<p>Transcribes an audio file into text, through the openai API.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>str = \"whisper-1\",</p> <code>'whisper-1'</code> <code>api_key</code> <code>Optional[str]</code> <p>Optional[str] = None,</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>Optional[str] = None,</p> <code>None</code> <code>organization</code> <code>Optional[str]</code> <p>Optional[str] = None,</p> <code>None</code> <code>file</code> <code>Optional[str]</code> <p>Optional[str] = None,</p> <code>None</code> <code>record</code> <code>bool</code> <p>bool = False,</p> <code>False</code> <code>duration</code> <code>int</code> <p>int = 5,</p> <code>5</code> <p>Returns:</p> Type Description <p>str or Any: The transcribed text or an error message.</p> Source code in <code>zyx/_client/multimodal.py</code> <pre><code>def transcribe(\n    model: str = \"whisper-1\",\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    organization: Optional[str] = None,\n    file: Optional[str] = None,\n    record: bool = False,\n    duration: int = 5,\n):\n    \"\"\"Transcribes an audio file into text, through the openai API.\n\n    Parameters:\n        model: str = \"whisper-1\",\n        api_key: Optional[str] = None,\n        base_url: Optional[str] = None,\n        organization: Optional[str] = None,\n        file: Optional[str] = None,\n        record: bool = False,\n        duration: int = 5,\n\n    Returns:\n        str or Any: The transcribed text or an error message.\n    \"\"\"\n    from openai import OpenAI\n\n    try:\n        import sounddevice as sd\n        import soundfile as sf\n    except ImportError:\n        from .. import logger\n        from .utils.prompt_cli_install import prompt_cli_install\n\n        logger.critical(\n            \"The [italic]speak[/italic] function requires sounddevice and soundfile, which are not included in the base 'zyx' package. Please install them with [bold]`pip install sounddevice soundfile`[/bold].\"\n        )\n        prompt_cli_install(\"sounddevice soundfile\")\n\n    import io\n\n    client = OpenAI(api_key=api_key, base_url=base_url, organization=organization)\n\n    if record:\n        print(f\"Recording for {duration} seconds...\")\n        audio_data = sd.rec(int(duration * 44100), samplerate=44100, channels=1)\n        sd.wait()\n        print(\"Recording finished.\")\n\n        with io.BytesIO() as buffer:\n            sf.write(buffer, audio_data, 44100, format=\"wav\")\n            buffer.seek(0)\n            audio_file = buffer\n    elif file:\n        if not file.endswith((\".mp3\", \".wav\", \".m4a\")):\n            raise ValueError(\"File must be a .mp3, .wav, or .m4a file\")\n        audio_file = open(file, \"rb\")\n    else:\n        raise ValueError(\n            \"Either 'file' must be provided or 'record' must be set to True\"\n        )\n\n    try:\n        transcription = client.audio.transcriptions.create(\n            model=model, file=audio_file, response_format=\"text\"\n        )\n        return transcription\n    except Exception as e:\n        return str(e)\n</code></pre>"},{"location":"examples/llm/code/","title":"Generative Code Functions","text":"<p><code>zyx</code> comes with a set of functions to help you generate and execute code, making it easier to get things done in a weirder way. This builds off of the concept provided by Marvin's <code>.fn()</code> function.</p>"},{"location":"examples/llm/code/#function","title":".function()","text":"<p>Create a generative function</p> <pre><code>from zyx import function\n\n# .function() is a decorator, that generates your defined functions\n@function()\ndef create_pandas_df(data: list[dict]):\n    \"\"\"\n    Creates a pandas dataframe from a list of dictionaries.\n    \"\"\"\n\ndata = [\n    {\"name\": \"John\", \"age\": 20},\n    {\"name\": \"Jane\", \"age\": 21},\n    {\"name\": \"Doe\", \"age\": 22},\n]\n\ndf = create_pandas_df(data)\n\nprint(df.head())\n</code></pre> <pre><code># OUTPUT\n   name  age\n0  John   20\n1  Jane   21\n2   Doe   22\n</code></pre> <p>A quick abstraction to create both mock &amp; generated runnable python functions, using LLMs.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The model to use for the function.</p> <code>'gpt-4o-mini'</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key to use for the function.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>The base URL to use for the function.</p> <code>None</code> <code>mode</code> <code>Literal['json', 'md_json', 'tools']</code> <p>The mode to use for the function.</p> <code>'md_json'</code> <code>mock</code> <code>bool</code> <p>Whether to use the mock mode for the function.</p> <code>False</code> <code>kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable[[Callable], Callable]</code> <p>The function response or the generated code response (Any).</p> Source code in <code>zyx/_client/llm/function.py</code> <pre><code>def function(\n    model: str = \"gpt-4o-mini\",\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    mode: Literal[\"json\", \"md_json\", \"tools\"] = \"md_json\",\n    mock: bool = False,\n    verbose: bool = False,\n    **kwargs,\n) -&gt; Callable[[Callable], Callable]:\n    \"\"\"A quick abstraction to create both mock &amp; generated runnable python functions, using LLMs.\n\n    Parameters:\n        model (str): The model to use for the function.\n        api_key (Optional[str]): The API key to use for the function.\n        base_url (Optional[str]): The base URL to use for the function.\n        mode (Literal[\"json\", \"md_json\", \"tools\"]): The mode to use for the function.\n        mock (bool): Whether to use the mock mode for the function.\n        kwargs (dict): Additional keyword arguments to pass to the function.\n\n    Returns:\n        The function response or the generated code response (Any).\n    \"\"\"\n\n    def decorator(f: Callable) -&gt; Callable:\n        from ..completion import CompletionClient\n        from pydantic import create_model\n        from functools import wraps\n        import tenacity\n        import tempfile\n        import subprocess\n        import sys\n        import json\n        import os\n        import ast\n        import importlib\n\n        @wraps(f)\n        @tenacity.retry(\n            stop=tenacity.stop_after_attempt(3),\n            retry=tenacity.retry_if_exception_type((RuntimeError, Exception)),\n            wait=tenacity.wait_exponential(multiplier=1, min=4, max=10),\n        )\n        def wrapper(*args, **kwargs):\n            type_hints = get_type_hints(f)\n            return_type = type_hints.pop(\"return\", Any)\n            function_args = {k: v for k, v in type_hints.items()}\n            input_dict = dict(zip(function_args.keys(), args))\n            input_dict.update(kwargs)\n\n            if not mock:\n\n                class CodeGenerationModel(BaseModel):\n                    code: str = Field(\n                        ..., description=\"Complete Python code as a single string\"\n                    )\n\n                error_context = \"\"\n                try:\n                    # Prepare the import statement and return type\n                    import_statement = \"import typing\\n\"\n                    return_type_str = (\n                        f\"typing.Any\" if return_type == Any else str(return_type)\n                    )\n\n                    messages = [\n                        {\n                            \"role\": \"system\",\n                            \"content\": f\"\"\"\n                            ## CONTEXT ##\n\n                            You are a Python code generator. Your goal is to generate a Python function that matches this specification:\n                            Function: {f.__name__}\n                            Arguments and their types: {function_args}\n                            Return type: {return_type_str}\n                            Description: {f.__doc__} \\n\n\n                            ## OBJECTIVE ##\n\n                            Generate the complete Python code as a single string, including all necessary import statements.\n                            The code should define the function and include a call to the function with the provided inputs.\n                            The last line should assign the result of the function call to a variable named 'result'.\n                            Do not include any JSON encoding, printing, or explanations in your response.\n                            Ensure all required modules are imported.\n                            {error_context}\n                            \"\"\",\n                        },\n                        {\n                            \"role\": \"user\",\n                            \"content\": f\"Generate code for the function with these inputs: {input_dict}\",\n                        },\n                    ]\n                    response = CompletionClient().completion(\n                        messages=messages,\n                        model=model,\n                        api_key=api_key,\n                        base_url=base_url,\n                        response_model=CodeGenerationModel,\n                        mode=\"md_json\"\n                        if model.startswith((\"ollama/\", \"ollama_chat/\"))\n                        else mode,\n                        **kwargs,\n                    )\n\n                    if verbose:\n                        print(f\"Code generation response: {response}\")\n\n                    # Prepend the import statement to the generated code\n                    full_code = import_statement + response.code\n\n                    # Create a temporary Python file\n                    with tempfile.NamedTemporaryFile(\n                        mode=\"w\", suffix=\".py\", delete=False\n                    ) as temp_file:\n                        temp_file.write(full_code)\n\n                    # Execute the temporary Python file\n                    env = os.environ.copy()\n                    env[\"PYTHONPATH\"] = \":\".join(sys.path)\n                    result = subprocess.run(\n                        [sys.executable, temp_file.name],\n                        capture_output=True,\n                        text=True,\n                        env=env,\n                    )\n                    from rich.prompt import Confirm\n\n                    if result.returncode != 0:\n                        if \"ModuleNotFoundError\" in result.stderr:\n                            missing_library = result.stderr.split(\"'\")[1]\n                            install = Confirm.ask(\n                                prompt=f\"The library '[bold blue3]{missing_library}[/bold blue3]' is required but not installed. Do you want to install it?\"\n                            )\n                            if install:\n                                subprocess.check_call(\n                                    [\n                                        sys.executable,\n                                        \"-m\",\n                                        \"pip\",\n                                        \"install\",\n                                        missing_library,\n                                    ]\n                                )\n                                print(\n                                    f\"Installed {missing_library}. Rerunning the code.\"\n                                )\n                                # Rerun the code after installation\n                                result = subprocess.run(\n                                    [sys.executable, temp_file.name],\n                                    capture_output=True,\n                                    text=True,\n                                    env=env,\n                                )\n                                if result.returncode != 0:\n                                    raise RuntimeError(\n                                        f\"Error executing generated code after library installation: {result.stderr}\"\n                                    )\n                            else:\n                                raise RuntimeError(\n                                    f\"Required library '{missing_library}' is not installed. Please install it manually and try again.\"\n                                )\n                        else:\n                            raise RuntimeError(\n                                f\"Error executing generated code: {result.stderr}\"\n                            )\n\n                    # Import the generated module\n                    spec = importlib.util.spec_from_file_location(\n                        \"generated_module\", temp_file.name\n                    )\n                    module = importlib.util.module_from_spec(spec)\n                    spec.loader.exec_module(module)\n\n                    # Return the result directly\n                    return module.result\n\n                except Exception as e:\n                    print(f\"Error in code generation or execution, retrying...\")\n\n                    error_context = f\"\"\"\n                    Previous attempt failed with the following error:\n                    {str(e)}\n\n                    Traceback:\n                    {traceback.format_exc()}\n\n                    Please adjust the code to avoid this error and ensure it runs successfully.\n                    \"\"\"\n                    raise RuntimeError(\n                        f\"Error in code generation or execution: {str(e)}\"\n                    )\n\n            else:\n                FunctionResponseModel = create_model(\n                    \"FunctionResponseModel\",\n                    output=(return_type, ...),\n                )\n                messages = [\n                    {\n                        \"role\": \"system\",\n                        \"content\": f\"\"\"\n                        You are a Python function emulator. Your goal is to simulate the response of this Python function:\n                        Function: {f.__name__}\n                        Arguments and their types: {function_args}\n                        Return type: {return_type}\n                        Description: {f.__doc__}\n                        Respond only with the output the function would produce, without any additional explanation.\n                        \"\"\",\n                    },\n                    {\"role\": \"user\", \"content\": f\"Function inputs: {input_dict}\"},\n                ]\n                response = CompletionClient().completion(\n                    messages=messages,\n                    model=model,\n                    api_key=api_key,\n                    base_url=base_url,\n                    response_model=FunctionResponseModel,\n                    mode=\"md_json\"\n                    if model.startswith((\"ollama/\", \"ollama_chat/\"))\n                    else mode,\n                    **kwargs,\n                )\n                return response.output\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"examples/llm/code/#code","title":".code()","text":"<p>Directly create a code object</p> <pre><code># .code() is an easier to use version of .function()\nfrom zyx import code\n\nlogger = code(\"A logger named zyx logger\")\n\nlogger.info(\"Hello, world!\")\n</code></pre> <pre><code>2024-09-07 23:57:43,792 - zyx_logger - INFO - Hello, world!\n</code></pre> <p>Generate, execute Python code based on a string description, and return the resulting object.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>str</code> <p>A string describing the desired object or functionality.</p> required <code>model</code> <code>str</code> <p>The model to use for code generation.</p> <code>'gpt-4o-mini'</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key to use for the function.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>The base URL to use for the function.</p> <code>None</code> <code>mode</code> <code>Literal['json', 'md_json', 'tools']</code> <p>The mode to use for the function.</p> <code>'md_json'</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output.</p> <code>False</code> <code>kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The object created by executing the generated code.</p> Source code in <code>zyx/_client/llm/code.py</code> <pre><code>def code(\n    description: str,\n    model: str = \"gpt-4o-mini\",\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    mode: Literal[\"json\", \"md_json\", \"tools\"] = \"md_json\",\n    verbose: bool = False,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"Generate, execute Python code based on a string description, and return the resulting object.\n\n    Parameters:\n        description (str): A string describing the desired object or functionality.\n        model (str): The model to use for code generation.\n        api_key (Optional[str]): The API key to use for the function.\n        base_url (Optional[str]): The base URL to use for the function.\n        mode (Literal[\"json\", \"md_json\", \"tools\"]): The mode to use for the function.\n        verbose (bool): Whether to print verbose output.\n        kwargs (dict): Additional keyword arguments to pass to the function.\n\n    Returns:\n        The object created by executing the generated code.\n    \"\"\"\n    from ..completion import CompletionClient\n    import traceback\n    import tempfile\n    import sys\n    import os\n    import importlib\n\n    class CodeGenerationModel(BaseModel):\n        code: str = Field(..., description=\"Complete Python code as a single string\")\n\n    try:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"\n                You are a Python code generator. Your goal is to generate Python code based on the given description.\n                Generate the complete Python code as a single string, including all necessary import statements.\n                The code should define any necessary functions or classes and create the described object.\n                The last line should assign the created object to a variable named 'result'.\n                Do not include any JSON encoding, printing, or explanations in your response.\n                Ensure all required modules are imported.\n                \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Generate Python code to create this object: {description}\",\n            },\n        ]\n        response = CompletionClient().completion(\n            messages=messages,\n            model=model,\n            api_key=api_key,\n            base_url=base_url,\n            response_model=CodeGenerationModel,\n            mode=\"md_json\" if model.startswith((\"ollama/\", \"ollama_chat/\")) else mode,\n            **kwargs,\n        )\n\n        if verbose:\n            print(f\"Generated code:\\n{response.code}\")\n\n        # Create a temporary Python file\n        with tempfile.NamedTemporaryFile(\n            mode=\"w\", suffix=\".py\", delete=False\n        ) as temp_file:\n            temp_file.write(response.code)\n            temp_file_path = temp_file.name\n\n        # Add the current directory to sys.path to allow importing the temporary module\n        sys.path.insert(0, os.path.dirname(temp_file_path))\n\n        try:\n            # Import the generated module\n            module_name = os.path.splitext(os.path.basename(temp_file_path))[0]\n            spec = importlib.util.spec_from_file_location(module_name, temp_file_path)\n            module = importlib.util.module_from_spec(spec)\n            spec.loader.exec_module(module)\n\n            # Return the result object\n            return module.result\n        finally:\n            # Clean up: remove the temporary file and restore sys.path\n            os.unlink(temp_file_path)\n            sys.path.pop(0)\n\n    except Exception as e:\n        print(f\"Error in code generation or execution: {str(e)}\")\n        print(f\"Traceback: {traceback.format_exc()}\")\n        raise\n</code></pre>"},{"location":"examples/llm/nlp/","title":"NLP Functions","text":"<p>zyx comes with a set of NLP functions that are designed to work with text data. All of these functions are inspired from <code>MarvinAI</code>.</p>"},{"location":"examples/llm/nlp/#classify","title":".classify()","text":"<p>Classify text into multiple categories</p> <pre><code>from zyx import classify\n\nlabels = [\"positive\", \"negative\", \"neutral\"]\n\ntext = [\n    \"I love this product!\",\n    \"This is the worst thing I've ever used.\",\n    \"I'm indifferent about this.\",\n    \"What a wonderful day!\",\n    \"I'm not sure about this.\",\n    \"My day was horrible.\",\n    \"Thank you for your help!\"\n]\n\nclassifications = classify(inputs = text, labels = labels)\n\nprint(classifications)\n</code></pre> <pre><code># OUTPUT\n[\n    [ClassificationResult(text='I love this product!', label='positive')],\n    [ClassificationResult(text=\"This is the worst thing I've ever used.\", label='negative')],\n    [ClassificationResult(text=\"I'm indifferent about this.\", label='neutral')],\n    [ClassificationResult(text='What a wonderful day!', label='positive')],\n    [ClassificationResult(text=\"I'm not sure about this.\", label='neutral')],\n    [ClassificationResult(text='My day was horrible.', label='negative')],\n    [ClassificationResult(text='Thank you for your help!', label='positive')]\n]\n</code></pre> <p>Classifies the given text(s) into one of the given labels.</p> Example <pre><code>zyx.classify(\"I love programming.\", [\"positive\", \"negative\"])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, List[str]]</code> <p>The text(s) to classify.</p> required <code>labels</code> <code>List[str]</code> <p>The labels to classify the text(s) into.</p> required <code>n</code> <code>int</code> <p>The number of classifications to provide for each input.</p> <code>1</code> <code>model</code> <code>str</code> <p>The model to use for classification.</p> <code>'gpt-4o-mini'</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key to use for classification.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>The base URL to use for classification.</p> <code>None</code> <code>organization</code> <code>Optional[str]</code> <p>The organization to use for classification.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens to generate.</p> <code>None</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries to make.</p> <code>3</code> <code>temperature</code> <code>float</code> <p>The temperature to use for classification.</p> <code>0</code> <code>mode</code> <code>Literal['json', 'md_json', 'tools']</code> <p>The mode to use for classification.</p> <code>'md_json'</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output.</p> <code>False</code> <p>Returns:</p> Type Description <code>List</code> <p>List[List[ClassificationResult]]: The classifications for each input.</p> Source code in <code>zyx/_client/llm/classify.py</code> <pre><code>def classify(\n    inputs: Union[str, List[str]],\n    labels: List[str],\n    n: int = 1,\n    model: str = \"gpt-4o-mini\",\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    organization: Optional[str] = None,\n    max_tokens: Optional[int] = None,\n    max_retries: int = 3,\n    temperature: float = 0,\n    mode: Literal[\"json\", \"md_json\", \"tools\"] = \"md_json\",\n    verbose: bool = False,\n) -&gt; List:\n    \"\"\"\n    Classifies the given text(s) into one of the given labels.\n\n    Example:\n        ```python\n        zyx.classify(\"I love programming.\", [\"positive\", \"negative\"])\n        ```\n\n    Parameters:\n        inputs (Union[str, List[str]]): The text(s) to classify.\n        labels (List[str]): The labels to classify the text(s) into.\n        n (int): The number of classifications to provide for each input.\n        model (str): The model to use for classification.\n        api_key (Optional[str]): The API key to use for classification.\n        base_url (Optional[str]): The base URL to use for classification.\n        organization (Optional[str]): The organization to use for classification.\n        max_tokens (Optional[int]): The maximum number of tokens to generate.\n        max_retries (int): The maximum number of retries to make.\n        temperature (float): The temperature to use for classification.\n        mode (Literal[\"json\", \"md_json\", \"tools\"]): The mode to use for classification.\n        verbose (bool): Whether to print verbose output.\n\n    Returns:\n        List[List[ClassificationResult]]: The classifications for each input.\n    \"\"\"\n    from ..completion import CompletionClient\n    from pydantic import create_model\n\n    class ClassificationResult(BaseModel):\n        text: str\n        label: str\n\n    ResponseModel = create_model(\n        \"ResponseModel\", items=(List[ClassificationResult], ...)\n    )\n\n    system_message = f\"\"\"\n    You are a text classifier. Your task is to classify the given text(s) into one of the following categories:\n    {', '.join(labels)}\n\n    For each input, provide {n} classification(s). Each classification should include the original text \n    and the assigned label.\n    \"\"\"\n\n    if isinstance(inputs, str):\n        inputs = [inputs]\n    user_message = \"Classify the following text(s):\\n\\n\" + \"\\n\\n\".join(inputs)\n\n    response = CompletionClient().completion(\n        messages=[\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": user_message},\n        ],\n        model=model,\n        api_key=api_key,\n        base_url=base_url,\n        organization=organization,\n        max_tokens=max_tokens,\n        max_retries=max_retries,\n        temperature=temperature,\n        verbose=verbose,\n        mode=\"md_json\" if model.startswith((\"ollama/\", \"ollama_chat/\")) else mode,\n        response_model=ResponseModel,\n    )\n\n    results = response.items\n    if len(inputs) == 1:\n        return results\n    else:\n        grouped_results = []\n        for i in range(0, len(results), n):\n            grouped_results.append(results[i : i + n])\n        return grouped_results\n</code></pre>"},{"location":"examples/llm/nlp/#extract","title":".extract()","text":"<p>Extract information from text into labels</p> <pre><code>from zyx import extract\nfrom pydantic import BaseModel\n\ntext = \"One day, John went to the store. He bought a book and a pen. He then went to the park. He played football with his friends. He had a great time.\"\n\nclass Entities(BaseModel):\n    names: list[str]\n    places: list[str]\n    items: list[str]\n\nextracted = extract(Entities, text)\n\nprint(extracted)\n</code></pre> <pre><code># OUTPUT\nEntities(names=['John'], places=['store', 'park'], items=['book', 'pen', 'football'])\n</code></pre> Source code in <code>zyx/_client/llm/extract.py</code> <pre><code>def extract(\n    target: BaseModel,\n    text: str,\n    model: str = \"gpt-4o-mini\",\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    organization: Optional[str] = None,\n    max_tokens: Optional[int] = None,\n    max_retries: int = 3,\n    temperature: float = 0,\n    mode: Literal[\"json\", \"md_json\", \"tools\"] = \"md_json\",\n    verbose: bool = False,\n):\n    from ..completion import CompletionClient\n\n    system_message = f\"\"\"\n    You are an information extractor. Your task is to extract relevant information from the given text \n    and fit it into the following Pydantic model:\n\n    {target.model_json_schema()}\n\n    Instructions:\n    - Only Extract information from the text and fit it into the given model.\n    - Do not infer or generate any information that is not present in the input text.\n    - If a required field cannot be filled with information from the text, leave it as None or an empty string as appropriate.\n    \"\"\"\n\n    user_message = f\"Extract information from the following text and fit it into the given model:\\n\\n{text}\"\n\n    response = CompletionClient().completion(\n        messages=[\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": user_message},\n        ],\n        model=model,\n        api_key=api_key,\n        base_url=base_url,\n        organization=organization,\n        max_tokens=max_tokens,\n        max_retries=max_retries,\n        temperature=temperature,\n        verbose=verbose,\n        mode=\"md_json\" if model.startswith((\"ollama/\", \"ollama_chat/\")) else mode,\n        response_model=target,\n    )\n\n    return response\n</code></pre>"},{"location":"examples/llm/nlp/#generate","title":".generate()","text":"<p>Generate pydantic models quickly &amp; efficiently</p> <pre><code>from zyx import generate\nfrom pydantic import BaseModel\n\nclass Book(BaseModel):\n    title: str\n    author: str\n    pages: int\n\nbooks = generate(Book, n = 10, model = \"openai/gpt-4o-mini\")\n\nprint(books)\n</code></pre> <pre><code># OUTPUT\n[\n    Book(title='The Great Gatsby', author='F. Scott Fitzgerald', pages=180),\n    Book(title='1984', author='George Orwell', pages=328),\n    Book(title='To Kill a Mockingbird', author='Harper Lee', pages=281),\n    Book(title='Pride and Prejudice', author='Jane Austen', pages=279),\n    Book(title='The Catcher in the Rye', author='J.D. Salinger', pages=214),\n    Book(title='Moby Dick', author='Herman Melville', pages=585),\n    Book(title='War and Peace', author='Leo Tolstoy', pages=1225),\n    Book(title='The Hobbit', author='J.R.R. Tolkien', pages=310),\n    Book(title='Fahrenheit 451', author='Ray Bradbury', pages=158),\n    Book(title='The Alchemist', author='Paulo Coelho', pages=208)\n]\n</code></pre> <p>Generates a list of instances of the given Pydantic model.</p> Example <pre><code>import zyx\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nzyx.generate(User, n=5)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>BaseModel</code> <p>The Pydantic model to generate instances of.</p> required <code>instructions</code> <code>Optional[str]</code> <p>The instructions for the generator.</p> <code>None</code> <code>n</code> <code>int</code> <p>The number of instances to generate.</p> <code>1</code> <code>model</code> <code>str</code> <p>The model to use for generation.</p> <code>'gpt-4o-mini'</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key to use for generation.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>The base URL to use for generation.</p> <code>None</code> <code>organization</code> <code>Optional[str]</code> <p>The organization to use for generation.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens to generate.</p> <code>None</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries to make.</p> <code>3</code> <code>temperature</code> <code>float</code> <p>The temperature to use for generation.</p> <code>0</code> <code>mode</code> <code>Literal['json', 'md_json', 'tools']</code> <p>The mode to use for generation.</p> <code>'md_json'</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output.</p> <code>False</code> <p>Returns:</p> Type Description <code>List</code> <p>List[BaseModel]: A list of instances of the given Pydantic model.</p> Source code in <code>zyx/_client/llm/generate.py</code> <pre><code>def generate(\n    target: BaseModel,\n    instructions: Optional[str] = None,\n    n: int = 1,\n    model: str = \"gpt-4o-mini\",\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    organization: Optional[str] = None,\n    max_tokens: Optional[int] = None,\n    max_retries: int = 3,\n    temperature: float = 0,\n    mode: Literal[\"json\", \"md_json\", \"tools\"] = \"md_json\",\n    verbose: bool = False,\n) -&gt; List:\n    \"\"\"\n    Generates a list of instances of the given Pydantic model.\n\n    Example:\n        ```python\n        import zyx\n\n        class User(BaseModel):\n            name: str\n            age: int\n\n        zyx.generate(User, n=5)\n        ```\n\n    Parameters:\n        target (BaseModel): The Pydantic model to generate instances of.\n        instructions (Optional[str]): The instructions for the generator.\n        n (int): The number of instances to generate.\n        model (str): The model to use for generation.\n        api_key (Optional[str]): The API key to use for generation.\n        base_url (Optional[str]): The base URL to use for generation.\n        organization (Optional[str]): The organization to use for generation.\n        max_tokens (Optional[int]): The maximum number of tokens to generate.\n        max_retries (int): The maximum number of retries to make.\n        temperature (float): The temperature to use for generation.\n        mode (Literal[\"json\", \"md_json\", \"tools\"]): The mode to use for generation.\n        verbose (bool): Whether to print verbose output.\n\n    Returns:\n        List[BaseModel]: A list of instances of the given Pydantic model.\n    \"\"\"\n\n    from ..completion import CompletionClient\n    from pydantic import create_model\n\n    ResponseModel = create_model(\"ResponseModel\", items=(List[target], ...))\n\n    system_message = f\"\"\"\n    You are a data generator. Your task is to generate {n} valid instance(s) of the following Pydantic model:\n\n    {target.model_json_schema()}\n\n    Ensure that all generated instances comply with the model's schema and constraints.\n    \"\"\"\n    user_message = (\n        instructions\n        if instructions\n        else f\"Generate {n} instance(s) of the given model.\"\n    )\n\n    response = CompletionClient().completion(\n        messages=[\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": user_message},\n        ],\n        model=model,\n        api_key=api_key,\n        base_url=base_url,\n        organization=organization,\n        max_tokens=max_tokens,\n        max_retries=max_retries,\n        temperature=temperature,\n        verbose=verbose,\n        mode=\"md_json\" if model.startswith((\"ollama/\", \"ollama_chat/\")) else mode,\n        response_model=ResponseModel,\n    )\n    return response.items\n</code></pre>"},{"location":"examples/llm/prompts/","title":"Create Optimized System Prompts","text":"<p>zyxzyx provides a way to create system prompts, optimized for your use case. These can either use the <code>COSTAR</code> or <code>TIDD-EC</code> frameworks, with more to come.</p>"},{"location":"examples/llm/prompts/#create_system_prompt","title":".create_system_prompt()","text":"<p>Create a system prompt, optimized for your use case</p> <pre><code>from zyx import create_system_prompt\n\nprompt = create_system_prompt(\n    \"A world class author\",\n    type = \"costar\",\n    model = \"anthropic/claude-3-5-sonnet-20240620\"\n)\n\nprint(prompt)\n</code></pre> <pre><code># OUTPUT\n## Context ##\nYou are a world-renowned author with numerous bestsellers and literary awards to your name. Your works span various\ngenres, from historical fiction to contemporary literature, and have been translated into multiple languages. Your \nunique voice and storytelling abilities have captivated readers worldwide, earning you a reputation as one of the \nmost influential writers of your generation.\n\n\n## Objective ##\nYour task is to craft compelling narratives, develop rich characters, and create immersive worlds that resonate \nwith readers on a deep emotional level. You should be able to seamlessly blend elements of plot, character \ndevelopment, and thematic depth to produce literary works of the highest quality. Your writing should demonstrate \nmastery of language, pacing, and narrative structure, while also pushing the boundaries of conventional \nstorytelling.\n\n\n## Style ##\nYour writing style should be sophisticated and evocative, with a keen attention to detail and a masterful command \nof language. Employ a wide range of literary techniques, including vivid imagery, metaphor, and symbolism, to \nenhance the reader's experience and convey complex ideas and emotions.\n\n\n## Tone ##\nAdapt your tone to suit the specific work you're creating, but always maintain an underlying sense of authenticity \nand emotional resonance. Your writing should evoke a range of emotions in your readers, from joy to sorrow, \nintrigue to contemplation.\n\n\n## Audience ##\nYour audience consists of discerning readers who appreciate literary excellence, as well as casual readers seeking \nengaging stories. Your work should be accessible enough to appeal to a broad readership while also offering depth \nand complexity for more critical analysis.\n\n\n## Response_format ##\nProduce well-structured, polished prose that adheres to the highest standards of literary craftsmanship. Your \nwriting should be free of grammatical errors and demonstrate a mastery of narrative techniques. When appropriate, \nincorporate dialogue, descriptive passages, and internal monologue to create a rich, multidimensional reading \nexperience.\n</code></pre> Source code in <code>zyx/_client/llm/create_system_prompt.py</code> <pre><code>def create_system_prompt(\n    instructions: str,\n    type: PROMPT_TYPES = \"costar\",\n    model: str = \"gpt-4o-mini\",\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    temperature: float = None,\n    response_format: Union[Literal[\"pydantic\"], Literal[\"dict\"], None] = None,\n    verbose: bool = False,\n) -&gt; Union[BaseModel, str, dict]:\n    from ..completion import completion\n\n    try:\n        if type not in PROMPT_TYPES_MAPPING:\n            raise ValueError(\n                f\"Invalid prompt type: {type}. Must be one of {PROMPT_TYPES_MAPPING.keys()}\"\n            )\n\n        response_model = get_response_model(type=type)\n\n        if verbose:\n            print(\n                f\"Generating system prompt for {type} with instructions: {instructions}\"\n            )\n\n        system_prompt = get_system_prompt(type=type)\n\n    except Exception as e:\n        raise e\n\n    if verbose:\n        print(f\"System prompt: {system_prompt}\")\n\n    try:\n        prompt = completion(\n            model=model,\n            messages=[\n                system_prompt,\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Generate a system prompt for the following instructions:\\n\\nINSTRUCTIONS:\\n{instructions}\",\n                },\n            ],\n            api_key=api_key,\n            base_url=base_url,\n            temperature=temperature,\n            response_model=response_model,\n            verbose=verbose,\n        )\n    except Exception as e:\n        raise e\n\n    if prompt is None:\n        raise ValueError(\"Completion function returned None\")\n\n    if verbose:\n        print(f\"Prompt content: {prompt}\")\n\n    if response_format == \"pydantic\":\n        return prompt\n\n    response_string = []\n\n    for field in prompt.model_fields:\n        value = getattr(prompt, field, None)\n        if isinstance(value, list):\n            formatted_value = \"\\n\".join(f\"- {item}\" for item in value)\n        else:\n            formatted_value = value\n        response_string.append(f\"## {field.capitalize()} ##\\n{formatted_value}\\n\\n\")\n\n    if response_format == \"dict\":\n        return {\"role\": \"system\", \"content\": \"\\n\".join(response_string)}\n\n    return \"\\n\".join(response_string)\n</code></pre>"},{"location":"examples/llm/prompts/#optimize_system_prompt","title":".optimize_system_prompt()","text":"<p>Optimize a system prompt</p> <pre><code>from zyx import optimize_system_prompt\n\nsystem_prompt = \"You are a helpful ai assistant, who reasons before responding.\"\n\nprompt = optimize_system_prompt(\n    system_prompt,\n    type = \"tidd-ec\",\n    model = \"anthropic/claude-3-haiku-20240307\"\n)\n\nprint(prompt)\n</code></pre> <pre><code># OUTPUT\n## Task ##\nYou are a highly capable and thoughtful AI assistant, tasked with optimizing an existing system prompt to better \nalign with the user's needs and objectives. The goal is to provide a more tailored and impactful response from the \nlanguage model, while maintaining a professional and helpful tone.\n\n\n## Instructions ##\n- Review the existing system prompt carefully and identify areas for improvement.\n- Enhance the prompt by incorporating additional context, instructions, and guidelines to ensure the language \nmodel's responses are more relevant, coherent, and aligned with the user's desired outcomes.\n- Craft the optimized system prompt in a clear and concise manner, ensuring the language model has a solid \nunderstanding of its role and the expected characteristics of its responses.\n\n\n## Do ##\n- Clearly define the assistant's role and responsibilities, emphasizing its ability to reason, analyze information,\nand provide thoughtful, well-considered responses.\n- Incorporate specific instructions on the tone, style, and content that should be present in the language model's \noutputs, such as maintaining a professional and helpful demeanor, providing relevant and actionable information, \nand avoiding irrelevant or potentially harmful content.\n- Encourage the language model to engage in critical thinking, research, and analysis to formulate its responses, \nrather than relying solely on pre-programmed or generic responses.\n\n\n## Donts ##\n- Avoid vague or overly broad instructions that could lead to ambiguous or inconsistent responses from the language\nmodel.\n- Do not provide instructions that could result in the language model generating responses that are biased, \nunethical, or harmful in any way.\n- Refrain from including instructions that could limit the language model's ability to provide thoughtful, nuanced,\nand contextually appropriate responses.\n\n\n## Examples ##\n- You are an AI assistant with strong reasoning and analytical capabilities. Your role is to provide helpful, \ninformative, and well-considered responses to the user's queries. Maintain a professional, courteous, and objective\ntone throughout your interactions.\n- When presented with a question or request, take the time to carefully analyze the context, gather relevant \ninformation, and formulate a thoughtful, coherent response that addresses the user's needs. Avoid generic or \npre-written responses, and instead tailor your output to the specific situation.\n- If you encounter a query that requires additional research or clarification, communicate this to the user and \nprovide a timeline for when you can deliver a more comprehensive response. Your goal is to be a reliable and \ntrustworthy assistant, not just a source of quick, potentially inaccurate information.\n</code></pre> Source code in <code>zyx/_client/llm/optimize_system_prompt.py</code> <pre><code>def optimize_system_prompt(\n    prompt: Union[str, dict[str, str]],\n    type: PROMPT_TYPES = \"costar\",\n    model: str = \"gpt-4o-mini\",\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    temperature: float = None,\n    response_format: Union[Literal[\"pydantic\"], Literal[\"dict\"], None] = None,\n    verbose: bool = False,\n) -&gt; Union[str, dict[str, str]]:\n    from .create_system_prompt import create_system_prompt\n\n    prompt = create_system_prompt(\n        instructions=f\"\"\"\n## IMPORTANT INSTRUCTIONS ## \\n\\n\n\nTHE USER HAS REQUESTED YOU OPTIMIZE THEIR EXISTING SYSTEM PROMPT. \\n\\n\n\n[EXISTING SYSTEM PROMPT]\n{prompt}\n[/EXISTING SYSTEM PROMPT] \\n\\n\n\nGENERATE ONLY THE OPTIMIZED SYSTEM PROMPT.\n        \"\"\",\n        type=type,\n        model=model,\n        api_key=api_key,\n        base_url=base_url,\n        temperature=temperature,\n        response_format=response_format,\n        verbose=verbose,\n    )\n\n    return prompt\n</code></pre>"},{"location":"examples/llm/reasoning/","title":"Reasoning Based Functions","text":"<p><code>zyx</code> provides a way to create reasoning based functions, which can be used to solve complex problems. </p> <p>The following papers were implemented when creating these functions:</p> Paper Link Least-to-Most Prompting Enables Complex Reasoning in Large Language Models Link Self-Consistency Improves Chain of Thought Reasoning in Language Models Link Self-Refine: Iterative Refinement with Self-Feedback Link Tree of Thoughts: Deliberate Problem Solving with Large Language Models Link"},{"location":"examples/llm/reasoning/#plan","title":".plan()","text":"<p>Create an optimize plan using either the <code>Tree of Thought</code> or <code>Least to Most</code> frameworks.</p> <pre><code>from zyx import plan\n\nsolution = plan(\n    \"\"\"How would i solve this sudoku puzzle?\n\n    | 8 0 0 | 0 0 0 | 0 0 0 |\n    | 0 0 3 | 6 0 0 | 0 0 0 |\n    | 0 7 0 | 0 9 0 | 2 0 0 |\n    \"\"\",\n    process = \"tree_of_thought\",\n    model = \"openai/gpt-4o-mini\",\n)\n\nprint(solution)\n</code></pre> Output <pre><code># OUTPUT\nPlan(\n    tasks=[\n        Task(\n            description='Based on the evaluation: After evaluating the three approaches to solving the puzzle, the \nmost promising approach is the Constraint Propagation method. This approach effectively narrows down the \npossibilities for each empty cell, which can lead to a quicker solution in many cases. The outcomes for each \napproach are as follows: 1. **Backtracking Algorithm**: - **Best-case**: Quick solution with minimal backtracking. \n- **Average-case**: Moderate backtracking required, leading to a solution after several iterations. - \n**Worst-case**: Lengthy solving process due to excessive backtracking. 2. **Constraint Propagation**: - \n**Best-case**: Unique solution found quickly without guessing. - **Average-case**: Constraints reduce possibilities\nbut still require some guessing or backtracking. - **Worst-case**: Minimal reduction in possibilities, leading to a\nlengthy solving process. 3. **Human-like Techniques**: - **Best-case**: Quick identification of placements, solving\nthe puzzle easily. - **Average-case**: Some progress made, but complex strategies needed to finish. - \n**Worst-case**: Solver gets stuck and cannot find a solution without algorithmic methods. Given these evaluations, \nthe Constraint Propagation method stands out as it can lead to a unique solution efficiently, minimizing the need \nfor guessing and backtracking. To implement the Constraint Propagation approach, the following detailed, actionable\ntasks can be outlined: 1. **Analyze the Initial Puzzle State**: Identify all empty cells and the current numbers in\nthe grid to establish initial constraints. 2. **Apply Constraint Rules**: For each empty cell, apply Sudoku rules \nto eliminate impossible numbers based on existing numbers in the same row, column, and box. 3. **Update \nPossibilities**: Continuously update the list of possible numbers for each empty cell as constraints are applied. \n4. **Identify Unique Candidates**: Look for cells that have only one possible number left and fill them in, further\nreducing possibilities for other cells. 5. **Iterate Until Completion**: Repeat the process of applying constraints\nand filling in unique candidates until the puzzle is solved or no further progress can be made.',\n            details=None\n        )\n    ]\n)\n</code></pre> <p>Generates a plan based on the input, using either the Least-to-Most or Tree of Thoughts prompting methods.</p> Example <pre><code>import zyx\nfrom pydantic import BaseModel\n\nclass ProjectTask(BaseModel):\n    name: str\n    duration: int\n    dependencies: List[str]\n\n# Using a string input with Least-to-Most method\nplan1 = zyx.plan(\"Create a website for a small business\", process=\"least_to_most\", steps=7)\n\n# Using a Pydantic model input with Tree of Thoughts method\nplan2 = zyx.plan(ProjectTask, process=\"tree_of_thought\", steps=3)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Union[str, BaseModel]</code> <p>The input for plan generation, either a string or a Pydantic model.</p> required <code>model</code> <code>str</code> <p>The model to use for generation.</p> <code>'gpt-4o-mini'</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key to use for generation.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>The base URL to use for generation.</p> <code>None</code> <code>organization</code> <code>Optional[str]</code> <p>The organization to use for generation.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens to generate.</p> <code>None</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries to make.</p> <code>3</code> <code>temperature</code> <code>float</code> <p>The temperature to use for generation.</p> <code>0</code> <code>mode</code> <code>Literal['json', 'md_json', 'tools']</code> <p>The mode to use for generation.</p> <code>'md_json'</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output.</p> <code>False</code> <code>process</code> <code>Literal['least_to_most', 'tree_of_thought']</code> <p>The planning method to use.</p> <code>'least_to_most'</code> <code>steps</code> <code>int</code> <p>The number of steps to include in the plan (default is 5).</p> <code>5</code> <p>Returns:</p> Name Type Description <code>Plan</code> <code>Plan</code> <p>A Plan object containing a list of generated tasks.</p> Source code in <code>zyx/_client/llm/plan.py</code> <pre><code>def plan(\n    input: Union[str, BaseModel],\n    process: Literal[\"least_to_most\", \"tree_of_thought\"] = \"least_to_most\",\n    model: str = \"gpt-4o-mini\",\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    organization: Optional[str] = None,\n    max_tokens: Optional[int] = None,\n    max_retries: int = 3,\n    temperature: float = 0,\n    mode: Literal[\"json\", \"md_json\", \"tools\"] = \"md_json\",\n    verbose: bool = False,\n    steps: int = 5,\n) -&gt; Plan:\n    \"\"\"\n    Generates a plan based on the input, using either the Least-to-Most or Tree of Thoughts prompting\n    methods.\n\n    Example:\n        ```python\n        import zyx\n        from pydantic import BaseModel\n\n        class ProjectTask(BaseModel):\n            name: str\n            duration: int\n            dependencies: List[str]\n\n        # Using a string input with Least-to-Most method\n        plan1 = zyx.plan(\"Create a website for a small business\", process=\"least_to_most\", steps=7)\n\n        # Using a Pydantic model input with Tree of Thoughts method\n        plan2 = zyx.plan(ProjectTask, process=\"tree_of_thought\", steps=3)\n        ```\n\n    Parameters:\n        input (Union[str, BaseModel]): The input for plan generation, either a string or a Pydantic model.\n        model (str): The model to use for generation.\n        api_key (Optional[str]): The API key to use for generation.\n        base_url (Optional[str]): The base URL to use for generation.\n        organization (Optional[str]): The organization to use for generation.\n        max_tokens (Optional[int]): The maximum number of tokens to generate.\n        max_retries (int): The maximum number of retries to make.\n        temperature (float): The temperature to use for generation.\n        mode (Literal[\"json\", \"md_json\", \"tools\"]): The mode to use for generation.\n        verbose (bool): Whether to print verbose output.\n        process (Literal[\"least_to_most\", \"tree_of_thought\"]): The planning method to use.\n        steps (int): The number of steps to include in the plan (default is 5).\n\n    Returns:\n        Plan: A Plan object containing a list of generated tasks.\n    \"\"\"\n    client = CompletionClient()\n\n    if process == \"least_to_most\":\n        if isinstance(input, str):\n            return _generate_plan_from_string_l2m(\n                input,\n                client,\n                model,\n                api_key,\n                base_url,\n                organization,\n                max_tokens,\n                max_retries,\n                temperature,\n                mode,\n                verbose,\n                steps,\n            )\n        elif isinstance(input, type) and issubclass(input, BaseModel):\n            return _generate_plan_from_model_l2m(\n                input,\n                client,\n                model,\n                api_key,\n                base_url,\n                organization,\n                max_tokens,\n                max_retries,\n                temperature,\n                mode,\n                verbose,\n                steps,\n            )\n    elif process == \"tree_of_thought\":\n        if isinstance(input, str):\n            return _generate_plan_from_string_tot(\n                input,\n                client,\n                model,\n                api_key,\n                base_url,\n                organization,\n                max_tokens,\n                max_retries,\n                temperature,\n                mode,\n                verbose,\n                steps,\n            )\n        elif isinstance(input, type) and issubclass(input, BaseModel):\n            return _generate_plan_from_model_tot(\n                input,\n                client,\n                model,\n                api_key,\n                base_url,\n                organization,\n                max_tokens,\n                max_retries,\n                temperature,\n                mode,\n                verbose,\n                steps,\n            )\n    else:\n        raise ValueError(\n            \"Invalid planning method. Choose either 'least_to_most' or 'tree_of_thought'.\"\n        )\n</code></pre>"},{"location":"examples/llm/reasoning/#least_to_most","title":".least_to_most()","text":"<p>Create a least to most based function</p> <pre><code>from zyx import least_to_most\n\nsolution = least_to_most(\n    \"\"\"Solve the following sudoku puzzle: \n\n    | 8 0 0 | 0 0 0 | 0 0 0 |\n    | 0 0 3 | 6 0 0 | 0 0 0 |\n    | 0 7 0 | 0 9 0 | 2 0 0 |\n    | 0 5 0 | 0 0 7 | 0 0 0 |\n    | 0 0 0 | 0 4 5 | 7 0 0 |\n    | 0 0 0 | 1 0 0 | 0 3 0 |\n    \"\"\",\n    model = \"openai/gpt-4o-mini\",\n)\n\nprint(solution)\n</code></pre> Output <pre><code># OUTPUT\nLeastToMostResult(\n    final_answer='The solved Sudoku puzzle is:\\n\\n| 8 4 2 | 5 1 6 | 3 9 7 |\\n| 1 9 3 | 6 2 7 | 5 4 8 |\\n| 6 7 5 | 3\n9 4 | 2 1 8 |\\n| 4 5 6 | 2 8 7 | 1 8 9 |\\n| 3 8 9 | 8 4 5 | 7 2 6 |\\n| 2 1 7 | 1 6 9 | 4 3 5 |\\n\\nThis solution was\nachieved by identifying empty cells, determining possible numbers for each cell, filling in cells with only one \noption, and using a backtracking algorithm for cells with multiple options while ensuring no conflicts in rows, \ncolumns, and 3x3 grids.',\n    sub_problems=[\n        SubProblem(\n            description='Identify the empty cells in the Sudoku grid.',\n            solution=\"To identify the empty cells in a Sudoku grid, look for cells that contain a zero or a \nplaceholder indicating emptiness. For example, in a grid where '0' represents an empty cell, you would scan through\neach row and column to find all instances of '0'.\"\n        ),\n        SubProblem(\n            description='Determine the possible numbers for each empty cell based on Sudoku rules.',\n            solution='To determine the possible numbers for each empty cell in a Sudoku grid, follow these steps: \n1. For each empty cell, identify the numbers already present in the same row, column, and 3x3 subgrid. 2. Create a \nlist of numbers from 1 to 9. 3. Remove the numbers found in the row, column, and subgrid from this list. 4. The \nremaining numbers in the list are the possible candidates for that empty cell.'\n        ),\n        SubProblem(\n            description=\"Fill in the cells with the only possible number if there's only one option.\",\n            solution='To fill in the cells with the only possible number in a Sudoku grid, follow these steps: 1. \nFor each empty cell, check the list of possible candidates determined previously. 2. If the list contains only one \nnumber, fill that cell with that number. 3. Repeat this process until no more cells can be filled with a single \noption.'\n        ),\n        SubProblem(\n            description='Use a backtracking algorithm to try different numbers in cells when multiple options are \navailable.',\n            solution='To use a backtracking algorithm for solving Sudoku when multiple options are available, \nfollow these steps: 1. Start with the first empty cell in the grid. 2. For each possible number (from 1 to 9) that \ncan be placed in that cell, do the following:   a. Place the number in the cell.   b. Recursively attempt to solve \nthe Sudoku grid with this new configuration.   c. If the grid is solved, return true.   d. If placing the number \ndoes not lead to a solution, remove the number (backtrack) and try the next possible number. 3. If all numbers have\nbeen tried and none lead to a solution, return false to indicate that the Sudoku cannot be solved with the current \nconfiguration. 4. Continue this process until the entire grid is filled or all possibilities have been exhausted.'\n        ),\n        SubProblem(\n            description='Check for conflicts in rows, columns, and 3x3 grids after each placement.',\n            solution='To check for conflicts in rows, columns, and 3x3 grids after each placement in a Sudoku grid,\nfollow these steps: 1. After placing a number in a cell, verify that the same number does not already exist in the \nsame row. 2. Check the corresponding column to ensure the number is not present there as well. 3. Identify the 3x3 \nsubgrid that contains the cell and confirm that the number is not already placed within that subgrid. 4. If any \nconflicts are found during these checks, the placement is invalid, and the number should be removed (backtracked). \nThis process ensures that the Sudoku rules are upheld after each placement.'\n        ),\n        SubProblem(\n            description='Continue the process until the Sudoku puzzle is completely solved.',\n            solution='Continue the process of solving the Sudoku puzzle by identifying empty cells, determining \npossible numbers for each empty cell, filling in cells with only one option, and using a backtracking algorithm for\ncells with multiple options. After each placement, check for conflicts in rows, columns, and 3x3 grids to ensure \nthe Sudoku rules are upheld. Repeat these steps until the entire grid is filled and the puzzle is completely \nsolved.'\n        )\n    ]\n)\n</code></pre> <p>Implements the Least-to-Most prompting technique to break down and solve complex problems.</p> Example <pre><code>result = zyx.least_to_most(\"Explain the process of photosynthesis in plants.\")\nprint(result.final_answer)\nfor sub_problem in result.sub_problems:\n    print(f\"Sub-problem: {sub_problem.description}\")\n    print(f\"Solution: {sub_problem.solution}\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>problem</code> <code>str</code> <p>The complex problem to solve.</p> required <code>model</code> <code>str</code> <p>The model to use for generation.</p> <code>'gpt-4o-mini'</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key to use for generation.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>The base URL to use for generation.</p> <code>None</code> <code>organization</code> <code>Optional[str]</code> <p>The organization to use for generation.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens to generate.</p> <code>None</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries to make.</p> <code>3</code> <code>temperature</code> <code>float</code> <p>The temperature to use for generation.</p> <code>0.2</code> <code>mode</code> <code>Literal['json', 'md_json', 'tools']</code> <p>The mode to use for generation.</p> <code>'md_json'</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>LeastToMostResult</code> <code>LeastToMostResult</code> <p>The final answer and the list of sub-problems with their solutions.</p> Source code in <code>zyx/_client/llm/least_to_most.py</code> <pre><code>def least_to_most(\n    problem: str,\n    model: str = \"gpt-4o-mini\",\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    organization: Optional[str] = None,\n    max_tokens: Optional[int] = None,\n    max_retries: int = 3,\n    temperature: float = 0.2,\n    mode: Literal[\"json\", \"md_json\", \"tools\"] = \"md_json\",\n    verbose: bool = False,\n) -&gt; LeastToMostResult:\n    \"\"\"\n    Implements the Least-to-Most prompting technique to break down and solve complex problems.\n\n    Example:\n        ```python\n        result = zyx.least_to_most(\"Explain the process of photosynthesis in plants.\")\n        print(result.final_answer)\n        for sub_problem in result.sub_problems:\n            print(f\"Sub-problem: {sub_problem.description}\")\n            print(f\"Solution: {sub_problem.solution}\")\n        ```\n\n    Parameters:\n        problem (str): The complex problem to solve.\n        model (str): The model to use for generation.\n        api_key (Optional[str]): The API key to use for generation.\n        base_url (Optional[str]): The base URL to use for generation.\n        organization (Optional[str]): The organization to use for generation.\n        max_tokens (Optional[int]): The maximum number of tokens to generate.\n        max_retries (int): The maximum number of retries to make.\n        temperature (float): The temperature to use for generation.\n        mode (Literal[\"json\", \"md_json\", \"tools\"]): The mode to use for generation.\n        verbose (bool): Whether to print verbose output.\n\n    Returns:\n        LeastToMostResult: The final answer and the list of sub-problems with their solutions.\n    \"\"\"\n    from ..completion import CompletionClient\n\n    # Step 1: Break down the problem into sub-problems\n    system_message = \"\"\"\n    You are a problem-solving assistant using the Least-to-Most method.\n    Break down the given complex problem into smaller, manageable sub-problems.\n    List these sub-problems in order, from the simplest to the most complex.\n    \"\"\"\n\n    user_message = f\"Problem: {problem}\\nBreak this down into sub-problems:\"\n\n    sub_problems_response = CompletionClient().completion(\n        messages=[\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": user_message},\n        ],\n        model=model,\n        api_key=api_key,\n        base_url=base_url,\n        organization=organization,\n        max_tokens=max_tokens,\n        max_retries=max_retries,\n        temperature=temperature,\n        verbose=verbose,\n        mode=\"md_json\" if model.startswith((\"ollama/\", \"ollama_chat/\")) else mode,\n        response_model=List[str],\n    )\n\n    sub_problems = [SubProblem(description=desc) for desc in sub_problems_response]\n\n    # Step 2: Solve each sub-problem sequentially\n    for i, sub_problem in enumerate(sub_problems):\n        system_message = \"\"\"\n        You are a problem-solving assistant. Solve the given sub-problem based on the context provided.\n        \"\"\"\n\n        context = \"\\n\".join(\n            [\n                f\"{j+1}. {sp.description}: {sp.solution}\"\n                for j, sp in enumerate(sub_problems[:i])\n            ]\n        )\n        user_message = (\n            f\"Context:\\n{context}\\n\\nSub-problem to solve: {sub_problem.description}\"\n        )\n\n        solution = CompletionClient().completion(\n            messages=[\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": user_message},\n            ],\n            model=model,\n            api_key=api_key,\n            base_url=base_url,\n            organization=organization,\n            max_tokens=max_tokens,\n            max_retries=max_retries,\n            temperature=temperature,\n            verbose=verbose,\n            mode=\"md_json\" if model.startswith((\"ollama/\", \"ollama_chat/\")) else mode,\n            response_model=str,\n        )\n\n        sub_problems[i].solution = solution\n\n    # Step 3: Generate final answer\n    system_message = \"\"\"\n    You are a problem-solving assistant. Given the original problem and the solutions to its sub-problems,\n    provide a comprehensive final answer.\n    \"\"\"\n\n    sub_problems_summary = \"\\n\".join(\n        [f\"{i+1}. {sp.description}: {sp.solution}\" for i, sp in enumerate(sub_problems)]\n    )\n    user_message = f\"Original problem: {problem}\\n\\nSub-problems and solutions:\\n{sub_problems_summary}\\n\\nProvide a comprehensive final answer:\"\n\n    final_answer = CompletionClient().completion(\n        messages=[\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": user_message},\n        ],\n        model=model,\n        api_key=api_key,\n        base_url=base_url,\n        organization=organization,\n        max_tokens=max_tokens,\n        max_retries=max_retries,\n        temperature=temperature,\n        verbose=verbose,\n        mode=\"md_json\" if model.startswith((\"ollama/\", \"ollama_chat/\")) else mode,\n        response_model=str,\n    )\n\n    return LeastToMostResult(final_answer=final_answer, sub_problems=sub_problems)\n</code></pre>"},{"location":"examples/llm/reasoning/#self_consistency","title":".self_consistency()","text":"<p>Create a self-consistency based function</p> <pre><code>from zyx import self_consistency\n\nsolution = self_consistency(\n    \"\"\"Solve the following sudoku puzzle: \n\n    | 8 0 0 | 0 0 0 | 0 0 0 |\n    | 0 0 3 | 6 0 0 | 0 0 0 |\n    | 0 7 0 | 0 9 0 | 2 0 0 |\n    | 0 5 0 | 0 0 7 | 0 0 0 |\n    | 0 0 0 | 0 4 5 | 7 0 0 |\n    | 0 0 0 | 1 0 0 | 0 3 0 |\n    \"\"\",\n    model = \"openai/gpt-4o-mini\",\n    num_paths = 5\n)\n\nprint(solution)\n</code></pre> Output <pre><code># OUTPUT\nSelfConsistencyResult(\n    final_answer='| 8 1 2 | 5 3 4 | 6 7 9 |\\n| 4 9 3 | 6 7 8 | 1 2 5 |\\n| 6 7 5 | 2 9 1 | 2 4 3 |\\n| 9 5 8 | 3 2 7 \n| 4 6 1 |\\n| 3 6 4 | 8 4 5 | 7 9 2 |\\n| 2 8 7 | 1 6 9 | 5 3 4 |',\n    confidence=0.2,\n    reasoning_paths=[\n        ReasoningPath(\n            steps=[\n                'Start with the provided Sudoku grid.',\n                'Identify the empty cells in the grid, marked by 0.',\n                'Use Sudoku rules to fill in the grid: each number 1-9 must appear exactly once in each row, \ncolumn, and 3x3 subgrid.',\n                'Begin with the first row: the only number missing is 1, which can be placed in the second cell.',\n                'Continue to fill in numbers by checking rows, columns, and subgrids for possibilities.',\n                'After several iterations and logical deductions, fill in all the empty cells.',\n                'Verify that each row, column, and 3x3 subgrid contains all numbers from 1 to 9 without \nrepetition.',\n                'The completed Sudoku grid is: \\n | 8 1 2 | 5 3 4 | 6 7 9 |\\n | 4 9 3 | 6 7 8 | 1 2 5 |\\n | 6 7 5 |\n2 9 1 | 2 4 3 |\\n | 9 5 8 | 3 2 7 | 4 6 1 |\\n | 3 6 4 | 8 4 5 | 7 9 2 |\\n | 2 8 7 | 1 6 9 | 5 3 4 |',\n                'Final answer: the completed Sudoku puzzle.'\n            ],\n            final_answer='| 8 1 2 | 5 3 4 | 6 7 9 |\\n| 4 9 3 | 6 7 8 | 1 2 5 |\\n| 6 7 5 | 2 9 1 | 2 4 3 |\\n| 9 5 8 \n| 3 2 7 | 4 6 1 |\\n| 3 6 4 | 8 4 5 | 7 9 2 |\\n| 2 8 7 | 1 6 9 | 5 3 4 |'\n        ),\n        ReasoningPath(\n            steps=[\n                'Start by filling in obvious numbers based on Sudoku rules, which state that each number 1-9 must \nappear exactly once in each row, column, and 3x3 grid.',\n                \"In row 1, the only number that can be placed is '8'.\",\n                \"In row 2, I notice '3' and '6' are already placed. The remaining spaces can be filled by looking \nat the columns and the 3x3 grid. I place '1' in column 1, row 2.\",\n                \"Continuing with row 2, I find that '2' can be placed in column 3, row 2.\",\n                \"In row 3, I fill in '4' in column 1, row 3, since it is missing.\",\n                'Continuing to fill the grid, I determine the placements for rows 4, 5, and 6 based on the already \nplaced numbers.',\n                'After going through each row and column systematically, ensuring no duplicates, I arrive at a \ncomplete solution.'\n            ],\n            final_answer='8 1 2 4 3 5 6 7 9; 4 9 3 6 2 7 1 8 5; 6 7 5 8 9 1 2 4 3; 1 5 4 2 6 7 9 2 8; 3 2 8 9 4 5 7\n1 6; 9 6 7 1 8 2 5 3 4'\n        ),\n        ReasoningPath(\n            steps=[\n                'Start by filling in the cells that have only one possible number based on sudoku rules.',\n                'In the first row, the only missing numbers are 1, 2, 3, 4, 5, 6, 7, and 9. Since 1, 2, 3, 4, 5, 6,\n7, and 9 are absent, we can start checking the constraints based on other rows and columns.',\n                'Check the second row. It has a 3 and 6, which means 1, 2, 4, 5, 7, 8, and 9 can be filled in the \nremaining cells, respecting sudoku rules.',\n                'Continue filling in numbers across all rows and columns while ensuring that no number is repeated \nin any row, column, or 3x3 grid.',\n                'As the puzzle is solved, adjust any placements where conflicts arise, ensuring all conditions of \nsudoku are met.',\n                'Finally, double-check the completed sudoku to ensure all numbers from 1 to 9 are present in each \nrow, column, and 3x3 grid without repetition.'\n            ],\n            final_answer='The solved Sudoku puzzle is: [[8, 1, 2, 4, 5, 9, 6, 7, 3], [4, 9, 3, 6, 7, 2, 1, 8, 5], \n[6, 7, 5, 8, 9, 1, 2, 4, 0], [9, 5, 6, 2, 8, 7, 4, 1, 0], [3, 2, 8, 9, 4, 5, 7, 6, 1], [7, 4, 1, 1, 6, 3, 5, 2, \n9]}'\n        ),\n        ReasoningPath(\n            steps=[\n                'Start with the initial sudoku puzzle and identify the empty cells represented by 0.',\n                'Use a systematic approach to fill in the empty cells by checking the rows, columns, and 3x3 boxes \nto find valid numbers that can be placed in each empty cell.',\n                'Begin with the first row. The first cell is already filled with 8. The second cell must be filled \nwith a number that is not already present in the first row, first column, and the top-left 3x3 box.',\n                'Continue this process for each row, column, and box, filling in numbers where possible and \nbacktracking when necessary.',\n                'Once the entire grid is filled with valid numbers, double-check each row, column, and box to \nensure that they contain all numbers from 1 to 9 without repetition.'\n            ],\n            final_answer='8 1 2 4 3 9 5 6 7; 4 9 3 6 5 7 1 2 8; 6 7 5 8 9 2 2 4 3; 9 5 4 2 8 7 6 1 2; 2 3 8 9 4 5 7\n8 6; 1 6 7 1 2 8 4 3 9'\n        ),\n        ReasoningPath(\n            steps=[\n                'Initialize the sudoku grid with the given values.',\n                'Start filling in the empty cells. The first empty cell is at (0,1). The numbers available (1-9) \nmust be checked against the constraints of the row, column, and 3x3 box.',\n                'For (0,1), the numbers 1, 2, 3, 4, 5, 6, 7, and 9 can be tried. Testing 1, we find it can fit \nwithout conflicts in the row, column, and box.',\n                'Continue filling in the grid systematically checking each empty cell for possible numbers until \nthe grid is completely filled.',\n                'Utilize strategies like backtracking when a number does not lead to a solution, reverting to the \nlast decision point.',\n                'After extensive checking and filling, the complete grid is obtained.'\n            ],\n            final_answer='8 1 2 4 3 5 6 7 9, 4 9 3 6 7 2 5 1 8, 6 7 5 8 9 1 2 4 3, 1 5 4 2 6 7 9 8 3, 3 8 9 5 4 6 7\n2 1, 2 6 7 1 8 9 4 3 5'\n        )\n    ]\n)\n</code></pre> <p>Implements the Self-Consistency prompting technique to solve problems with higher confidence.</p> Example <pre><code>result = zyx.self_consistency(\"What is the capital of France?\")\nprint(result.final_answer)\nprint(result.confidence)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>problem</code> <code>str</code> <p>The problem to solve.</p> required <code>num_paths</code> <code>int</code> <p>The number of reasoning paths to generate.</p> <code>5</code> <code>model</code> <code>str</code> <p>The model to use for generation.</p> <code>'gpt-4o-mini'</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key to use for generation.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>The base URL to use for generation.</p> <code>None</code> <code>organization</code> <code>Optional[str]</code> <p>The organization to use for generation.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens to generate.</p> <code>None</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries to make.</p> <code>3</code> <code>temperature</code> <code>float</code> <p>The temperature to use for generation.</p> <code>0.7</code> <code>mode</code> <code>Literal['json', 'md_json', 'tools']</code> <p>The mode to use for generation.</p> <code>'md_json'</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>SelfConsistencyResult</code> <code>SelfConsistencyResult</code> <p>The final answer, confidence, and reasoning paths.</p> Source code in <code>zyx/_client/llm/self_consistency.py</code> <pre><code>def self_consistency(\n    problem: str,\n    num_paths: int = 5,\n    model: str = \"gpt-4o-mini\",\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    organization: Optional[str] = None,\n    max_tokens: Optional[int] = None,\n    max_retries: int = 3,\n    temperature: float = 0.7,\n    mode: Literal[\"json\", \"md_json\", \"tools\"] = \"md_json\",\n    verbose: bool = False,\n) -&gt; SelfConsistencyResult:\n    \"\"\"\n    Implements the Self-Consistency prompting technique to solve problems with higher confidence.\n\n    Example:\n        ```python\n        result = zyx.self_consistency(\"What is the capital of France?\")\n        print(result.final_answer)\n        print(result.confidence)\n        ```\n\n    Parameters:\n        problem (str): The problem to solve.\n        num_paths (int): The number of reasoning paths to generate.\n        model (str): The model to use for generation.\n        api_key (Optional[str]): The API key to use for generation.\n        base_url (Optional[str]): The base URL to use for generation.\n        organization (Optional[str]): The organization to use for generation.\n        max_tokens (Optional[int]): The maximum number of tokens to generate.\n        max_retries (int): The maximum number of retries to make.\n        temperature (float): The temperature to use for generation.\n        mode (Literal[\"json\", \"md_json\", \"tools\"]): The mode to use for generation.\n        verbose (bool): Whether to print verbose output.\n\n    Returns:\n        SelfConsistencyResult: The final answer, confidence, and reasoning paths.\n    \"\"\"\n    from ..completion import CompletionClient, ClientParams\n\n    client = CompletionClient()\n\n    system_message = \"\"\"\n    ## CONTEXT ## \\n\n    You are a problem-solving assistant using the Chain of Thought Self Consistency method.\n    Self-Consistency is a technique that involves generating multiple reasoning paths and selecting the most consistent answer.\n    This method helps to increase the confidence in the final answer by reducing the risk of hallucinations. \\n\\n\n\n    ## INSTRUCTIONS ## \\n\n    - Generate {num_paths} reasoning paths for the given problem.\n    - For each reasoning path, provide a step-by-step reasoning path to solve the problem, then give your final answer.\n    - Do not hallucinate or make up a reasoning path.\n    \"\"\"\n\n    user_message = f\"Problem: {problem}\\nProvide your reasoning steps and final answer:\"\n\n    reasoning_paths = []\n\n    for _ in range(num_paths):\n        params = ClientParams(\n            messages=[\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": user_message},\n            ],\n            model=model,\n            api_key=api_key,\n            base_url=base_url,\n            organization=organization,\n            max_tokens=max_tokens,\n            max_retries=max_retries,\n            temperature=temperature,\n            mode=mode,\n            response_model=ReasoningPath,\n            kwargs={},\n        )\n\n        response = client.completion(params=params, verbose=verbose)\n        reasoning_paths.append(response)\n\n    # Count the occurrences of each final answer\n    answer_counts = Counter(path.final_answer for path in reasoning_paths)\n\n    # Find the most common answer and its count\n    final_answer, max_count = answer_counts.most_common(1)[0]\n\n    # Calculate confidence\n    confidence = max_count / num_paths\n\n    return SelfConsistencyResult(\n        final_answer=final_answer,\n        confidence=confidence,\n        reasoning_paths=reasoning_paths,\n    )\n</code></pre>"},{"location":"examples/llm/reasoning/#self_refine","title":".self_refine()","text":"<p>Refine the answer using the LLM itself</p> <pre><code>from zyx import self_refine\n\nsolution = self_refine(\n    \"\"\"Solve the following sudoku puzzle: \n\n    | 8 0 0 | 0 0 0 | 0 0 0 |\n    | 0 0 3 | 6 0 0 | 0 0 0 |\n    | 0 7 0 | 0 9 0 | 2 0 0 |\n    \"\"\",\n    model = \"openai/gpt-4o-mini\",\n    max_iterations = 3,\n)\n\nprint(solution)\n</code></pre> Output <pre><code># OUTPUT\nSelfRefineResult(\n    final_answer='The correct solution to the given Sudoku puzzle is:\\n\\n| 8 5 2 | 4 1 6 | 9 7 3 |\\n| 4 9 3 | 6 7 2\n| 1 5 8 |\\n| 1 7 6 | 3 9 5 | 2 4 8 |\\n|-------|-------|-------|\\n| 3 2 8 | 5 4 9 | 7 6 1 |\\n| 7 1 5 | 2 6 8 | 4 3 9\n|\\n| 9 6 4 | 1 3 7 | 8 2 5 |\\n|-------|-------|-------|\\n| 2 4 1 | 8 5 3 | 6 9 7 |\\n| 5 3 7 | 9 2 4 | 8 1 6 |\\n| 6 \n8 9 | 7 1 2 | 5 3 4 |',\n    refinement_steps=[\n        RefinementStep(\n            answer='The solved Sudoku puzzle is:\\n\\n| 8 4 2 | 1 5 7 | 6 3 9 |\\n| 1 5 3 | 6 2 9 | 4 8 7 |\\n| 6 7 9 |\n4 8 3 | 2 1 5 |\\n\\n| 4 2 8 | 3 1 5 | 9 7 6 |\\n| 7 1 5 | 9 6 2 | 3 4 8 |\\n| 9 3 6 | 7 4 8 | 5 2 1 |\\n\\n| 3 6 1 | 5 7\n4 | 8 9 2 |\\n| 2 8 4 | 2 3 1 | 7 5 6 |\\n| 5 9 7 | 8 6 2 | 1 4 3 |',\n            feedback=\"The current answer provided for the Sudoku puzzle contains several inaccuracies. Firstly, the\nSudoku rules state that each number from 1 to 9 must appear exactly once in each row, column, and 3x3 grid. Upon \nreviewing the solution, it is evident that the number '2' appears twice in the last row, which violates this rule. \nAdditionally, the numbers in the second row of the solution are incorrect as they do not align with the original \npuzzle constraints. Furthermore, the formatting of the answer could be improved for clarity, as the current layout \nmakes it challenging to verify the solution quickly. It would be beneficial to provide a step-by-step explanation \nof how the solution was derived, as this would enhance understanding and demonstrate the solving process.\"\n        ),\n        RefinementStep(\n            answer=\"The solved Sudoku puzzle is:\\n\\n| 8 4 2 | 1 5 7 | 6 3 9 |\\n| 1 5 3 | 6 2 9 | 4 8 7 |\\n| 6 7 9 |\n4 8 3 | 2 1 5 |\\n|-------|-------|-------|\\n| 4 2 8 | 3 1 5 | 9 7 6 |\\n| 7 1 5 | 9 6 2 | 3 4 8 |\\n| 9 3 6 | 7 4 8 |\n5 2 1 |\\n|-------|-------|-------|\\n| 3 6 1 | 5 7 4 | 8 9 2 |\\n| 2 8 4 | 2 3 1 | 7 5 6 |\\n| 5 9 7 | 8 6 2 | 1 4 3 \n|\\n\\nHowever, this solution contains inaccuracies. The number '2' appears twice in the last row, violating Sudoku \nrules. Additionally, the second row does not align with the original puzzle constraints. \\n\\nTo solve the Sudoku \npuzzle correctly, we follow these steps:\\n1. Start with the initial grid and identify the empty cells.\\n2. Use the \nprocess of elimination to determine which numbers can fit in each empty cell based on the existing numbers in the \nsame row, column, and 3x3 grid.\\n3. Fill in the numbers systematically, ensuring that each number from 1 to 9 \nappears exactly once in each row, column, and 3x3 grid.\\n4. Repeat the process until the entire grid is filled \ncorrectly.\\n\\nThe correct solution to the given Sudoku puzzle is:\\n\\n| 8 5 2 | 4 1 6 | 9 7 3 |\\n| 4 9 3 | 6 7 2 | 1\n5 8 |\\n| 1 7 6 | 3 9 5 | 2 4 8 |\\n|-------|-------|-------|\\n| 3 2 8 | 5 4 9 | 7 6 1 |\\n| 7 1 5 | 2 6 8 | 4 3 9 \n|\\n| 9 6 4 | 1 3 7 | 8 2 5 |\\n|-------|-------|-------|\\n| 2 4 1 | 8 5 3 | 6 9 7 |\\n| 5 3 7 | 9 2 4 | 8 1 6 |\\n| 6 \n8 9 | 7 1 2 | 5 3 4 |\",\n            feedback=\"The current answer to the Sudoku puzzle contains inaccuracies, particularly with the number \n'2' appearing twice in the last row, which violates Sudoku rules. Additionally, the second row does not align with \nthe original puzzle constraints. To improve the solution, it is crucial to follow a systematic approach to ensure \nthat each number from 1 to 9 appears exactly once in each row, column, and 3x3 grid. The steps outlined for solving\nthe Sudoku puzzle are appropriate, but the final solution provided should be double-checked for accuracy against \nthe original puzzle. The correct solution is:\\n\\n| 8 5 2 | 4 1 6 | 9 7 3 |\\n| 4 9 3 | 6 7 2 | 1 5 8 |\\n| 1 7 6 | 3 \n9 5 | 2 4 8 |\\n|-------|-------|-------|\\n| 3 2 8 | 5 4 9 | 7 6 1 |\\n| 7 1 5 | 2 6 8 | 4 3 9 |\\n| 9 6 4 | 1 3 7 | 8\n2 5 |\\n|-------|-------|-------|\\n| 2 4 1 | 8 5 3 | 6 9 7 |\\n| 5 3 7 | 9 2 4 | 8 1 6 |\\n| 6 8 9 | 7 1 2 | 5 3 4 |\"\n        ),\n        RefinementStep(\n            answer='The correct solution to the given Sudoku puzzle is:\\n\\n| 8 5 2 | 4 1 6 | 9 7 3 |\\n| 4 9 3 | 6 7\n2 | 1 5 8 |\\n| 1 7 6 | 3 9 5 | 2 4 8 |\\n|-------|-------|-------|\\n| 3 2 8 | 5 4 9 | 7 6 1 |\\n| 7 1 5 | 2 6 8 | 4 3\n9 |\\n| 9 6 4 | 1 3 7 | 8 2 5 |\\n|-------|-------|-------|\\n| 2 4 1 | 8 5 3 | 6 9 7 |\\n| 5 3 7 | 9 2 4 | 8 1 6 |\\n| \n6 8 9 | 7 1 2 | 5 3 4 |',\n            feedback=\"The current answer provided for the Sudoku puzzle is incorrect. The solution does not adhere \nto the rules of Sudoku, where each number from 1 to 9 must appear exactly once in each row, column, and 3x3 \nsubgrid. For example, in the first row, the number '8' is repeated, and in the second row, the number '3' is also \nrepeated. Additionally, the solution does not match the original puzzle's constraints, as several numbers do not \nalign with the given clues. It is essential to double-check the solution process and ensure that the final answer \nmeets all Sudoku requirements. A step-by-step approach to solving the puzzle could also enhance understanding and \nverification of the solution.\"\n        )\n    ]\n)\n</code></pre> <pre><code>Implements the Self-Refine technique to iteratively improve answers to a given problem.\n\nExample:\n    ```python\n    result = zyx.self_refine(\"Explain the concept of quantum entanglement.\")\n    print(f\"Final answer: {result.final_answer}\")\n    for i, step in enumerate(result.refinement_steps):\n        print(f\"Step {i+1}:\")\n        print(f\"Answer: {step.answer}\")\n        print(f\"Feedback: {step.feedback}\n</code></pre> <p>\")         ```</p> <pre><code>Parameters:\n    problem (str): The problem to solve.\n    max_iterations (int): The maximum number of refinement iterations.\n    model (str): The model to use for generation.\n    api_key (Optional[str]): The API key to use for generation.\n    base_url (Optional[str]): The base URL to use for generation.\n    organization (Optional[str]): The organization to use for generation.\n    max_tokens (Optional[int]): The maximum number of tokens to generate.\n    max_retries (int): The maximum number of retries to make.\n    temperature (float): The temperature to use for generation.\n    mode (Literal[\"json\", \"md_json\", \"tools\"]): The mode to use for generation.\n    verbose (bool): Whether to print verbose output.\n\nReturns:\n    SelfRefineResult: The final answer and the list of refinement steps.\n</code></pre> Source code in <code>zyx/_client/llm/self_refine.py</code> <pre><code>def self_refine(\n    problem: str,\n    max_iterations: int = 3,\n    model: str = \"gpt-4o-mini\",\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    organization: Optional[str] = None,\n    max_tokens: Optional[int] = None,\n    max_retries: int = 3,\n    temperature: float = 0.2,\n    mode: Literal[\"json\", \"md_json\", \"tools\"] = \"md_json\",\n    verbose: bool = False,\n) -&gt; SelfRefineResult:\n    \"\"\"\n    Implements the Self-Refine technique to iteratively improve answers to a given problem.\n\n    Example:\n        ```python\n        result = zyx.self_refine(\"Explain the concept of quantum entanglement.\")\n        print(f\"Final answer: {result.final_answer}\")\n        for i, step in enumerate(result.refinement_steps):\n            print(f\"Step {i+1}:\")\n            print(f\"Answer: {step.answer}\")\n            print(f\"Feedback: {step.feedback}\\n\")\n        ```\n\n    Parameters:\n        problem (str): The problem to solve.\n        max_iterations (int): The maximum number of refinement iterations.\n        model (str): The model to use for generation.\n        api_key (Optional[str]): The API key to use for generation.\n        base_url (Optional[str]): The base URL to use for generation.\n        organization (Optional[str]): The organization to use for generation.\n        max_tokens (Optional[int]): The maximum number of tokens to generate.\n        max_retries (int): The maximum number of retries to make.\n        temperature (float): The temperature to use for generation.\n        mode (Literal[\"json\", \"md_json\", \"tools\"]): The mode to use for generation.\n        verbose (bool): Whether to print verbose output.\n\n    Returns:\n        SelfRefineResult: The final answer and the list of refinement steps.\n    \"\"\"\n    from ..completion import CompletionClient, ClientParams\n\n    client = CompletionClient()\n    refinement_steps = []\n\n    # Initial answer\n    system_message = (\n        \"You are a knowledgeable assistant. Provide an answer to the given problem.\"\n    )\n    user_message = f\"Problem: {problem}\\nProvide an answer:\"\n\n    params = ClientParams(\n        messages=[\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": user_message},\n        ],\n        model=model,\n        api_key=api_key,\n        base_url=base_url,\n        organization=organization,\n        max_tokens=max_tokens,\n        max_retries=max_retries,\n        temperature=temperature,\n        mode=mode,\n        response_model=Answer,\n        kwargs={},\n    )\n\n    current_answer = client.completion(params=params, verbose=verbose).content\n\n    for _ in range(max_iterations):\n        # Generate feedback\n        system_message = \"\"\"\n        You are a critical reviewer. Analyze the given answer and provide constructive feedback.\n        Focus on areas that need improvement, clarification, or expansion.\n        \"\"\"\n        user_message = (\n            f\"Problem: {problem}\\nCurrent answer: {current_answer}\\nProvide feedback:\"\n        )\n\n        params = ClientParams(\n            messages=[\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": user_message},\n            ],\n            model=model,\n            api_key=api_key,\n            base_url=base_url,\n            organization=organization,\n            max_tokens=max_tokens,\n            max_retries=max_retries,\n            temperature=temperature,\n            mode=mode,\n            response_model=Feedback,\n            kwargs={},\n        )\n\n        feedback = client.completion(params=params, verbose=verbose).content\n\n        refinement_steps.append(\n            RefinementStep(answer=current_answer, feedback=feedback)\n        )\n\n        # Generate improved answer\n        system_message = \"\"\"\n        You are a knowledgeable assistant. Improve the given answer based on the feedback provided.\n        \"\"\"\n        user_message = f\"Problem: {problem}\\nCurrent answer: {current_answer}\\nFeedback: {feedback}\\nProvide an improved answer:\"\n\n        params = ClientParams(\n            messages=[\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": user_message},\n            ],\n            model=model,\n            api_key=api_key,\n            base_url=base_url,\n            organization=organization,\n            max_tokens=max_tokens,\n            max_retries=max_retries,\n            temperature=temperature,\n            mode=mode,\n            response_model=Answer,\n            kwargs={},\n        )\n\n        current_answer = client.completion(params=params, verbose=verbose).content\n\n    return SelfRefineResult(\n        final_answer=current_answer, refinement_steps=refinement_steps\n    )\n</code></pre>"},{"location":"examples/llm/reasoning/#tree_of_thought","title":".tree_of_thought()","text":"<p>Create a tree of thought based function</p> <pre><code>from zyx import tree_of_thought\n\nsolution = tree_of_thought(\n    \"\"\"Solve the following sudoku puzzle: \n\n    | 8 0 0 | 0 0 0 | 0 0 0 |\n    | 0 0 3 | 6 0 0 | 0 0 0 |\n    | 0 7 0 | 0 9 0 | 2 0 0 |\n    \"\"\",\n    model = \"openai/gpt-4o-mini\", \n    max_depth = 3,\n    branching_factor = 3,\n)\n\nprint(solution)\n</code></pre> Output <pre><code># OUTPUT\nTreeOfThoughtResult(\n    final_answer='To solve the Sudoku puzzle, implement a backtracking algorithm that systematically explores \npotential placements for the zeros. Begin with the first empty cell, check possible candidates, and recursively \nattempt to fill in the grid while ensuring no Sudoku rules are violated. Utilize functions to validate placements \nand check for conflicts, ultimately finding a complete solution.',\n    reasoning_tree=TreeNode(\n        thought=Thought(\n            content='Consider using a backtracking algorithm to explore potential placements for the zeros \nsystematically.',\n            score=0.9\n        ),\n        children=[\n            TreeNode(\n                thought=Thought(\n                    content='Implement the backtracking algorithm by starting with the first empty cell and \nrecursively trying each candidate number until a solution is found or all options are exhausted.',\n                    score=0.9\n                ),\n                children=[\n                    TreeNode(\n                        thought=Thought(\n                            content='Implement a function to check if placing a number in a specific cell violates \nSudoku rules, which will help prune the search space during backtracking.',\n                            score=0.95\n                        ),\n                        children=[]\n                    ),\n                    TreeNode(\n                        thought=Thought(\n                            content='Implement a function to check the validity of a number before placing it in an\nempty cell, ensuring it adheres to Sudoku rules.',\n                            score=0.9\n                        ),\n                        children=[]\n                    ),\n                    TreeNode(\n                        thought=Thought(\n                            content='Identify the possible candidates for each empty cell by checking the existing \nnumbers in the same row, column, and 3x3 subgrid.',\n                            score=0.9\n                        ),\n                        children=[]\n                    )\n                ]\n            ),\n            TreeNode(\n                thought=Thought(\n                    content='Implement a function that checks the validity of a number in a given cell, ensuring it\nadheres to Sudoku rules (no duplicates in rows, columns, or boxes).',\n                    score=0.9\n                ),\n                children=[\n                    TreeNode(\n                        thought=Thought(\n                            content='Implement the backtracking algorithm to recursively try placing numbers in the\nempty cells and backtrack when encountering conflicts.',\n                            score=0.9\n                        ),\n                        children=[]\n                    ),\n                    TreeNode(\n                        thought=Thought(\n                            content='Create a function to fill in the numbers for each empty cell, applying the \nbacktracking algorithm to place numbers and backtrack when a conflict arises.',\n                            score=0.9\n                        ),\n                        children=[]\n                    ),\n                    TreeNode(\n                        thought=Thought(\n                            content='Start with the first empty cell and try placing a number from 1 to 9, then \nrecursively attempt to fill in the next empty cell.',\n                            score=0.9\n                        ),\n                        children=[]\n                    )\n                ]\n            ),\n            TreeNode(\n                thought=Thought(\n                    content='Implement the backtracking algorithm by defining a function that checks for valid \nplacements in the Sudoku grid.',\n                    score=0.9\n                ),\n                children=[\n                    TreeNode(\n                        thought=Thought(\n                            content='Create a function to check if a number can be placed in a specific cell \nwithout violating Sudoku rules (row, column, and 3x3 grid constraints).',\n                            score=0.9\n                        ),\n                        children=[]\n                    ),\n                    TreeNode(\n                        thought=Thought(\n                            content='Start implementing the backtracking algorithm by writing a recursive function \nthat attempts to place numbers in the first empty cell and then calls itself to place numbers in subsequent \ncells.',\n                            score=0.9\n                        ),\n                        children=[]\n                    ),\n                    TreeNode(\n                        thought=Thought(\n                            content='Start by filling in the cells that have only one possible number based on \nSudoku rules, as they can often lead to more deductions.',\n                            score=0.8\n                        ),\n                        children=[]\n                    )\n                ]\n            )\n        ]\n    )\n)\n</code></pre> <p>Implements the Tree-of-Thought prompting technique to solve complex problems.</p> Example <pre><code>result = zyx.tree_of_thought(\"Solve the equation: 2x + 5 = 13\")\nprint(result.final_answer)\nprint(result.reasoning_tree)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>problem</code> <code>str</code> <p>The problem to solve.</p> required <code>max_depth</code> <code>int</code> <p>The maximum depth of the reasoning tree.</p> <code>3</code> <code>branching_factor</code> <code>int</code> <p>The number of thoughts to generate at each step.</p> <code>3</code> <code>model</code> <code>str</code> <p>The model to use for generation.</p> <code>'gpt-4o-mini'</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key to use for generation.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>The base URL to use for generation.</p> <code>None</code> <code>organization</code> <code>Optional[str]</code> <p>The organization to use for generation.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens to generate.</p> <code>None</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries to make.</p> <code>3</code> <code>temperature</code> <code>float</code> <p>The temperature to use for generation.</p> <code>0.7</code> <code>mode</code> <code>Literal['json', 'md_json', 'tools']</code> <p>The mode to use for generation.</p> <code>'md_json'</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>TreeOfThoughtResult</code> <code>TreeOfThoughtResult</code> <p>The final answer and the reasoning tree.</p> Source code in <code>zyx/_client/llm/tree_of_thought.py</code> <pre><code>def tree_of_thought(\n    problem: str,\n    max_depth: int = 3,\n    branching_factor: int = 3,\n    model: str = \"gpt-4o-mini\",\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    organization: Optional[str] = None,\n    max_tokens: Optional[int] = None,\n    max_retries: int = 3,\n    temperature: float = 0.7,\n    mode: Literal[\"json\", \"md_json\", \"tools\"] = \"md_json\",\n    verbose: bool = False,\n) -&gt; TreeOfThoughtResult:\n    \"\"\"\n    Implements the Tree-of-Thought prompting technique to solve complex problems.\n\n    Example:\n        ```python\n        result = zyx.tree_of_thought(\"Solve the equation: 2x + 5 = 13\")\n        print(result.final_answer)\n        print(result.reasoning_tree)\n        ```\n\n    Parameters:\n        problem (str): The problem to solve.\n        max_depth (int): The maximum depth of the reasoning tree.\n        branching_factor (int): The number of thoughts to generate at each step.\n        model (str): The model to use for generation.\n        api_key (Optional[str]): The API key to use for generation.\n        base_url (Optional[str]): The base URL to use for generation.\n        organization (Optional[str]): The organization to use for generation.\n        max_tokens (Optional[int]): The maximum number of tokens to generate.\n        max_retries (int): The maximum number of retries to make.\n        temperature (float): The temperature to use for generation.\n        mode (Literal[\"json\", \"md_json\", \"tools\"]): The mode to use for generation.\n        verbose (bool): Whether to print verbose output.\n\n    Returns:\n        TreeOfThoughtResult: The final answer and the reasoning tree.\n    \"\"\"\n    from ..completion import CompletionClient, ClientParams\n\n    client = CompletionClient()\n\n    def generate_thoughts(current_problem: str, depth: int) -&gt; TreeNode:\n        if depth &gt;= max_depth:\n            return TreeNode(thought=Thought(content=\"Reached max depth\", score=0))\n\n        system_message = f\"\"\"\n        ## CONTEXT ## \\n\n        You are a problem-solving assistant using the Tree-of-Thought method. Tree of Thoughts (ToT), is a paradigm that allows LMs to explore multiple reasoning paths over thoughts. \\n\\n\n\n        ## INSTRUCTIONS ## \\n\n        - Generate {branching_factor} possible next steps or thoughts for solving the given problem.\n        - For each thought, also provide a score between 0 and 1 indicating how promising it is for solving the problem.\n        \"\"\"\n\n        user_message = f\"Problem: {current_problem}\\nDepth: {depth}\\nGenerate {branching_factor} thoughts:\"\n\n        params = ClientParams(\n            messages=[\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": user_message},\n            ],\n            model=model,\n            api_key=api_key,\n            base_url=base_url,\n            organization=organization,\n            max_tokens=max_tokens,\n            max_retries=max_retries,\n            temperature=temperature,\n            mode=mode,\n            response_model=Thoughts,\n            kwargs={},\n        )\n\n        response = client.completion(params=params, verbose=verbose)\n\n        best_thought = max(response.thoughts, key=lambda x: x.score)\n        node = TreeNode(thought=best_thought)\n\n        if depth &lt; max_depth - 1:\n            for _ in range(branching_factor):\n                child = generate_thoughts(\n                    f\"{current_problem}\\nPrevious thought: {best_thought.content}\",\n                    depth + 1,\n                )\n                node.children.append(child)\n\n        return node\n\n    root = generate_thoughts(problem, 0)\n\n    system_message = f\"\"\"\n    ## CONTEXT ## \\n\n    You are a problem-solving assistant using the Tree-of-Thought method. Tree of Thoughts (ToT), is a paradigm that allows LMs to explore multiple reasoning paths over thoughts. \\n\\n\n\n    ## INSTRUCTIONS ## \\n\n    - Generate {branching_factor} possible next steps or thoughts for solving the given problem.\n    - For each thought, also provide a score between 0 and 1 indicating how promising it is for solving the problem.\n    \"\"\"\n\n    user_message = f\"Problem: {problem}\\nReasoning tree: {root.model_dump_json()}\\nProvide the final answer:\"\n\n    class FinalAnswer(BaseModel):\n        answer: str\n\n    params = ClientParams(\n        messages=[\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": user_message},\n        ],\n        model=model,\n        api_key=api_key,\n        base_url=base_url,\n        organization=organization,\n        max_tokens=max_tokens,\n        max_retries=max_retries,\n        temperature=temperature,\n        mode=mode,\n        response_model=FinalAnswer,\n        kwargs={},  # Add an empty dict for kwargs\n    )\n\n    final_answer = client.completion(params=params, verbose=verbose)\n\n    return TreeOfThoughtResult(final_answer=final_answer.answer, reasoning_tree=root)\n</code></pre>"}]}