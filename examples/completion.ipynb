{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating **Completions** with **zyx**\n",
    "\n",
    "The primary module of zyx, is the universal <code>.completion()</code> function. This module is an extensive wrapper around the <code>litellm .completion()</code> function, as well as the Instructor library. </br>\n",
    "\n",
    "The <code>.completion()</code> function is capable of\n",
    "\n",
    "- **Generations with any LiteLLM compatible model**\n",
    "    - Ollama, OpenAI, Anthropic, Groq, Mistral, and more!\n",
    "- **Direct Instructor Pydantc structured outputs**\n",
    "- **Tool calling & execution support. (Get a tool interpretation with one function)**\n",
    "    - zyx provides a few prebuilt tools out of the box\n",
    "    - Can take in a list of **Python functions**, **OpenAI dictionaries**, or **Pydantic models** as tools!\n",
    "    - Automatic tool execution if a tool is called through the <code>run_tools</code> parameter\n",
    "- **Streaming**\n",
    "- **New** Vision support \n",
    "    - Pass in a list of urls\n",
    "    - Currently uses multi shot prompting if a response model or tools were also passed.\n",
    "-  **New** Prompt optimization \n",
    "    - Creates or optimizes a task tuned system prompt using either the *COSTAR* or *TIDD-EC* frameworks automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Standard Completion**\n",
    "\n",
    "Lets start by generated a simple LLM completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hammad/miniconda3/envs/zyx/lib/python3.11/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'underscore_attrs_are_private' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hello! I'm just a program, but I'm here and ready to help you. How can I assist you today?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Hello! I'm just a program, but I'm here and ready to help you. How can I assist you today?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simplest Way to Generate\n",
    "# Defaults to \"gpt-4o-mini\" if no model is provided\n",
    "from zyx import completion\n",
    "\n",
    "response = completion(\"Hi, how are you?\")\n",
    "\n",
    "# Returns a standard OpenAI style response object\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The completion function is able to take in messages as a <code>string</code>, or a <code>list of OpenAI formatted messages</code>. </br>\n",
    "\n",
    "The OpenAI message format is as follows:\n",
    "\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"}, # System Prompt\n",
    "    {\"role\": \"user\", \"content\": \"Hi, how are you?\"} # User Message\n",
    "    # {\"role\": \"assistant\", \"content\": \"I am fine, thank you!\"} # Assistant Message\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ModelResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-7894f781-84d4-4a6c-88b7-d0d20ee3f2da'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Choices</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">finish_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">index</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Message</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Estoy bien, gracias. ¿Y tú?'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">function_call</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1725776260</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'ollama/llama3.1'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Usage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span>, <span style=\"color: #808000; text-decoration-color: #808000\">total_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">39</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mModelResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-7894f781-84d4-4a6c-88b7-d0d20ee3f2da'\u001b[0m,\n",
       "    \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mChoices\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mfinish_reason\u001b[0m=\u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[33mindex\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "            \u001b[33mmessage\u001b[0m=\u001b[1;35mMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mcontent\u001b[0m=\u001b[32m'Estoy bien, gracias. ¿Y tú?'\u001b[0m,\n",
       "                \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "                \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[33mfunction_call\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mcreated\u001b[0m=\u001b[1;36m1725776260\u001b[0m,\n",
       "    \u001b[33mmodel\u001b[0m=\u001b[32m'ollama/llama3.1'\u001b[0m,\n",
       "    \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,\n",
       "    \u001b[33msystem_fingerprint\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33musage\u001b[0m=\u001b[1;35mUsage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcompletion_tokens\u001b[0m=\u001b[1;36m11\u001b[0m, \u001b[33mprompt_tokens\u001b[0m=\u001b[1;36m28\u001b[0m, \u001b[33mtotal_tokens\u001b[0m=\u001b[1;36m39\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from zyx import completion\n",
    "\n",
    "# Lets pass in a system prompt to change the behavior of the assistant\n",
    "response = completion(\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You only respond in spanish\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hi how are you?\"}\n",
    "    ],\n",
    "    model = \"ollama/llama3.1\", # Any LiteLLM compatible model will work\n",
    "    max_tokens = 100,\n",
    "    temperature = 0.5,\n",
    ")\n",
    "\n",
    "# Lets print the full response object\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generating Structured Outputs with Pydantic & Instructor**\n",
    "\n",
    "<samp>zyx</samp> builds the <code>instructor</code> wrapper on top of the <code>litellm</code> completion wrapper, so you're able to use the functionality of instructor straight from the same function. </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Standard Instructor Completion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Person</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'John Doe'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">age</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mPerson\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname\u001b[0m=\u001b[32m'John Doe'\u001b[0m, \u001b[33mage\u001b[0m=\u001b[1;36m30\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Person Name: <span style=\"color: #008000; text-decoration-color: #008000\">'John Doe'</span>, Person Age: <span style=\"color: #008000; text-decoration-color: #008000\">'30'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Person Name: \u001b[32m'John Doe'\u001b[0m, Person Age: \u001b[32m'30'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import zyx\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "# We can pass in a system prompt to change the behavior of the assistant\n",
    "response = zyx.completion(\n",
    "    \"Create a mock person\",\n",
    "    response_model = Person\n",
    ")\n",
    "\n",
    "# Lets print the full response object\n",
    "print(response)\n",
    "\n",
    "print(f\"Person Name: '{response.name}', Person Age: '{response.age}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **You also have the ability to change the instructor parsing mode as a parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Person</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">alias</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Spiderman'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">real_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Peter Parker'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">age</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mPerson\u001b[0m\u001b[1m(\u001b[0m\u001b[33malias\u001b[0m=\u001b[32m'Spiderman'\u001b[0m, \u001b[33mreal_name\u001b[0m=\u001b[32m'Peter Parker'\u001b[0m, \u001b[33mage\u001b[0m=\u001b[1;36m25\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Alias: <span style=\"color: #008000; text-decoration-color: #008000\">'Spiderman'</span>, Real Name: <span style=\"color: #008000; text-decoration-color: #008000\">'Peter Parker'</span>, Age: <span style=\"color: #008000; text-decoration-color: #008000\">'25'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Alias: \u001b[32m'Spiderman'\u001b[0m, Real Name: \u001b[32m'Peter Parker'\u001b[0m, Age: \u001b[32m'25'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import zyx\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Person(BaseModel):\n",
    "    alias : str\n",
    "    real_name: str\n",
    "    age: int\n",
    "\n",
    "# We can pass in a system prompt to change the behavior of the assistant\n",
    "response = zyx.completion(\n",
    "    \"Who is Spiderman?\",\n",
    "    response_model = Person,\n",
    "    model = \"ollama/llama3.1\", # Works with any LiteLLM compatible model\n",
    "    mode = \"md_json\" # This is the default mode, but we can also use \"json\", or \"tools\"\n",
    ")\n",
    "\n",
    "# Lets print the full response object\n",
    "print(response)\n",
    "\n",
    "print(f\"Alias: '{response.alias}', Real Name: '{response.real_name}', Age: '{response.age}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tool Calling & Execution**\n",
    "\n",
    "The completion function has built in support for tool calling and execution. </br>\n",
    "\n",
    "You can pass in a list of tools to the <code>tools</code> parameter. </br>\n",
    "A tool can be:\n",
    "\n",
    "- A python function\n",
    "- An OpenAI formatted tool\n",
    "- A Pydantic model\n",
    "\n",
    "> **Note** : zyx also comes with a few prebuilt tools, the web search tool however, requires a Tavily API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tool Calling**\n",
    "\n",
    "Like most LLM frameworks, zyx returns tool call outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionMessageToolCall</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">function</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Function</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">arguments</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{\"query\":\"2024 Euro Cup Final winner\",\"max_results\":\"5\"}'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'web_search'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'call_yKFgoci3OCOXy6zza7vVODzC'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mChatCompletionMessageToolCall\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mfunction\u001b[0m=\u001b[1;35mFunction\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33marguments\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"query\":\"2024 Euro Cup Final winner\",\"max_results\":\"5\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "            \u001b[33mname\u001b[0m=\u001b[32m'web_search'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'call_yKFgoci3OCOXy6zza7vVODzC'\u001b[0m,\n",
       "        \u001b[33mtype\u001b[0m=\u001b[32m'function'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets return a tool call\n",
    "import zyx\n",
    "\n",
    "# Lets use the prebuilt web search tool!\n",
    "response = zyx.completion(\n",
    "    \"Who won the 2024 Euro Cup Final?\",\n",
    "    tools = [zyx.tools.web_search],\n",
    "    run_tools = False # This wont execute the tool, it will just return the tool call\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tool Execution**\n",
    "\n",
    "Now lets execute the tool call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Spain won the <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span> Euro Cup Final, defeating England <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. The decisive goal was scored by substitute Mikel \n",
       "Oyarzabal in the 86th minute. This victory marked Spain's fourth European championship title. You can find more \n",
       "details about the match <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://en.wikipedia.org/wiki/UEFA_Euro_2024_Final).</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Spain won the \u001b[1;36m2024\u001b[0m Euro Cup Final, defeating England \u001b[1;36m2\u001b[0m-\u001b[1;36m1\u001b[0m. The decisive goal was scored by substitute Mikel \n",
       "Oyarzabal in the 86th minute. This victory marked Spain's fourth European championship title. You can find more \n",
       "details about the match \u001b[1m(\u001b[0m\u001b[4;94mhttps://en.wikipedia.org/wiki/UEFA_Euro_2024_Final\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets execute a tool call\n",
    "import zyx\n",
    "\n",
    "# If Running in Notebook, set the TAVILY_API_KEY environment variable explicitly\n",
    "import os\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"tvly-xxxx\"\n",
    "\n",
    "# Lets use the prebuilt web search tool!\n",
    "response = zyx.completion(\n",
    "    \"Who won the 2024 Euro Cup Final?\",\n",
    "    tools = [zyx.tools.web_search],\n",
    "    run_tools = True # This will execute the tool now\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zyx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
